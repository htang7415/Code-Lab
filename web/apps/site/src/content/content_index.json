{
  "generated_at": "2026-01-31T16:36:22.888Z",
  "tracks": [
    {
      "id": "dsa",
      "name": "Data Structures & Algorithms",
      "description": "Core patterns, data structures, and algorithmic analysis.",
      "accent": "#1f77b4",
      "accentVar": "--track-dsa",
      "topicCount": 21,
      "moduleCount": 1,
      "problemCount": 1
    },
    {
      "id": "ml",
      "name": "Machine Learning",
      "description": "Math foundations, models, optimization, and systems.",
      "accent": "#ff7f0e",
      "accentVar": "--track-ml",
      "topicCount": 13,
      "moduleCount": 178,
      "problemCount": 0
    },
    {
      "id": "ai-agents",
      "name": "AI Agents",
      "description": "Prompting, tool use, memory, and evaluation.",
      "accent": "#2ca02c",
      "accentVar": "--track-ai-agents",
      "topicCount": 9,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "id": "databases",
      "name": "Databases",
      "description": "Schema design, indexing, transactions, and query plans.",
      "accent": "#17becf",
      "accentVar": "--track-databases",
      "topicCount": 10,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "id": "software-engineering",
      "name": "Software Engineering",
      "description": "APIs, performance, testing, and system design.",
      "accent": "#d62728",
      "accentVar": "--track-se",
      "topicCount": 10,
      "moduleCount": 0,
      "problemCount": 0
    }
  ],
  "topics": [
    {
      "track": "software-engineering",
      "topic": "apis",
      "name": "APIs",
      "path": "/track/software-engineering/apis",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "arrays",
      "name": "Arrays",
      "path": "/track/dsa/arrays",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 1,
      "problemCount": 1
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "name": "Backtracking",
      "path": "/track/dsa/backtracking",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "binary-search",
      "name": "Binary Search",
      "path": "/track/dsa/binary-search",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "bit",
      "name": "Bit",
      "path": "/track/dsa/bit",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "databases",
      "topic": "caching",
      "name": "Caching",
      "path": "/track/databases/caching",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "name": "Computer Vision",
      "path": "/track/ml/computer-vision",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 16,
      "problemCount": 0
    },
    {
      "track": "software-engineering",
      "topic": "concurrency",
      "name": "Concurrency",
      "path": "/track/software-engineering/concurrency",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "ml",
      "topic": "data",
      "name": "Data",
      "path": "/track/ml/data",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 7,
      "problemCount": 0
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "name": "Deep Learning",
      "path": "/track/ml/deep-learning",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 33,
      "problemCount": 0
    },
    {
      "track": "software-engineering",
      "topic": "design-patterns",
      "name": "Design Patterns",
      "path": "/track/software-engineering/design-patterns",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "dp",
      "name": "Dynamic Programming",
      "path": "/track/dsa/dp",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "ai-agents",
      "topic": "evals",
      "name": "Evals",
      "path": "/track/ai-agents/evals",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "name": "Evaluation",
      "path": "/track/ml/evaluation",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 14,
      "problemCount": 0
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "name": "Fundamentals",
      "path": "/track/ml/fundamentals",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 19,
      "problemCount": 0
    },
    {
      "track": "ml",
      "topic": "generative",
      "name": "Generative",
      "path": "/track/ml/generative",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 7,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "graphs",
      "name": "Graphs",
      "path": "/track/dsa/graphs",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "greedy",
      "name": "Greedy",
      "path": "/track/dsa/greedy",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "ai-agents",
      "topic": "guardrails",
      "name": "Guardrails",
      "path": "/track/ai-agents/guardrails",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "hashing",
      "name": "Hashing",
      "path": "/track/dsa/hashing",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "heap",
      "name": "Heap",
      "path": "/track/dsa/heap",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "databases",
      "topic": "indexing",
      "name": "Indexing",
      "path": "/track/databases/indexing",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "intervals",
      "name": "Intervals",
      "path": "/track/dsa/intervals",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "linked-list",
      "name": "Linked List",
      "path": "/track/dsa/linked-list",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "ml",
      "topic": "llm",
      "name": "LLM",
      "path": "/track/ml/llm",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 21,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "math",
      "name": "Math",
      "path": "/track/dsa/math",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "ai-agents",
      "topic": "memory",
      "name": "Memory",
      "path": "/track/ai-agents/memory",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "ml",
      "topic": "mlops",
      "name": "MLOps",
      "path": "/track/ml/mlops",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 10,
      "problemCount": 0
    },
    {
      "track": "ml",
      "topic": "models",
      "name": "Models",
      "path": "/track/ml/models",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 15,
      "problemCount": 0
    },
    {
      "track": "ai-agents",
      "topic": "multi-agent",
      "name": "Multi Agent",
      "path": "/track/ai-agents/multi-agent",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "databases",
      "topic": "nosql",
      "name": "Nosql",
      "path": "/track/databases/nosql",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "ai-agents",
      "topic": "observability",
      "name": "Observability",
      "path": "/track/ai-agents/observability",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "ml",
      "topic": "optimization",
      "name": "Optimization",
      "path": "/track/ml/optimization",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 16,
      "problemCount": 0
    },
    {
      "track": "software-engineering",
      "topic": "performance",
      "name": "Performance",
      "path": "/track/software-engineering/performance",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "ai-agents",
      "topic": "planning",
      "name": "Planning",
      "path": "/track/ai-agents/planning",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "ai-agents",
      "topic": "prompting",
      "name": "Prompting",
      "path": "/track/ai-agents/prompting",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "software-engineering",
      "topic": "python",
      "name": "Python",
      "path": "/track/software-engineering/python",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "databases",
      "topic": "query-plans",
      "name": "Query Plans",
      "path": "/track/databases/query-plans",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "ai-agents",
      "topic": "rag",
      "name": "Rag",
      "path": "/track/ai-agents/rag",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "recursion",
      "name": "Recursion",
      "path": "/track/dsa/recursion",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "name": "Reinforcement Learning",
      "path": "/track/ml/reinforcement-learning",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 12,
      "problemCount": 0
    },
    {
      "track": "databases",
      "topic": "relational",
      "name": "Relational",
      "path": "/track/databases/relational",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "ml",
      "topic": "representation",
      "name": "Representation",
      "path": "/track/ml/representation",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "software-engineering",
      "topic": "rust",
      "name": "Rust",
      "path": "/track/software-engineering/rust",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "databases",
      "topic": "schema-design",
      "name": "Schema Design",
      "path": "/track/databases/schema-design",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "software-engineering",
      "topic": "security-basics",
      "name": "Security Basics",
      "path": "/track/software-engineering/security-basics",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "shortest-path",
      "name": "Shortest Path",
      "path": "/track/dsa/shortest-path",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "sorting",
      "name": "Sorting",
      "path": "/track/dsa/sorting",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "databases",
      "topic": "sql-patterns",
      "name": "SQL Patterns",
      "path": "/track/databases/sql-patterns",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "stack-queue",
      "name": "Stack Queue",
      "path": "/track/dsa/stack-queue",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "databases",
      "topic": "streaming",
      "name": "Streaming",
      "path": "/track/databases/streaming",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "strings",
      "name": "Strings",
      "path": "/track/dsa/strings",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "software-engineering",
      "topic": "system-design",
      "name": "System Design",
      "path": "/track/software-engineering/system-design",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "ml",
      "topic": "systems",
      "name": "Systems",
      "path": "/track/ml/systems",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 8,
      "problemCount": 0
    },
    {
      "track": "software-engineering",
      "topic": "testing",
      "name": "Testing",
      "path": "/track/software-engineering/testing",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "ai-agents",
      "topic": "tool-use",
      "name": "Tool Use",
      "path": "/track/ai-agents/tool-use",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "software-engineering",
      "topic": "tooling",
      "name": "Tooling",
      "path": "/track/software-engineering/tooling",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "topo-sort",
      "name": "Topo Sort",
      "path": "/track/dsa/topo-sort",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "databases",
      "topic": "transactions",
      "name": "Transactions",
      "path": "/track/databases/transactions",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "trees",
      "name": "Trees",
      "path": "/track/dsa/trees",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "tries",
      "name": "Tries",
      "path": "/track/dsa/tries",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "dsa",
      "topic": "union-find",
      "name": "Union Find",
      "path": "/track/dsa/union-find",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    },
    {
      "track": "databases",
      "topic": "vector-db",
      "name": "Vector Db",
      "path": "/track/databases/vector-db",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0,
      "problemCount": 0
    }
  ],
  "modules": [
    {
      "track": "dsa",
      "topic": "arrays",
      "slug": "prefix-sum",
      "title": "Prefix Sum",
      "path": "modules/dsa/arrays/prefix-sum",
      "summary": "> Track: `dsa` | Topic: `arrays`",
      "readme": "# Prefix Sum\n\n> Track: `dsa` | Topic: `arrays`\n\n## Concept\n\nA **prefix sum** (cumulative sum) array stores running totals so that any\nsubarray sum can be computed in **O(1)** after **O(n)** preprocessing.\n\nUse cases: range sum queries, subarray sum problems, difference arrays,\nand 2D grid prefix sums.\n\n## Math\n\n$$\\text{Given an array \\texttt{a} of length \\texttt{n}, the prefix sum array \\texttt{p} of length \\texttt{n+1}:}$$\n\n```\np[0] = 0\np[i] = a[0] + a[1] +... + a[i-1]    for i = 1..n\n```\n\n$$\\text{Sum of elements from index \\texttt{l} to \\texttt{r} (inclusive):}$$\n\n```\nsum(l, r) = p[r+1] - p[l]\n```\n\n- $\\text{Build time: **O(n)**}$\n- $\\text{Query time: **O(1)**}$\n\n## Function\n\n```python\ndef prefix_sum(arr: list[int]) -> list[int]\n```\n\n- `arr` — input array of integers\n- Returns — prefix sum array of length `len(arr) + 1`\n- `result[0] = 0`, `result[i] = sum(arr[0..i-1])`\n",
      "sources": [
        {
          "path": "modules/dsa/arrays/prefix-sum/python/prefix_sum.py",
          "language": "python",
          "content": "\"\"\"Prefix sum — build and query range sums in O(1).\"\"\"\n\nfrom typing import List\n\n\ndef build_prefix_sum(arr: List[int]) -> List[int]:\n    \"\"\"Return a prefix sum array of length len(arr)+1.\n\n    prefix[i] = sum of arr[0..i-1], with prefix[0] = 0.\n    \"\"\"\n    prefix = [0] * (len(arr) + 1)\n    for i, val in enumerate(arr):\n        prefix[i + 1] = prefix[i] + val\n    return prefix\n\n\ndef range_sum(prefix: List[int], left: int, right: int) -> int:\n    \"\"\"Return sum of the original array from index left to right (inclusive).\n\n    Requires the prefix sum array from build_prefix_sum.\n    \"\"\"\n    return prefix[right + 1] - prefix[left]\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "alexnet",
      "title": "AlexNet",
      "path": "modules/ml/computer-vision/alexnet",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# AlexNet\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nAlexNet is a classic CNN architecture with characteristic layer design.\n\n## Math\n\n$$\\text{Architecture defined by ordered layers.}$$\n\n## Function\n\n```python\ndef layers() -> list[str]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/alexnet/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/alexnet/python/alexnet.py",
          "language": "python",
          "content": "def layers() -> list[str]:\n    return [\"conv\", \"relu\", \"pool\", \"fc\"]\n"
        },
        {
          "path": "modules/ml/computer-vision/alexnet/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn layers() -> Vec<&'static str> {\n    vec![\"conv\", \"relu\", \"pool\", \"fc\"]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "bilinear-resizing",
      "title": "Bilinear Resizing",
      "path": "modules/ml/computer-vision/bilinear-resizing",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Bilinear Resizing\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nBilinear interpolation blends four neighbors.\n\n## Math\n\n$$f(x,y) = \\sum_{i,j} w_{ij} v_{ij}$$\n\n## Function\n\n```python\ndef bilinear_sample(v00: float, v01: float, v10: float, v11: float, tx: float, ty: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/bilinear-resizing/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/bilinear-resizing/python/bilinear_resizing.py",
          "language": "python",
          "content": "def bilinear_sample(v00: float, v01: float, v10: float, v11: float, tx: float, ty: float) -> float:\n    a = v00 * (1 - tx) + v10 * tx\n    b = v01 * (1 - tx) + v11 * tx\n    return a * (1 - ty) + b * ty\n"
        },
        {
          "path": "modules/ml/computer-vision/bilinear-resizing/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn bilinear_sample(v00: f64, v01: f64, v10: f64, v11: f64, tx: f64, ty: f64) -> f64 {\n    let a = v00 * (1.0 - tx) + v10 * tx;\n    let b = v01 * (1.0 - tx) + v11 * tx;\n    a * (1.0 - ty) + b * ty\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "cnn-2d-vs-3d",
      "title": "2D vs 3D CNN",
      "path": "modules/ml/computer-vision/cnn-2d-vs-3d",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# 2D vs 3D CNN\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\n3D CNNs convolve over time/depth in addition to height/width.\n\n## Math\n\n$$output_depth = input_depth - kernel_depth + 1$$\n\n## Function\n\n```python\ndef output_depth(input_depth: int, kernel_depth: int) -> int:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/cnn-2d-vs-3d/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/cnn-2d-vs-3d/python/cnn_2d_vs_3d.py",
          "language": "python",
          "content": "def output_depth(input_depth: int, kernel_depth: int) -> int:\n    return input_depth - kernel_depth + 1\n"
        },
        {
          "path": "modules/ml/computer-vision/cnn-2d-vs-3d/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn output_depth(input_depth: i32, kernel_depth: i32) -> i32 {\n    input_depth - kernel_depth + 1\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "cnn-basics",
      "title": "CNNs",
      "path": "modules/ml/computer-vision/cnn-basics",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# CNNs\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nCNNs use convolutional filters to capture local patterns.\n\n## Math\n\n$$feature_map = conv(input, kernel)$$\n\n## Function\n\n```python\ndef conv1d(signal: list[float], kernel: list[float]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/cnn-basics/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/cnn-basics/python/cnn_basics.py",
          "language": "python",
          "content": "def conv1d(signal: list[float], kernel: list[float]) -> list[float]:\n    out = []\n    k = len(kernel)\n    for i in range(len(signal) - k + 1):\n        out.append(sum(signal[i + j] * kernel[j] for j in range(k)))\n    return out\n"
        },
        {
          "path": "modules/ml/computer-vision/cnn-basics/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn conv1d(signal: &[f64], kernel: &[f64]) -> Vec<f64> {\n    let k = kernel.len();\n    let mut out = Vec::new();\n    for i in 0..=signal.len() - k {\n        let mut acc = 0.0;\n        for j in 0..k {\n            acc += signal[i + j] * kernel[j];\n        }\n        out.push(acc);\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "contrast-brightness",
      "title": "Contrast and Brightness",
      "path": "modules/ml/computer-vision/contrast-brightness",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Contrast and Brightness\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nAdjust brightness and contrast via linear transform.\n\n## Math\n\n$$x' = a x + b$$\n\n## Function\n\n```python\ndef adjust(x: float, a: float, b: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/contrast-brightness/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/contrast-brightness/python/contrast_brightness.py",
          "language": "python",
          "content": "def adjust(x: float, a: float, b: float) -> float:\n    return a * x + b\n"
        },
        {
          "path": "modules/ml/computer-vision/contrast-brightness/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn adjust(x: f64, a: f64, b: f64) -> f64 {\n    a * x + b\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "convolution-layer",
      "title": "Convolution Layer",
      "path": "modules/ml/computer-vision/convolution-layer",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Convolution Layer\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nA convolution layer slides a kernel over the input.\n\n## Math\n\n$$(I * K)_{i,j} = \\sum_{u,v} K_{u,v} I_{i+u, j+v}$$\n\n## Function\n\n```python\ndef conv2d(image: list[list[float]], kernel: list[list[float]]) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/convolution-layer/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/convolution-layer/python/convolution_layer.py",
          "language": "python",
          "content": "def conv2d(image: list[list[float]], kernel: list[list[float]]) -> list[list[float]]:\n    h = len(image)\n    w = len(image[0])\n    kh = len(kernel)\n    kw = len(kernel[0])\n    out = []\n    for i in range(h - kh + 1):\n        row = []\n        for j in range(w - kw + 1):\n            val = 0.0\n            for u in range(kh):\n                for v in range(kw):\n                    val += image[i + u][j + v] * kernel[u][v]\n            row.append(val)\n        out.append(row)\n    return out\n"
        },
        {
          "path": "modules/ml/computer-vision/convolution-layer/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn conv2d(image: &[Vec<f64>], kernel: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let h = image.len();\n    let w = image[0].len();\n    let kh = kernel.len();\n    let kw = kernel[0].len();\n    let mut out = Vec::new();\n    for i in 0..=h - kh {\n        let mut row = Vec::new();\n        for j in 0..=w - kw {\n            let mut val = 0.0;\n            for u in 0..kh {\n                for v in 0..kw {\n                    val += image[i + u][j + v] * kernel[u][v];\n                }\n            }\n            row.push(val);\n        }\n        out.push(row);\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "data-augmentation",
      "title": "Data Augmentation",
      "path": "modules/ml/computer-vision/data-augmentation",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Data Augmentation\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nAugmentation applies random transforms like flips.\n\n## Math\n\n$$x' = flip(x)$$\n\n## Function\n\n```python\ndef horizontal_flip(image: list[list[int]]) -> list[list[int]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/data-augmentation/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/data-augmentation/python/data_augmentation.py",
          "language": "python",
          "content": "def horizontal_flip(image: list[list[int]]) -> list[list[int]]:\n    return [list(reversed(row)) for row in image]\n"
        },
        {
          "path": "modules/ml/computer-vision/data-augmentation/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn horizontal_flip(image: &[Vec<i32>]) -> Vec<Vec<i32>> {\n    let mut out = Vec::new();\n    for row in image {\n        let mut r = row.clone();\n        r.reverse();\n        out.push(r);\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "image-preprocessing",
      "title": "Image Preprocessing",
      "path": "modules/ml/computer-vision/image-preprocessing",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Image Preprocessing\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nPreprocessing normalizes pixel ranges for stable training.\n\n## Math\n\n$$x' = x / 255$$\n\n## Function\n\n```python\ndef normalize_pixels(pixels: list[int]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/image-preprocessing/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/image-preprocessing/python/image_preprocessing.py",
          "language": "python",
          "content": "def normalize_pixels(pixels: list[int]) -> list[float]:\n    return [p / 255.0 for p in pixels]\n"
        },
        {
          "path": "modules/ml/computer-vision/image-preprocessing/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn normalize_pixels(pixels: &[i32]) -> Vec<f64> {\n    pixels.iter().map(|p| *p as f64 / 255.0).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "lenet-5",
      "title": "LeNet-5",
      "path": "modules/ml/computer-vision/lenet-5",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# LeNet-5\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nLeNet-5 is a classic CNN architecture with characteristic layer design.\n\n## Math\n\n$$\\text{Architecture defined by ordered layers.}$$\n\n## Function\n\n```python\ndef layers() -> list[str]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/lenet-5/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/lenet-5/python/lenet_5.py",
          "language": "python",
          "content": "def layers() -> list[str]:\n    return [\"conv\", \"relu\", \"pool\", \"fc\"]\n"
        },
        {
          "path": "modules/ml/computer-vision/lenet-5/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn layers() -> Vec<&'static str> {\n    vec![\"conv\", \"relu\", \"pool\", \"fc\"]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "non-maximum-suppression",
      "title": "Non-Maximum Suppression",
      "path": "modules/ml/computer-vision/non-maximum-suppression",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Non-Maximum Suppression\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nNMS removes overlapping boxes with lower scores.\n\n## Math\n\n$$Keep highest score; suppress IoU > threshold.$$\n\n## Function\n\n```python\ndef iou(box_a: tuple[float, float, float, float], box_b: tuple[float, float, float, float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/non-maximum-suppression/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/non-maximum-suppression/python/non_maximum_suppression.py",
          "language": "python",
          "content": "def iou(box_a: tuple[float, float, float, float], box_b: tuple[float, float, float, float]) -> float:\n    ax1, ay1, ax2, ay2 = box_a\n    bx1, by1, bx2, by2 = box_b\n    inter_x1 = max(ax1, bx1)\n    inter_y1 = max(ay1, by1)\n    inter_x2 = min(ax2, bx2)\n    inter_y2 = min(ay2, by2)\n    if inter_x2 <= inter_x1 or inter_y2 <= inter_y1:\n        return 0.0\n    inter = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)\n    area_a = (ax2 - ax1) * (ay2 - ay1)\n    area_b = (bx2 - bx1) * (by2 - by1)\n    return inter / (area_a + area_b - inter)\n"
        },
        {
          "path": "modules/ml/computer-vision/non-maximum-suppression/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn iou(a: (f64, f64, f64, f64), b: (f64, f64, f64, f64)) -> f64 {\n    let (ax1, ay1, ax2, ay2) = a;\n    let (bx1, by1, bx2, by2) = b;\n    let inter_x1 = ax1.max(bx1);\n    let inter_y1 = ay1.max(by1);\n    let inter_x2 = ax2.min(bx2);\n    let inter_y2 = ay2.min(by2);\n    if inter_x2 <= inter_x1 || inter_y2 <= inter_y1 {\n        return 0.0;\n    }\n    let inter = (inter_x2 - inter_x1) * (inter_y2 - inter_y1);\n    let area_a = (ax2 - ax1) * (ay2 - ay1);\n    let area_b = (bx2 - bx1) * (by2 - by1);\n    inter / (area_a + area_b - inter)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "optical-flow-epe",
      "title": "Optical Flow (EPE)",
      "path": "modules/ml/computer-vision/optical-flow-epe",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Optical Flow (EPE)\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nEndpoint error measures flow prediction error.\n\n## Math\n\n$$\\mathrm{EPE} = \\sqrt{(u-u^*)^2 + (v-v^*)^2}$$\n\n## Function\n\n```python\ndef epe(pred: tuple[float, float], target: tuple[float, float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/optical-flow-epe/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/optical-flow-epe/python/optical_flow_epe.py",
          "language": "python",
          "content": "def epe(pred: tuple[float, float], target: tuple[float, float]) -> float:\n    return ((pred[0] - target[0]) ** 2 + (pred[1] - target[1]) ** 2) ** 0.5\n"
        },
        {
          "path": "modules/ml/computer-vision/optical-flow-epe/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn epe(pred: (f64, f64), target: (f64, f64)) -> f64 {\n    let dx = pred.0 - target.0;\n    let dy = pred.1 - target.1;\n    (dx * dx + dy * dy).sqrt()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "pooling",
      "title": "Pooling (Max/Average)",
      "path": "modules/ml/computer-vision/pooling",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Pooling (Max/Average)\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nPooling downsamples by taking max or average over windows.\n\n## Math\n\n$$\n\\begin{aligned}\ny_{\\max} &= \\max_{i \\in \\mathcal{W}} x_i \\\\\ny_{\\text{avg}} &= \\frac{1}{|\\mathcal{W}|}\\sum_{i \\in \\mathcal{W}} x_i\n\\end{aligned}\n$$\n\n## Function\n\n```python\ndef max_pool(window: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/pooling/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/pooling/python/pooling.py",
          "language": "python",
          "content": "def max_pool(window: list[float]) -> float:\n    return max(window)\n\n\ndef avg_pool(window: list[float]) -> float:\n    return sum(window) / len(window)\n"
        },
        {
          "path": "modules/ml/computer-vision/pooling/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn max_pool(window: &[f64]) -> f64 {\n    window.iter().copied().fold(f64::NEG_INFINITY, f64::max)\n}\n\npub fn avg_pool(window: &[f64]) -> f64 {\n    window.iter().sum::<f64>() / window.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "resnet",
      "title": "ResNet",
      "path": "modules/ml/computer-vision/resnet",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# ResNet\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nResNet is a classic CNN architecture with characteristic layer design.\n\n## Math\n\n$$\\text{Architecture defined by ordered layers.}$$\n\n## Function\n\n```python\ndef layers() -> list[str]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/resnet/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/resnet/python/resnet.py",
          "language": "python",
          "content": "def layers() -> list[str]:\n    return [\"conv\", \"relu\", \"pool\", \"fc\"]\n"
        },
        {
          "path": "modules/ml/computer-vision/resnet/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn layers() -> Vec<&'static str> {\n    vec![\"conv\", \"relu\", \"pool\", \"fc\"]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "rgb-to-grayscale",
      "title": "RGB to Grayscale",
      "path": "modules/ml/computer-vision/rgb-to-grayscale",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# RGB to Grayscale\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nConvert RGB to grayscale using luminance weights.\n\n## Math\n\n$$Y = 0.299R + 0.587G + 0.114B$$\n\n## Function\n\n```python\ndef rgb_to_gray(r: int, g: int, b: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/rgb-to-grayscale/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/rgb-to-grayscale/python/rgb_to_grayscale.py",
          "language": "python",
          "content": "def rgb_to_gray(r: int, g: int, b: int) -> float:\n    return 0.299 * r + 0.587 * g + 0.114 * b\n"
        },
        {
          "path": "modules/ml/computer-vision/rgb-to-grayscale/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn rgb_to_gray(r: i32, g: i32, b: i32) -> f64 {\n    0.299 * r as f64 + 0.587 * g as f64 + 0.114 * b as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "sobel-edge-detection",
      "title": "Sobel Edge Detection",
      "path": "modules/ml/computer-vision/sobel-edge-detection",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Sobel Edge Detection\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nSobel filters estimate image gradients.\n\n## Math\n\n$$Gx = Kx * I, Gy = Ky * I$$\n\n## Function\n\n```python\ndef sobel_center(patch: list[list[float]]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/sobel-edge-detection/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/sobel-edge-detection/python/sobel_edge_detection.py",
          "language": "python",
          "content": "def sobel_center(patch: list[list[float]]) -> float:\n    gx = (\n        -1 * patch[0][0] + 0 * patch[0][1] + 1 * patch[0][2]\n        -2 * patch[1][0] + 0 * patch[1][1] + 2 * patch[1][2]\n        -1 * patch[2][0] + 0 * patch[2][1] + 1 * patch[2][2]\n    )\n    gy = (\n        -1 * patch[0][0] -2 * patch[0][1] -1 * patch[0][2]\n         0 * patch[1][0] +0 * patch[1][1] +0 * patch[1][2]\n         1 * patch[2][0] +2 * patch[2][1] +1 * patch[2][2]\n    )\n    return (gx ** 2 + gy ** 2) ** 0.5\n"
        },
        {
          "path": "modules/ml/computer-vision/sobel-edge-detection/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn sobel_center(patch: &[Vec<f64>]) -> f64 {\n    let gx = -1.0 * patch[0][0] + 1.0 * patch[0][2]\n        -2.0 * patch[1][0] + 2.0 * patch[1][2]\n        -1.0 * patch[2][0] + 1.0 * patch[2][2];\n    let gy = -1.0 * patch[0][0] - 2.0 * patch[0][1] - 1.0 * patch[0][2]\n        + 1.0 * patch[2][0] + 2.0 * patch[2][1] + 1.0 * patch[2][2];\n    (gx * gx + gy * gy).sqrt()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "vggnet",
      "title": "VGGNet",
      "path": "modules/ml/computer-vision/vggnet",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# VGGNet\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nVGGNet is a classic CNN architecture with characteristic layer design.\n\n## Math\n\n$$\\text{Architecture defined by ordered layers.}$$\n\n## Function\n\n```python\ndef layers() -> list[str]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/vggnet/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/vggnet/python/vggnet.py",
          "language": "python",
          "content": "def layers() -> list[str]:\n    return [\"conv\", \"relu\", \"pool\", \"fc\"]\n"
        },
        {
          "path": "modules/ml/computer-vision/vggnet/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn layers() -> Vec<&'static str> {\n    vec![\"conv\", \"relu\", \"pool\", \"fc\"]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "data",
      "slug": "batch-iterator",
      "title": "Batch Iterator",
      "path": "modules/ml/data/batch-iterator",
      "summary": "> Track: `ml` | Topic: `data`",
      "readme": "# Batch Iterator\n\n> Track: `ml` | Topic: `data`\n\n## Concept\n\nA batch iterator yields index ranges for mini-batches.\n\n## Math\n\n$$b_i = \\left[iB, \\min\\left((i+1)B, N\\right)\\right)$$\n\n## Function\n\n```python\ndef batch_indices(n: int, batch_size: int) -> list[list[int]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/data/batch-iterator/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/data/batch-iterator/python/batch_iterator.py",
          "language": "python",
          "content": "def batch_indices(n: int, batch_size: int) -> list[list[int]]:\n    batches = []\n    for start in range(0, n, batch_size):\n        batches.append(list(range(start, min(n, start + batch_size))))\n    return batches\n"
        },
        {
          "path": "modules/ml/data/batch-iterator/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn batch_indices(n: usize, batch_size: usize) -> Vec<Vec<usize>> {\n    let mut batches = Vec::new();\n    let mut start = 0;\n    while start < n {\n        let end = usize::min(n, start + batch_size);\n        batches.push((start..end).collect());\n        start += batch_size;\n    }\n    batches\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "data",
      "slug": "class-imbalance",
      "title": "Handling Class Imbalance",
      "path": "modules/ml/data/class-imbalance",
      "summary": "> Track: `ml` | Topic: `data`",
      "readme": "# Handling Class Imbalance\n\n> Track: `ml` | Topic: `data`\n\n## Concept\n\nCompute class weights to rebalance loss contributions.\n\n## Math\n\n$$w_c = N / (K * count_c)$$\n\n## Function\n\n```python\ndef class_weights(labels: list[int]) -> dict[int, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/data/class-imbalance/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/data/class-imbalance/python/class_imbalance.py",
          "language": "python",
          "content": "from collections import Counter\n\n\ndef class_weights(labels: list[int]) -> dict[int, float]:\n    counts = Counter(labels)\n    n = len(labels)\n    k = len(counts)\n    return {cls: n / (k * cnt) for cls, cnt in counts.items()}\n"
        },
        {
          "path": "modules/ml/data/class-imbalance/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashMap;\n\npub fn class_weights(labels: &[i32]) -> HashMap<i32, f64> {\n    let mut counts: HashMap<i32, usize> = HashMap::new();\n    for label in labels {\n        *counts.entry(*label).or_insert(0) += 1;\n    }\n    let n = labels.len() as f64;\n    let k = counts.len() as f64;\n    let mut weights = HashMap::new();\n    for (cls, cnt) in counts {\n        weights.insert(cls, n / (k * cnt as f64));\n    }\n    weights\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "data",
      "slug": "data-leakage",
      "title": "Data Leakage",
      "path": "modules/ml/data/data-leakage",
      "summary": "> Track: `ml` | Topic: `data`",
      "readme": "# Data Leakage\n\n> Track: `ml` | Topic: `data`\n\n## Concept\n\nLeakage happens when train and test data overlap or share future info.\n\n## Math\n\n$$Leakage if |train ∩ test| > 0.$$\n\n## Function\n\n```python\ndef has_leakage(train_ids: list[int], test_ids: list[int]) -> bool:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/data/data-leakage/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/data/data-leakage/python/data_leakage.py",
          "language": "python",
          "content": "def has_leakage(train_ids: list[int], test_ids: list[int]) -> bool:\n    return bool(set(train_ids) & set(test_ids))\n"
        },
        {
          "path": "modules/ml/data/data-leakage/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashSet;\n\npub fn has_leakage(train_ids: &[i32], test_ids: &[i32]) -> bool {\n    let train: HashSet<i32> = train_ids.iter().copied().collect();\n    test_ids.iter().any(|id| train.contains(id))\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "data",
      "slug": "dataset-batch-epoch",
      "title": "Dataset vs Batch vs Epoch",
      "path": "modules/ml/data/dataset-batch-epoch",
      "summary": "> Track: `ml` | Topic: `data`",
      "readme": "# Dataset vs Batch vs Epoch\n\n> Track: `ml` | Topic: `data`\n\n## Concept\n\nA dataset is the full collection; a batch is a subset; an epoch is one full pass.\n\n## Math\n\n$$num_batches = ceil(dataset_size / batch_size)$$\n\n## Function\n\n```python\ndef num_batches(dataset_size: int, batch_size: int) -> int:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/data/dataset-batch-epoch/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/data/dataset-batch-epoch/python/dataset_batch_epoch.py",
          "language": "python",
          "content": "import math\n\n\ndef num_batches(dataset_size: int, batch_size: int) -> int:\n    return math.ceil(dataset_size / batch_size)\n"
        },
        {
          "path": "modules/ml/data/dataset-batch-epoch/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn num_batches(dataset_size: usize, batch_size: usize) -> usize {\n    (dataset_size + batch_size - 1) / batch_size\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "data",
      "slug": "polynomial-features",
      "title": "Polynomial Feature Expansion",
      "path": "modules/ml/data/polynomial-features",
      "summary": "> Track: `ml` | Topic: `data`",
      "readme": "# Polynomial Feature Expansion\n\n> Track: `ml` | Topic: `data`\n\n## Concept\n\nPolynomial features add powers of input to increase model capacity.\n\n## Math\n\n$$phi(x) = [x, x^2,..., x^d]$$\n\n## Function\n\n```python\ndef poly_features(x: float, degree: int) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/data/polynomial-features/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/data/polynomial-features/python/polynomial_features.py",
          "language": "python",
          "content": "def poly_features(x: float, degree: int) -> list[float]:\n    return [x ** d for d in range(1, degree + 1)]\n"
        },
        {
          "path": "modules/ml/data/polynomial-features/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn poly_features(x: f64, degree: usize) -> Vec<f64> {\n    (1..=degree).map(|d| x.powi(d as i32)).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "data",
      "slug": "stratified-split",
      "title": "Stratified Split",
      "path": "modules/ml/data/stratified-split",
      "summary": "> Track: `ml` | Topic: `data`",
      "readme": "# Stratified Split\n\n> Track: `ml` | Topic: `data`\n\n## Concept\n\nStratified splits preserve class proportions across splits.\n\n## Math\n\n$$\\text{Sample indices per class with same ratio.}$$\n\n## Function\n\n```python\ndef stratified_split(labels: list[int], train_frac: float) -> tuple[list[int], list[int]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/data/stratified-split/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/data/stratified-split/python/stratified_split.py",
          "language": "python",
          "content": "from collections import defaultdict\n\n\ndef stratified_split(labels: list[int], train_frac: float) -> tuple[list[int], list[int]]:\n    buckets = defaultdict(list)\n    for idx, label in enumerate(labels):\n        buckets[label].append(idx)\n    train, test = [], []\n    for idxs in buckets.values():\n        cut = int(len(idxs) * train_frac)\n        train.extend(idxs[:cut])\n        test.extend(idxs[cut:])\n    return train, test\n"
        },
        {
          "path": "modules/ml/data/stratified-split/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashMap;\n\npub fn stratified_split(labels: &[i32], train_frac: f64) -> (Vec<usize>, Vec<usize>) {\n    let mut buckets: HashMap<i32, Vec<usize>> = HashMap::new();\n    for (idx, label) in labels.iter().enumerate() {\n        buckets.entry(*label).or_default().push(idx);\n    }\n    let mut train = Vec::new();\n    let mut test = Vec::new();\n    for idxs in buckets.values() {\n        let cut = (idxs.len() as f64 * train_frac) as usize;\n        train.extend_from_slice(&idxs[..cut]);\n        test.extend_from_slice(&idxs[cut..]);\n    }\n    (train, test)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "data",
      "slug": "train-validation-test-split",
      "title": "Train/Validation/Test Split",
      "path": "modules/ml/data/train-validation-test-split",
      "summary": "> Track: `ml` | Topic: `data`",
      "readme": "# Train/Validation/Test Split\n\n> Track: `ml` | Topic: `data`\n\n## Concept\n\nSplit indices into train, validation, and test partitions.\n\n## Math\n\n$$n_train = floor(N * train_frac), n_val = floor(N * val_frac)$$\n\n## Function\n\n```python\ndef split_indices(n: int, train_frac: float, val_frac: float) -> tuple[list[int], list[int], list[int]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/data/train-validation-test-split/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/data/train-validation-test-split/python/train_validation_test_split.py",
          "language": "python",
          "content": "def split_indices(n: int, train_frac: float, val_frac: float) -> tuple[list[int], list[int], list[int]]:\n    n_train = int(n * train_frac)\n    n_val = int(n * val_frac)\n    train = list(range(0, n_train))\n    val = list(range(n_train, n_train + n_val))\n    test = list(range(n_train + n_val, n))\n    return train, val, test\n"
        },
        {
          "path": "modules/ml/data/train-validation-test-split/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn split_indices(n: usize, train_frac: f64, val_frac: f64) -> (Vec<usize>, Vec<usize>, Vec<usize>) {\n    let n_train = (n as f64 * train_frac).floor() as usize;\n    let n_val = (n as f64 * val_frac).floor() as usize;\n    let train: Vec<usize> = (0..n_train).collect();\n    let val: Vec<usize> = (n_train..(n_train + n_val)).collect();\n    let test: Vec<usize> = (n_train + n_val..n).collect();\n    (train, val, test)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "activation-failure-modes",
      "title": "Activation Failure Modes",
      "path": "modules/ml/deep-learning/activation-failure-modes",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Activation Failure Modes\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nDead ReLU and saturation reduce gradient flow.\n\n## Math\n\n$$|\\sigma'(x)| \\approx 0 \\quad \\text{for large } |x|$$\n\n## Function\n\n```python\ndef dead_relu_fraction(values: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/activation-failure-modes/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/activation-failure-modes/python/activation_failure_modes.py",
          "language": "python",
          "content": "def dead_relu_fraction(values: list[float]) -> float:\n    if not values:\n        return 0.0\n    dead = sum(1 for v in values if v <= 0.0)\n    return dead / len(values)\n"
        },
        {
          "path": "modules/ml/deep-learning/activation-failure-modes/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn dead_relu_fraction(values: &[f64]) -> f64 {\n    if values.is_empty() {\n        return 0.0;\n    }\n    let dead = values.iter().filter(|v| **v <= 0.0).count();\n    dead as f64 / values.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "activations-modern",
      "title": "Modern Activations",
      "path": "modules/ml/deep-learning/activations-modern",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Modern Activations\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nModern activations like GeLU, Swish, SwiGLU, and Mish improve expressiveness.\n\n## Math\n\n$$\n\\begin{aligned}\n\\mathrm{Swish}(x) &= x\\,\\sigma(x) \\\\\n\\mathrm{GeLU}(x) &\\approx 0.5x\\left(1+\\tanh\\left(\\sqrt{\\frac{2}{\\pi}}\\left(x+0.044715x^3\\right)\\right)\\right)\n\\end{aligned}\n$$\n\n## Function\n\n```python\ndef modern_activations(x: float) -> dict[str, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/activations-modern/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/activations-modern/python/activations_modern.py",
          "language": "python",
          "content": "import math\n\n\ndef modern_activations(x: float) -> dict[str, float]:\n    sigmoid = 1 / (1 + math.exp(-x))\n    swish = x * sigmoid\n    gelu = 0.5 * x * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3)))\n    mish = x * math.tanh(math.log1p(math.exp(x)))\n    swiglu = (x * sigmoid) * x\n    return {\"swish\": swish, \"gelu\": gelu, \"mish\": mish, \"swiglu\": swiglu}\n"
        },
        {
          "path": "modules/ml/deep-learning/activations-modern/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn modern_activations(x: f64) -> (f64, f64, f64, f64) {\n    let sigmoid = 1.0 / (1.0 + (-x).exp());\n    let swish = x * sigmoid;\n    let gelu = 0.5 * x * (1.0 + ( (2.0 / std::f64::consts::PI).sqrt() * (x + 0.044715 * x.powi(3)) ).tanh());\n    let mish = x * (1.0 + x.exp()).ln().tanh();\n    let swiglu = (x * sigmoid) * x;\n    (swish, gelu, mish, swiglu)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "activations-relu-family",
      "title": "ReLU Family",
      "path": "modules/ml/deep-learning/activations-relu-family",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# ReLU Family\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nReLU variants improve gradient flow by keeping nonzero slopes.\n\n## Math\n\n$$\\mathrm{ReLU}(x)=\\max(0,x),\\ \\mathrm{LeakyReLU}(x)=\\max(\\alpha x,x)$$\n\n## Function\n\n```python\ndef relu_family(x: float, alpha: float = 0.01) -> dict[str, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/activations-relu-family/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/activations-relu-family/python/activations_relu_family.py",
          "language": "python",
          "content": "def relu_family(x: float, alpha: float = 0.01) -> dict[str, float]:\n    relu = max(0.0, x)\n    leaky = x if x > 0 else alpha * x\n    elu = x if x > 0 else alpha * (pow(2.718281828, x) - 1)\n    prelu = x if x > 0 else alpha * x\n    return {\"relu\": relu, \"leaky_relu\": leaky, \"elu\": elu, \"prelu\": prelu}\n"
        },
        {
          "path": "modules/ml/deep-learning/activations-relu-family/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn relu_family(x: f64, alpha: f64) -> (f64, f64, f64, f64) {\n    let relu = x.max(0.0);\n    let leaky = if x > 0.0 { x } else { alpha * x };\n    let elu = if x > 0.0 { x } else { alpha * (x.exp() - 1.0) };\n    let prelu = if x > 0.0 { x } else { alpha * x };\n    (relu, leaky, elu, prelu)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "activations-sigmoid-tanh",
      "title": "Sigmoid/Tanh and Hard Variants",
      "path": "modules/ml/deep-learning/activations-sigmoid-tanh",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Sigmoid/Tanh and Hard Variants\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nSigmoid and tanh are smooth activations; hard variants approximate them for efficiency.\nDynamic Tanh (DyT) adds learnable scale/bias parameters to tanh.\n\n## Math\n\n- $\\sigma(x)=\\frac{1}{1+e^{-x}}$\n- $\\tanh(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$\n- $\\mathrm{hard\\_sigmoid}(x)=\\operatorname{clip}(0.2x+0.5, 0, 1)$\n- $\\mathrm{hardtanh}(x)=\\operatorname{clip}(x, -1, 1)$\n- $\\mathrm{dyt}(x)=\\gamma\\,\\tanh(\\alpha x)+\\beta$\n\n## Function\n\n```python\ndef activations(x: float) -> dict[str, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/activations-sigmoid-tanh/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/activations-sigmoid-tanh/python/activations_sigmoid_tanh.py",
          "language": "python",
          "content": "import math\n\n\ndef dynamic_tanh(x: float, alpha: float = 1.0, gamma: float = 1.0, beta: float = 0.0) -> float:\n    \"\"\"Dynamic Tanh (DyT): gamma * tanh(alpha * x) + beta.\"\"\"\n    return gamma * math.tanh(alpha * x) + beta\n\n\ndef activations(x: float) -> dict[str, float]:\n    sigmoid = 1 / (1 + math.exp(-x))\n    tanh = math.tanh(x)\n    hard_sigmoid = max(0.0, min(1.0, 0.2 * x + 0.5))\n    hardtanh = max(-1.0, min(1.0, x))\n    dynamic = dynamic_tanh(x, alpha=1.2, gamma=1.0, beta=0.0)\n    return {\n        \"sigmoid\": sigmoid,\n        \"tanh\": tanh,\n        \"hard_sigmoid\": hard_sigmoid,\n        \"hardtanh\": hardtanh,\n        \"dynamic_tanh\": dynamic,\n    }\n"
        },
        {
          "path": "modules/ml/deep-learning/activations-sigmoid-tanh/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn dynamic_tanh(x: f64, alpha: f64, gamma: f64, beta: f64) -> f64 {\n    gamma * (alpha * x).tanh() + beta\n}\n\npub fn activations(x: f64) -> (f64, f64, f64, f64, f64) {\n    let sigmoid = 1.0 / (1.0 + (-x).exp());\n    let tanh = x.tanh();\n    let hard_sigmoid = (0.2 * x + 0.5).clamp(0.0, 1.0);\n    let hardtanh = x.clamp(-1.0, 1.0);\n    let dynamic = dynamic_tanh(x, 1.2, 1.0, 0.0);\n    (sigmoid, tanh, hard_sigmoid, hardtanh, dynamic)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "activations-softmax-softplus-softsign",
      "title": "Softmax, Softplus, Softsign",
      "path": "modules/ml/deep-learning/activations-softmax-softplus-softsign",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Softmax, Softplus, Softsign\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nSoftmax normalizes logits, softplus smooths ReLU, softsign saturates gently.\n\n## Math\n\n$$softplus(x)=log(1+e^x), softsign(x)=x/(1+|x|)$$\n\n## Function\n\n```python\ndef softmax(row: list[float]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/activations-softmax-softplus-softsign/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/activations-softmax-softplus-softsign/python/activations_softmax_softplus_softsign.py",
          "language": "python",
          "content": "import math\n\n\ndef softmax(row: list[float]) -> list[float]:\n    m = max(row)\n    exps = [math.exp(x - m) for x in row]\n    s = sum(exps)\n    return [e / s for e in exps]\n\n\ndef softplus(x: float) -> float:\n    return math.log1p(math.exp(x))\n\n\ndef softsign(x: float) -> float:\n    return x / (1 + abs(x))\n"
        },
        {
          "path": "modules/ml/deep-learning/activations-softmax-softplus-softsign/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn softmax(row: &[f64]) -> Vec<f64> {\n    let m = row.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let exps: Vec<f64> = row.iter().map(|x| (x - m).exp()).collect();\n    let sum: f64 = exps.iter().sum();\n    exps.iter().map(|e| e / sum).collect()\n}\n\npub fn softplus(x: f64) -> f64 {\n    (1.0 + x.exp()).ln()\n}\n\npub fn softsign(x: f64) -> f64 {\n    x / (1.0 + x.abs())\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "automatic-differentiation",
      "title": "Automatic Differentiation",
      "path": "modules/ml/deep-learning/automatic-differentiation",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Automatic Differentiation\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nAutodiff computes derivatives by composing local gradients.\n\n## Math\n\n$$If y = x^2 and z = y + 3, then dz/dx = 2x.$$\n\n## Function\n\n```python\ndef forward_grad(x: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/automatic-differentiation/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/automatic-differentiation/python/automatic_differentiation.py",
          "language": "python",
          "content": "def forward_grad(x: float) -> float:\n    return 2.0 * x\n"
        },
        {
          "path": "modules/ml/deep-learning/automatic-differentiation/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn forward_grad(x: f64) -> f64 {\n    2.0 * x\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "backpropagation",
      "title": "Backpropagation",
      "path": "modules/ml/deep-learning/backpropagation",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Backpropagation\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nBackprop computes gradients using the chain rule from output to inputs.\n\n## Math\n\n$$For z = w x, dL/dw = dL/dz * x.$$\n\n## Function\n\n```python\ndef linear_backprop(x: float, w: float, grad_out: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/backpropagation/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/backpropagation/python/backpropagation.py",
          "language": "python",
          "content": "def linear_backprop(x: float, w: float, grad_out: float) -> float:\n    return grad_out * x\n"
        },
        {
          "path": "modules/ml/deep-learning/backpropagation/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn linear_backprop(x: f64, grad_out: f64) -> f64 {\n    grad_out * x\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "batchnorm-transformers",
      "title": "Why BatchNorm is Bad for Transformers",
      "path": "modules/ml/deep-learning/batchnorm-transformers",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Why BatchNorm is Bad for Transformers\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nBatchNorm mixes statistics across batch/time, which conflicts with sequence modeling.\n\n## Math\n\n$$\\mu_B = \\frac{1}{BT}\\sum_{b=1}^{B}\\sum_{t=1}^{T} x_{b,t}$$\n\n## Function\n\n```python\ndef batch_stats(matrix: list[list[float]]) -> tuple[float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/batchnorm-transformers/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/batchnorm-transformers/python/batchnorm_transformers.py",
          "language": "python",
          "content": "def batch_stats(matrix: list[list[float]]) -> tuple[float, float]:\n    values = [v for row in matrix for v in row]\n    mean = sum(values) / len(values)\n    var = sum((v - mean) ** 2 for v in values) / len(values)\n    return mean, var\n"
        },
        {
          "path": "modules/ml/deep-learning/batchnorm-transformers/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn batch_stats(matrix: &[Vec<f64>]) -> (f64, f64) {\n    let mut values = Vec::new();\n    for row in matrix {\n        for v in row {\n            values.push(*v);\n        }\n    }\n    let mean: f64 = values.iter().sum::<f64>() / values.len() as f64;\n    let var: f64 = values.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / values.len() as f64;\n    (mean, var)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "batchnorm",
      "title": "BatchNorm",
      "path": "modules/ml/deep-learning/batchnorm",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# BatchNorm\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nBatchNorm normalizes activations across the batch.\n\n## Math\n\n$$y = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n\n## Function\n\n```python\ndef batchnorm(x: list[float], eps: float = 1e-5) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/batchnorm/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/batchnorm/python/batchnorm.py",
          "language": "python",
          "content": "import math\n\n\ndef batchnorm(x: list[float], eps: float = 1e-5) -> list[float]:\n    mean = sum(x) / len(x)\n    var = sum((v - mean) ** 2 for v in x) / len(x)\n    return [(v - mean) / math.sqrt(var + eps) for v in x]\n"
        },
        {
          "path": "modules/ml/deep-learning/batchnorm/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn batchnorm(x: &[f64], eps: f64) -> Vec<f64> {\n    let mean: f64 = x.iter().sum::<f64>() / x.len() as f64;\n    let var: f64 = x.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / x.len() as f64;\n    x.iter().map(|v| (v - mean) / (var + eps).sqrt()).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "cross-entropy",
      "title": "Cross-Entropy Loss",
      "path": "modules/ml/deep-learning/cross-entropy",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Cross-Entropy Loss\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nCross-entropy compares a predicted distribution to a target class.\n\n## Math\n\n$$L = -\\log\\left(\\mathrm{softmax}(\\text{logits})_{\\text{target}}\\right)$$\n\n## Function\n\n```python\ndef cross_entropy(logits: list[float], target: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/cross-entropy/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/cross-entropy/python/cross_entropy.py",
          "language": "python",
          "content": "import math\n\n\ndef cross_entropy(logits: list[float], target: int) -> float:\n    m = max(logits)\n    exps = [math.exp(x - m) for x in logits]\n    s = sum(exps)\n    prob = exps[target] / s\n    return -math.log(prob)\n"
        },
        {
          "path": "modules/ml/deep-learning/cross-entropy/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn cross_entropy(logits: &[f64], target: usize) -> f64 {\n    let m = logits.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let exps: Vec<f64> = logits.iter().map(|x| (x - m).exp()).collect();\n    let sum: f64 = exps.iter().sum();\n    let prob = exps[target] / sum;\n    -prob.ln()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "dropout",
      "title": "Dropout",
      "path": "modules/ml/deep-learning/dropout",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Dropout\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nDropout randomly zeroes activations during training to reduce co-adaptation.\n\n## Math\n\n$$x' = mask * x / (1-p)$$\n\n## Function\n\n```python\ndef dropout(x: list[float], p: float, seed: int = 0) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/dropout/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/dropout/python/dropout.py",
          "language": "python",
          "content": "def dropout(x: list[float], p: float, seed: int = 0) -> list[float]:\n    out = []\n    state = seed\n    for v in x:\n        state = (1103515245 * state + 12345) % (2**31)\n        keep = (state / (2**31 - 1)) > p\n        out.append(v / (1 - p) if keep else 0.0)\n    return out\n"
        },
        {
          "path": "modules/ml/deep-learning/dropout/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn dropout(x: &[f64], p: f64, seed: u64) -> Vec<f64> {\n    let mut out = Vec::with_capacity(x.len());\n    let mut state = seed;\n    for &v in x {\n        state = (1103515245u64.wrapping_mul(state).wrapping_add(12345)) % (1u64 << 31);\n        let keep = (state as f64 / ((1u64 << 31) - 1) as f64) > p;\n        if keep {\n            out.push(v / (1.0 - p));\n        } else {\n            out.push(0.0);\n        }\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "early-stopping",
      "title": "Early Stopping",
      "path": "modules/ml/deep-learning/early-stopping",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Early Stopping\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nStop training when validation loss stops improving.\n\n## Math\n\n$$t - t_{\\text{best}} \\ge P$$\n\n## Function\n\n```python\ndef should_stop(losses: list[float], patience: int) -> bool:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/early-stopping/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/early-stopping/python/early_stopping.py",
          "language": "python",
          "content": "def should_stop(losses: list[float], patience: int) -> bool:\n    best = float(\"inf\")\n    bad = 0\n    for loss in losses:\n        if loss < best:\n            best = loss\n            bad = 0\n        else:\n            bad += 1\n            if bad >= patience:\n                return True\n    return False\n"
        },
        {
          "path": "modules/ml/deep-learning/early-stopping/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn should_stop(losses: &[f64], patience: usize) -> bool {\n    let mut best = f64::INFINITY;\n    let mut bad = 0;\n    for &loss in losses {\n        if loss < best {\n            best = loss;\n            bad = 0;\n        } else {\n            bad += 1;\n            if bad >= patience {\n                return true;\n            }\n        }\n    }\n    false\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "feedforward-neural-network",
      "title": "Feedforward Neural Network",
      "path": "modules/ml/deep-learning/feedforward-neural-network",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Feedforward Neural Network\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nA feedforward network applies a sequence of linear layers and activations.\n\n## Math\n\n$$For layer i: h_i = sigma(W_i h_{i-1} + b_i).$$\n\n## Function\n\n```python\ndef feedforward(x: list[float], weights: list[list[list[float]]], biases: list[list[float]]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/feedforward-neural-network/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/feedforward-neural-network/python/feedforward_neural_network.py",
          "language": "python",
          "content": "from __future__ import annotations\n\n\ndef _matvec(w: list[list[float]], x: list[float]) -> list[float]:\n    return [sum(w_row[j] * x[j] for j in range(len(x))) for w_row in w]\n\n\ndef _relu(x: list[float]) -> list[float]:\n    return [max(0.0, v) for v in x]\n\n\ndef feedforward(\n    x: list[float],\n    weights: list[list[list[float]]],\n    biases: list[list[float]],\n) -> list[float]:\n    h = x\n    for w, b in zip(weights, biases):\n        h = [v + b[i] for i, v in enumerate(_matvec(w, h))]\n        h = _relu(h)\n    return h\n"
        },
        {
          "path": "modules/ml/deep-learning/feedforward-neural-network/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn feedforward(\n    x: &[f64],\n    weights: &[Vec<Vec<f64>>],\n    biases: &[Vec<f64>],\n) -> Vec<f64> {\n    let mut h: Vec<f64> = x.to_vec();\n    for (w, b) in weights.iter().zip(biases.iter()) {\n        let mut next = vec![0.0; w.len()];\n        for i in 0..w.len() {\n            let mut acc = 0.0;\n            for j in 0..h.len() {\n                acc += w[i][j] * h[j];\n            }\n            next[i] = (acc + b[i]).max(0.0);\n        }\n        h = next;\n    }\n    h\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "focal-loss",
      "title": "Focal Loss",
      "path": "modules/ml/deep-learning/focal-loss",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Focal Loss\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nFocal loss down-weights easy examples for imbalanced classification.\n\n## Math\n\n$$L = -(1-p)^{\\gamma} \\log(p)$$\n\n## Function\n\n```python\ndef focal_loss(p: float, gamma: float = 2.0) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/focal-loss/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/focal-loss/python/focal_loss.py",
          "language": "python",
          "content": "import math\n\n\ndef focal_loss(p: float, gamma: float = 2.0) -> float:\n    return -((1 - p) ** gamma) * math.log(p)\n"
        },
        {
          "path": "modules/ml/deep-learning/focal-loss/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn focal_loss(p: f64, gamma: f64) -> f64 {\n    -((1.0 - p).powf(gamma)) * p.ln()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "gradient-checking",
      "title": "Gradient Checking",
      "path": "modules/ml/deep-learning/gradient-checking",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Gradient Checking\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nFinite differences approximate gradients to validate backprop.\n\n## Math\n\n$$\\frac{df}{dx} \\approx \\frac{f(x+\\epsilon) - f(x-\\epsilon)}{2\\epsilon}$$\n\n## Function\n\n```python\ndef grad_check(f, x: float, eps: float = 1e-5) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/gradient-checking/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/gradient-checking/python/gradient_checking.py",
          "language": "python",
          "content": "def grad_check(f, x: float, eps: float = 1e-5) -> float:\n    return (f(x + eps) - f(x - eps)) / (2 * eps)\n"
        },
        {
          "path": "modules/ml/deep-learning/gradient-checking/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn grad_check(f: fn(f64) -> f64, x: f64, eps: f64) -> f64 {\n    (f(x + eps) - f(x - eps)) / (2.0 * eps)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "gradient-flow",
      "title": "Effect on Gradient Flow",
      "path": "modules/ml/deep-learning/gradient-flow",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Effect on Gradient Flow\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nInitialization affects gradient variance as it propagates through layers.\n\n## Math\n\n$$Var(g_{l}) ≈ Var(g_{l+1}) * Var(W_l)$$\n\n## Function\n\n```python\ndef propagate_variance(var: float, layer_vars: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/gradient-flow/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/gradient-flow/python/gradient_flow.py",
          "language": "python",
          "content": "def propagate_variance(var: float, layer_vars: list[float]) -> float:\n    out = var\n    for v in layer_vars:\n        out *= v\n    return out\n"
        },
        {
          "path": "modules/ml/deep-learning/gradient-flow/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn propagate_variance(var: f64, layer_vars: &[f64]) -> f64 {\n    let mut out = var;\n    for v in layer_vars {\n        out *= v;\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "groupnorm",
      "title": "GroupNorm",
      "path": "modules/ml/deep-learning/groupnorm",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# GroupNorm\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nGroupNorm splits channels into groups and normalizes within each group.\n\n## Math\n\n$$y = \\frac{x - \\mu_g}{\\sqrt{\\sigma_g^2 + \\epsilon}}$$\n\n## Function\n\n```python\ndef groupnorm(x: list[float], groups: int, eps: float = 1e-5) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/groupnorm/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/groupnorm/python/groupnorm.py",
          "language": "python",
          "content": "import math\n\n\ndef groupnorm(x: list[float], groups: int, eps: float = 1e-5) -> list[float]:\n    size = len(x)\n    group_size = size // groups\n    out = []\n    for g in range(groups):\n        chunk = x[g * group_size : (g + 1) * group_size]\n        mean = sum(chunk) / len(chunk)\n        var = sum((v - mean) ** 2 for v in chunk) / len(chunk)\n        out.extend([(v - mean) / math.sqrt(var + eps) for v in chunk])\n    return out\n"
        },
        {
          "path": "modules/ml/deep-learning/groupnorm/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn groupnorm(x: &[f64], groups: usize, eps: f64) -> Vec<f64> {\n    let size = x.len();\n    let group_size = size / groups;\n    let mut out = Vec::with_capacity(size);\n    for g in 0..groups {\n        let start = g * group_size;\n        let end = start + group_size;\n        let chunk = &x[start..end];\n        let mean: f64 = chunk.iter().sum::<f64>() / chunk.len() as f64;\n        let var: f64 = chunk.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / chunk.len() as f64;\n        out.extend(chunk.iter().map(|v| (v - mean) / (var + eps).sqrt()));\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "he-initialization",
      "title": "He Initialization",
      "path": "modules/ml/deep-learning/he-initialization",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# He Initialization\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nHe initialization keeps variance stable for ReLU activations. Demo uses deterministic\npseudo-random.\n\n## Math\n\n$$W \\sim \\mathcal{N}\\left(0, \\frac{2}{fan_{in}}\\right)$$\n\n## Function\n\n```python\ndef he_normal(fan_in: int, fan_out: int, seed: int = 0) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/he-initialization/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/he-initialization/python/he_initialization.py",
          "language": "python",
          "content": "import math\n\n\ndef he_normal(fan_in: int, fan_out: int, seed: int = 0) -> list[float]:\n    std = math.sqrt(2 / fan_in)\n    # Box-Muller with deterministic LCG\n    values = []\n    state = seed\n    for _ in range(fan_in * fan_out // 2 + 1):\n        state = (1103515245 * state + 12345) % (2**31)\n        u1 = (state / (2**31 - 1)) or 1e-6\n        state = (1103515245 * state + 12345) % (2**31)\n        u2 = state / (2**31 - 1)\n        z0 = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        z1 = math.sqrt(-2 * math.log(u1)) * math.sin(2 * math.pi * u2)\n        values.extend([z0 * std, z1 * std])\n    return values[: fan_in * fan_out]\n"
        },
        {
          "path": "modules/ml/deep-learning/he-initialization/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn he_normal(fan_in: usize, fan_out: usize, seed: u64) -> Vec<f64> {\n    let std = (2.0 / fan_in as f64).sqrt();\n    let mut values = Vec::new();\n    let mut state = seed;\n    while values.len() < fan_in * fan_out {\n        state = (1103515245u64.wrapping_mul(state).wrapping_add(12345)) % (1u64 << 31);\n        let u1 = (state as f64 / ((1u64 << 31) - 1) as f64).max(1e-6);\n        state = (1103515245u64.wrapping_mul(state).wrapping_add(12345)) % (1u64 << 31);\n        let u2 = state as f64 / ((1u64 << 31) - 1) as f64;\n        let z0 = (-2.0 * u1.ln()).sqrt() * (2.0 * std::f64::consts::PI * u2).cos();\n        let z1 = (-2.0 * u1.ln()).sqrt() * (2.0 * std::f64::consts::PI * u2).sin();\n        values.push(z0 * std);\n        if values.len() < fan_in * fan_out {\n            values.push(z1 * std);\n        }\n    }\n    values\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "hinge-loss",
      "title": "Hinge Loss",
      "path": "modules/ml/deep-learning/hinge-loss",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Hinge Loss\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nHinge loss encourages a margin between correct and incorrect classes.\n\n## Math\n\n$$L = \\max(0, 1 - y \\cdot \\text{score})$$\n\n## Function\n\n```python\ndef hinge_loss(score: float, label: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/hinge-loss/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/hinge-loss/python/hinge_loss.py",
          "language": "python",
          "content": "def hinge_loss(score: float, label: int) -> float:\n    return max(0.0, 1 - label * score)\n"
        },
        {
          "path": "modules/ml/deep-learning/hinge-loss/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn hinge_loss(score: f64, label: i32) -> f64 {\n    let val = 1.0 - (label as f64) * score;\n    if val > 0.0 { val } else { 0.0 }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "huber-loss",
      "title": "Huber Loss",
      "path": "modules/ml/deep-learning/huber-loss",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Huber Loss\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nHuber loss is quadratic near zero and linear for large errors.\n\n## Math\n\n$$\nL =\n\\begin{cases}\n\\frac{1}{2} e^2, & |e| \\le \\delta \\\\\n\\delta\\left(|e| - \\frac{1}{2}\\delta\\right), & \\text{otherwise}\n\\end{cases}\n$$\n\n## Function\n\n```python\ndef huber(y: float, y_hat: float, delta: float = 1.0) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/huber-loss/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/huber-loss/python/huber_loss.py",
          "language": "python",
          "content": "def huber(y: float, y_hat: float, delta: float = 1.0) -> float:\n    e = y - y_hat\n    if abs(e) <= delta:\n        return 0.5 * e * e\n    return delta * (abs(e) - 0.5 * delta)\n"
        },
        {
          "path": "modules/ml/deep-learning/huber-loss/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn huber(y: f64, y_hat: f64, delta: f64) -> f64 {\n    let e = y - y_hat;\n    if e.abs() <= delta {\n        0.5 * e * e\n    } else {\n        delta * (e.abs() - 0.5 * delta)\n    }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "instancenorm",
      "title": "InstanceNorm",
      "path": "modules/ml/deep-learning/instancenorm",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# InstanceNorm\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nInstanceNorm normalizes per-sample per-channel, often for vision.\n\n## Math\n\n$$y = \\frac{x - \\mu_I}{\\sqrt{\\sigma_I^2 + \\epsilon}}$$\n\n## Function\n\n```python\ndef instancenorm(x: list[float], eps: float = 1e-5) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/instancenorm/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/instancenorm/python/instancenorm.py",
          "language": "python",
          "content": "import math\n\n\ndef instancenorm(x: list[float], eps: float = 1e-5) -> list[float]:\n    mean = sum(x) / len(x)\n    var = sum((v - mean) ** 2 for v in x) / len(x)\n    return [(v - mean) / math.sqrt(var + eps) for v in x]\n"
        },
        {
          "path": "modules/ml/deep-learning/instancenorm/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn instancenorm(x: &[f64], eps: f64) -> Vec<f64> {\n    let mean: f64 = x.iter().sum::<f64>() / x.len() as f64;\n    let var: f64 = x.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / x.len() as f64;\n    x.iter().map(|v| (v - mean) / (var + eps).sqrt()).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "knowledge-distillation-loss",
      "title": "Knowledge Distillation Loss",
      "path": "modules/ml/deep-learning/knowledge-distillation-loss",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Knowledge Distillation Loss\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nDistillation matches student logits to teacher logits.\n\n## Math\n\n$$L = \\mathrm{KL}\\left(\\mathrm{softmax}\\left(\\frac{z_s}{T}\\right)\\,\\middle\\|\\,\\mathrm{softmax}\\left(\\frac{z_t}{T}\\right)\\right)$$\n\n## Function\n\n```python\ndef distill_loss(student: list[float], teacher: list[float], temp: float = 1.0) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/knowledge-distillation-loss/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/knowledge-distillation-loss/python/knowledge_distillation_loss.py",
          "language": "python",
          "content": "import math\n\n\ndef _softmax(logits: list[float], temp: float) -> list[float]:\n    m = max(logits)\n    exps = [math.exp((x - m) / temp) for x in logits]\n    s = sum(exps)\n    return [e / s for e in exps]\n\n\ndef distill_loss(student: list[float], teacher: list[float], temp: float = 1.0) -> float:\n    ps = _softmax(student, temp)\n    pt = _softmax(teacher, temp)\n    loss = 0.0\n    for p_s, p_t in zip(ps, pt):\n        if p_t > 0 and p_s > 0:\n            loss += p_t * math.log(p_t / p_s)\n    return loss\n"
        },
        {
          "path": "modules/ml/deep-learning/knowledge-distillation-loss/rust/src/lib.rs",
          "language": "rust",
          "content": "fn softmax(logits: &[f64], temp: f64) -> Vec<f64> {\n    let m = logits.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let exps: Vec<f64> = logits.iter().map(|x| ((x - m) / temp).exp()).collect();\n    let sum: f64 = exps.iter().sum();\n    exps.iter().map(|e| e / sum).collect()\n}\n\npub fn distill_loss(student: &[f64], teacher: &[f64], temp: f64) -> f64 {\n    let ps = softmax(student, temp);\n    let pt = softmax(teacher, temp);\n    let mut loss = 0.0;\n    for (p_s, p_t) in ps.iter().zip(pt.iter()) {\n        if *p_t > 0.0 && *p_s > 0.0 {\n            loss += p_t * (p_t / p_s).ln();\n        }\n    }\n    loss\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "l1-regularization",
      "title": "L1 Regularization",
      "path": "modules/ml/deep-learning/l1-regularization",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# L1 Regularization\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nL1 adds an absolute weight penalty to encourage sparsity. This is also known as **Lasso** regularization.\n\n## Math\n\n$$L = L_0 + \\lambda \\sum_i |w_i|$$\n\n## Function\n\n```python\ndef l1_penalty(weights: list[float], lam: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/l1-regularization/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/l1-regularization/python/l1_regularization.py",
          "language": "python",
          "content": "def l1_penalty(weights: list[float], lam: float) -> float:\n    return lam * sum(abs(w) for w in weights)\n"
        },
        {
          "path": "modules/ml/deep-learning/l1-regularization/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn l1_penalty(weights: &[f64], lam: f64) -> f64 {\n    lam * weights.iter().map(|w| w.abs()).sum::<f64>()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "l2-regularization",
      "title": "L2 Regularization",
      "path": "modules/ml/deep-learning/l2-regularization",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# L2 Regularization\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nL2 adds a squared weight penalty to discourage large weights. This is also known as **Ridge** regularization.\n\n## Math\n\n$$L = L_0 + \\lambda \\sum_i w_i^2$$\n\n## Function\n\n```python\ndef l2_penalty(weights: list[float], lam: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/l2-regularization/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/l2-regularization/python/l2_regularization.py",
          "language": "python",
          "content": "def l2_penalty(weights: list[float], lam: float) -> float:\n    return lam * sum(w * w for w in weights)\n"
        },
        {
          "path": "modules/ml/deep-learning/l2-regularization/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn l2_penalty(weights: &[f64], lam: f64) -> f64 {\n    lam * weights.iter().map(|w| w * w).sum::<f64>()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "layernorm",
      "title": "LayerNorm",
      "path": "modules/ml/deep-learning/layernorm",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# LayerNorm\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nLayerNorm normalizes across features within a single sample.\n\n## Math\n\n$$y = \\frac{x - \\mu_f}{\\sqrt{\\sigma_f^2 + \\epsilon}}$$\n\n## Function\n\n```python\ndef layernorm(x: list[float], eps: float = 1e-5) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/layernorm/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/layernorm/python/layernorm.py",
          "language": "python",
          "content": "import math\n\n\ndef layernorm(x: list[float], eps: float = 1e-5) -> list[float]:\n    mean = sum(x) / len(x)\n    var = sum((v - mean) ** 2 for v in x) / len(x)\n    return [(v - mean) / math.sqrt(var + eps) for v in x]\n"
        },
        {
          "path": "modules/ml/deep-learning/layernorm/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn layernorm(x: &[f64], eps: f64) -> Vec<f64> {\n    let mean: f64 = x.iter().sum::<f64>() / x.len() as f64;\n    let var: f64 = x.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / x.len() as f64;\n    x.iter().map(|v| (v - mean) / (var + eps).sqrt()).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "mae-loss",
      "title": "Mean Absolute Error",
      "path": "modules/ml/deep-learning/mae-loss",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Mean Absolute Error\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nMAE penalizes absolute residuals.\n\n## Math\n\n$$\\mathrm{MAE} = \\frac{1}{n}\\sum_i |y_i - \\hat{y}_i|$$\n\n## Function\n\n```python\ndef mae(y: list[float], y_hat: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/mae-loss/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/mae-loss/python/mae_loss.py",
          "language": "python",
          "content": "def mae(y: list[float], y_hat: list[float]) -> float:\n    return sum(abs(a - b) for a, b in zip(y, y_hat)) / len(y)\n"
        },
        {
          "path": "modules/ml/deep-learning/mae-loss/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn mae(y: &[f64], y_hat: &[f64]) -> f64 {\n    let sum: f64 = y.iter().zip(y_hat.iter()).map(|(a, b)| (a - b).abs()).sum();\n    sum / y.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "mse-loss",
      "title": "Mean Squared Error",
      "path": "modules/ml/deep-learning/mse-loss",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Mean Squared Error\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nMSE penalizes squared residuals.\n\n## Math\n\n$$\\mathrm{MSE} = \\frac{1}{n}\\sum_i (y_i - \\hat{y}_i)^2$$\n\n## Function\n\n```python\ndef mse(y: list[float], y_hat: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/mse-loss/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/mse-loss/python/mse_loss.py",
          "language": "python",
          "content": "def mse(y: list[float], y_hat: list[float]) -> float:\n    return sum((a - b) ** 2 for a, b in zip(y, y_hat)) / len(y)\n"
        },
        {
          "path": "modules/ml/deep-learning/mse-loss/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn mse(y: &[f64], y_hat: &[f64]) -> f64 {\n    let sum: f64 = y.iter().zip(y_hat.iter()).map(|(a, b)| (a - b).powi(2)).sum();\n    sum / y.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "neuron-weights-bias-activation",
      "title": "Neuron, Weights, Bias, Activation",
      "path": "modules/ml/deep-learning/neuron-weights-bias-activation",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Neuron, Weights, Bias, Activation\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nA neuron computes a weighted sum plus bias, then applies an activation.\n\n## Math\n\n$$y = sigma(w · x + b)$$\n\n## Function\n\n```python\ndef neuron(x: list[float], w: list[float], b: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/neuron-weights-bias-activation/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/neuron-weights-bias-activation/python/neuron_weights_bias_activation.py",
          "language": "python",
          "content": "import math\n\n\ndef neuron(x: list[float], w: list[float], b: float) -> float:\n    z = sum(wi * xi for wi, xi in zip(w, x)) + b\n    return 1 / (1 + math.exp(-z))\n"
        },
        {
          "path": "modules/ml/deep-learning/neuron-weights-bias-activation/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn neuron(x: &[f64], w: &[f64], b: f64) -> f64 {\n    let mut z = b;\n    for (wi, xi) in w.iter().zip(x.iter()) {\n        z += wi * xi;\n    }\n    1.0 / (1.0 + (-z).exp())\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "rmse-loss",
      "title": "Root Mean Squared Error",
      "path": "modules/ml/deep-learning/rmse-loss",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Root Mean Squared Error\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nRMSE is the square root of MSE.\n\n## Math\n\n$$\\mathrm{RMSE} = \\sqrt{\\mathrm{MSE}}$$\n\n## Function\n\n```python\ndef rmse(y: list[float], y_hat: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/rmse-loss/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/rmse-loss/python/rmse_loss.py",
          "language": "python",
          "content": "import math\n\n\ndef rmse(y: list[float], y_hat: list[float]) -> float:\n    mse = sum((a - b) ** 2 for a, b in zip(y, y_hat)) / len(y)\n    return math.sqrt(mse)\n"
        },
        {
          "path": "modules/ml/deep-learning/rmse-loss/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn rmse(y: &[f64], y_hat: &[f64]) -> f64 {\n    let sum: f64 = y.iter().zip(y_hat.iter()).map(|(a, b)| (a - b).powi(2)).sum();\n    (sum / y.len() as f64).sqrt()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "rmsnorm",
      "title": "RMSNorm",
      "path": "modules/ml/deep-learning/rmsnorm",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# RMSNorm\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nRMSNorm normalizes by root-mean-square without centering.\n\n## Math\n\n$$y = \\frac{x}{\\sqrt{\\frac{1}{d}\\sum_{i=1}^{d} x_i^2 + \\epsilon}}$$\n\n## Function\n\n```python\ndef rmsnorm(x: list[float], eps: float = 1e-5) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/rmsnorm/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/rmsnorm/python/rmsnorm.py",
          "language": "python",
          "content": "import math\n\n\ndef rmsnorm(x: list[float], eps: float = 1e-5) -> list[float]:\n    rms = math.sqrt(sum(v * v for v in x) / len(x) + eps)\n    return [v / rms for v in x]\n"
        },
        {
          "path": "modules/ml/deep-learning/rmsnorm/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn rmsnorm(x: &[f64], eps: f64) -> Vec<f64> {\n    let rms = (x.iter().map(|v| v * v).sum::<f64>() / x.len() as f64 + eps).sqrt();\n    x.iter().map(|v| v / rms).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "vanishing-exploding-gradients",
      "title": "Vanishing and Exploding Gradients",
      "path": "modules/ml/deep-learning/vanishing-exploding-gradients",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Vanishing and Exploding Gradients\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nRepeated multiplication by small or large values shrinks or blows up gradients.\n\n## Math\n\n$$g_L = g_0 * Π w_i$$\n\n## Function\n\n```python\ndef gradient_chain(weights: list[float], grad: float = 1.0) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/vanishing-exploding-gradients/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/vanishing-exploding-gradients/python/vanishing_exploding_gradients.py",
          "language": "python",
          "content": "def gradient_chain(weights: list[float], grad: float = 1.0) -> float:\n    out = grad\n    for w in weights:\n        out *= w\n    return out\n"
        },
        {
          "path": "modules/ml/deep-learning/vanishing-exploding-gradients/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn gradient_chain(weights: &[f64], grad: f64) -> f64 {\n    let mut out = grad;\n    for w in weights {\n        out *= w;\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "weight-decay",
      "title": "Weight Decay",
      "path": "modules/ml/deep-learning/weight-decay",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Weight Decay\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nWeight decay shrinks weights during optimization, decoupled in AdamW.\n\n## Math\n\n$$w \\leftarrow w - \\text{lr}\\left(g + \\lambda w\\right)$$\n\n## Function\n\n```python\ndef weight_decay_step(w: float, grad: float, lr: float, lam: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/weight-decay/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/weight-decay/python/weight_decay.py",
          "language": "python",
          "content": "def weight_decay_step(w: float, grad: float, lr: float, lam: float) -> float:\n    return w - lr * (grad + lam * w)\n"
        },
        {
          "path": "modules/ml/deep-learning/weight-decay/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn weight_decay_step(w: f64, grad: f64, lr: f64, lam: f64) -> f64 {\n    w - lr * (grad + lam * w)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "xavier-initialization",
      "title": "Xavier/Glorot Initialization",
      "path": "modules/ml/deep-learning/xavier-initialization",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Xavier/Glorot Initialization\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nXavier init keeps variance stable for symmetric activations.\n\n## Math\n\n$$W \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{fan_{in}+fan_{out}}}, \\sqrt{\\frac{6}{fan_{in}+fan_{out}}}\\right)$$\n\n## Function\n\n```python\ndef xavier_uniform(fan_in: int, fan_out: int, seed: int = 0) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/xavier-initialization/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/xavier-initialization/python/xavier_initialization.py",
          "language": "python",
          "content": "import math\n\n\ndef xavier_uniform(fan_in: int, fan_out: int, seed: int = 0) -> list[float]:\n    limit = math.sqrt(6 / (fan_in + fan_out))\n    # simple LCG for deterministic demo\n    values = []\n    state = seed\n    for _ in range(fan_in * fan_out):\n        state = (1103515245 * state + 12345) % (2**31)\n        values.append(-limit + (state / (2**31 - 1)) * (2 * limit))\n    return values\n"
        },
        {
          "path": "modules/ml/deep-learning/xavier-initialization/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn xavier_uniform(fan_in: usize, fan_out: usize, seed: u64) -> Vec<f64> {\n    let limit = (6.0 / (fan_in + fan_out) as f64).sqrt();\n    let mut state = seed;\n    let mut out = Vec::with_capacity(fan_in * fan_out);\n    for _ in 0..fan_in * fan_out {\n        state = (1103515245u64.wrapping_mul(state).wrapping_add(12345)) % (1u64 << 31);\n        let frac = state as f64 / ((1u64 << 31) - 1) as f64;\n        out.push(-limit + frac * (2.0 * limit));\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "accuracy",
      "title": "Accuracy",
      "path": "modules/ml/evaluation/accuracy",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Accuracy\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nAccuracy is the fraction of correct predictions.\n\n## Math\n\n$$accuracy = correct / N$$\n\n## Function\n\n```python\ndef accuracy(y_true: list[int], y_pred: list[int]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/accuracy/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/accuracy/python/accuracy.py",
          "language": "python",
          "content": "def accuracy(y_true: list[int], y_pred: list[int]) -> float:\n    correct = sum(1 for a, b in zip(y_true, y_pred) if a == b)\n    return correct / len(y_true)\n"
        },
        {
          "path": "modules/ml/evaluation/accuracy/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn accuracy(y_true: &[i32], y_pred: &[i32]) -> f64 {\n    let mut correct = 0;\n    for (a, b) in y_true.iter().zip(y_pred.iter()) {\n        if a == b { correct += 1; }\n    }\n    correct as f64 / y_true.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "calinski-harabasz",
      "title": "Calinski-Harabasz Index",
      "path": "modules/ml/evaluation/calinski-harabasz",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Calinski-Harabasz Index\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nCalinski-Harabasz compares between- and within-cluster dispersion.\n\n## Math\n\n$$CH = (B/(k-1)) / (W/(n-k))$$\n\n## Function\n\n```python\ndef calinski_harabasz(b: float, w: float, k: int, n: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/calinski-harabasz/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/calinski-harabasz/python/calinski_harabasz.py",
          "language": "python",
          "content": "def calinski_harabasz(b: float, w: float, k: int, n: int) -> float:\n    return (b / (k - 1)) / (w / (n - k))\n"
        },
        {
          "path": "modules/ml/evaluation/calinski-harabasz/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn calinski_harabasz(b: f64, w: f64, k: i32, n: i32) -> f64 {\n    (b / (k as f64 - 1.0)) / (w / (n as f64 - k as f64))\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "confusion-matrix",
      "title": "Confusion Matrix",
      "path": "modules/ml/evaluation/confusion-matrix",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Confusion Matrix\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nConfusion matrix counts TP, FP, FN, TN.\n\n## Math\n\n$$[[TN, FP],[FN, TP]]$$\n\n## Function\n\n```python\ndef confusion_matrix(y_true: list[int], y_pred: list[int]) -> list[list[int]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/confusion-matrix/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/confusion-matrix/python/confusion_matrix.py",
          "language": "python",
          "content": "def confusion_matrix(y_true: list[int], y_pred: list[int]) -> list[list[int]]:\n    tn = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 0)\n    fp = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)\n    fn = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 0)\n    tp = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1)\n    return [[tn, fp], [fn, tp]]\n"
        },
        {
          "path": "modules/ml/evaluation/confusion-matrix/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn confusion_matrix(y_true: &[i32], y_pred: &[i32]) -> [[i32; 2]; 2] {\n    let mut tn = 0;\n    let mut fp = 0;\n    let mut fn_ = 0;\n    let mut tp = 0;\n    for (t, p) in y_true.iter().zip(y_pred.iter()) {\n        if *t == 0 && *p == 0 { tn += 1; }\n        if *t == 0 && *p == 1 { fp += 1; }\n        if *t == 1 && *p == 0 { fn_ += 1; }\n        if *t == 1 && *p == 1 { tp += 1; }\n    }\n    [[tn, fp], [fn_, tp]]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "davies-bouldin",
      "title": "Davies-Bouldin Index",
      "path": "modules/ml/evaluation/davies-bouldin",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Davies-Bouldin Index\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nDavies-Bouldin averages cluster similarity; lower is better.\n\n## Math\n\n$$DB = \\frac{1}{K}\\sum_i \\max_{j \\ne i} \\frac{s_i + s_j}{d_{ij}}$$\n\n## Function\n\n```python\ndef davies_bouldin(si: float, sj: float, dij: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/davies-bouldin/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/davies-bouldin/python/davies_bouldin.py",
          "language": "python",
          "content": "def davies_bouldin(si: float, sj: float, dij: float) -> float:\n    return (si + sj) / dij if dij > 0 else 0.0\n"
        },
        {
          "path": "modules/ml/evaluation/davies-bouldin/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn davies_bouldin(si: f64, sj: f64, dij: f64) -> f64 {\n    if dij > 0.0 { (si + sj) / dij } else { 0.0 }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "dice-score",
      "title": "Dice Score",
      "path": "modules/ml/evaluation/dice-score",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Dice Score\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nDice score measures overlap, common in segmentation.\n\n## Math\n\n$$\\text{2|A∩B|/(|A|+|B|)}$$\n\n## Function\n\n```python\ndef dice(a: set[int], b: set[int]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/dice-score/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/dice-score/python/dice_score.py",
          "language": "python",
          "content": "def dice(a: set[int], b: set[int]) -> float:\n    inter = len(a & b)\n    return 2 * inter / (len(a) + len(b)) if (len(a) + len(b)) > 0 else 0.0\n"
        },
        {
          "path": "modules/ml/evaluation/dice-score/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashSet;\n\npub fn dice(a: &HashSet<i32>, b: &HashSet<i32>) -> f64 {\n    let inter = a.intersection(b).count() as f64;\n    let denom = (a.len() + b.len()) as f64;\n    if denom == 0.0 { 0.0 } else { 2.0 * inter / denom }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "f1-score",
      "title": "F1 Score",
      "path": "modules/ml/evaluation/f1-score",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# F1 Score\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nF1 is the harmonic mean of precision and recall.\n\n## Math\n\n$$F1 = 2PR/(P+R)$$\n\n## Function\n\n```python\ndef f1_score(precision: float, recall: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/f1-score/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/f1-score/python/f1_score.py",
          "language": "python",
          "content": "def f1_score(precision: float, recall: float) -> float:\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n"
        },
        {
          "path": "modules/ml/evaluation/f1-score/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn f1_score(precision: f64, recall: f64) -> f64 {\n    if precision + recall == 0.0 { 0.0 } else { 2.0 * precision * recall / (precision + recall) }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "gini-impurity",
      "title": "Gini Impurity",
      "path": "modules/ml/evaluation/gini-impurity",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Gini Impurity\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nGini impurity measures class mixing in a node.\n\n## Math\n\n$$\\mathrm{Gini} = 1 - \\sum_c p_c^2$$\n\n## Function\n\n```python\ndef gini(labels: list[int]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/gini-impurity/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/gini-impurity/python/gini_impurity.py",
          "language": "python",
          "content": "from collections import Counter\n\n\ndef gini(labels: list[int]) -> float:\n    counts = Counter(labels)\n    n = len(labels)\n    return 1 - sum((c / n) ** 2 for c in counts.values())\n"
        },
        {
          "path": "modules/ml/evaluation/gini-impurity/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashMap;\n\npub fn gini(labels: &[i32]) -> f64 {\n    let mut counts: HashMap<i32, usize> = HashMap::new();\n    for label in labels {\n        *counts.entry(*label).or_insert(0) += 1;\n    }\n    let n = labels.len() as f64;\n    let mut sum_sq = 0.0;\n    for cnt in counts.values() {\n        let p = *cnt as f64 / n;\n        sum_sq += p * p;\n    }\n    1.0 - sum_sq\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "jaccard-index",
      "title": "Jaccard Index",
      "path": "modules/ml/evaluation/jaccard-index",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Jaccard Index\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nJaccard measures overlap between sets.\n\n## Math\n\n$$\\text{|A∩B| / |A∪B|}$$\n\n## Function\n\n```python\ndef jaccard(a: set[int], b: set[int]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/jaccard-index/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/jaccard-index/python/jaccard_index.py",
          "language": "python",
          "content": "def jaccard(a: set[int], b: set[int]) -> float:\n    inter = len(a & b)\n    union = len(a | b)\n    return inter / union if union > 0 else 0.0\n"
        },
        {
          "path": "modules/ml/evaluation/jaccard-index/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashSet;\n\npub fn jaccard(a: &HashSet<i32>, b: &HashSet<i32>) -> f64 {\n    let inter = a.intersection(b).count() as f64;\n    let union = a.union(b).count() as f64;\n    if union == 0.0 { 0.0 } else { inter / union }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "mae-vs-mse",
      "title": "MAE vs MSE",
      "path": "modules/ml/evaluation/mae-vs-mse",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# MAE vs MSE\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nCompare MAE and MSE to see sensitivity to outliers.\n\n## Math\n\n$$MAE uses |e|; MSE uses e^2.$$\n\n## Function\n\n```python\ndef mae_mse(y: list[float], y_hat: list[float]) -> tuple[float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/mae-vs-mse/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/mae-vs-mse/python/mae_vs_mse.py",
          "language": "python",
          "content": "def mae_mse(y: list[float], y_hat: list[float]) -> tuple[float, float]:\n    mae = sum(abs(a - b) for a, b in zip(y, y_hat)) / len(y)\n    mse = sum((a - b) ** 2 for a, b in zip(y, y_hat)) / len(y)\n    return mae, mse\n"
        },
        {
          "path": "modules/ml/evaluation/mae-vs-mse/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn mae_mse(y: &[f64], y_hat: &[f64]) -> (f64, f64) {\n    let mae = y.iter().zip(y_hat.iter()).map(|(a, b)| (a - b).abs()).sum::<f64>() / y.len() as f64;\n    let mse = y.iter().zip(y_hat.iter()).map(|(a, b)| (a - b).powi(2)).sum::<f64>() / y.len() as f64;\n    (mae, mse)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "matthews-correlation",
      "title": "Matthews Correlation Coefficient",
      "path": "modules/ml/evaluation/matthews-correlation",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Matthews Correlation Coefficient\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nMCC balances all confusion matrix terms.\n\n## Math\n\n$$\\mathrm{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$\n\n## Function\n\n```python\ndef mcc(tp: int, tn: int, fp: int, fn: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/matthews-correlation/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/matthews-correlation/python/matthews_correlation.py",
          "language": "python",
          "content": "import math\n\n\ndef mcc(tp: int, tn: int, fp: int, fn: int) -> float:\n    denom = math.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n    if denom == 0:\n        return 0.0\n    return (tp * tn - fp * fn) / denom\n"
        },
        {
          "path": "modules/ml/evaluation/matthews-correlation/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn mcc(tp: i32, tn: i32, fp: i32, fn_: i32) -> f64 {\n    let denom = ((tp + fp) as f64 * (tp + fn_) as f64 * (tn + fp) as f64 * (tn + fn_) as f64).sqrt();\n    if denom == 0.0 { 0.0 } else { (tp * tn - fp * fn_) as f64 / denom }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "precision-recall",
      "title": "Precision and Recall",
      "path": "modules/ml/evaluation/precision-recall",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Precision and Recall\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nPrecision measures correctness of positive predictions; recall measures coverage.\n\n## Math\n\n$$precision=TP/(TP+FP), recall=TP/(TP+FN)$$\n\n## Function\n\n```python\ndef precision_recall(y_true: list[int], y_pred: list[int]) -> tuple[float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/precision-recall/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/precision-recall/python/precision_recall.py",
          "language": "python",
          "content": "def precision_recall(y_true: list[int], y_pred: list[int]) -> tuple[float, float]:\n    tp = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1)\n    fp = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)\n    fn = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 0)\n    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n    return precision, recall\n"
        },
        {
          "path": "modules/ml/evaluation/precision-recall/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn precision_recall(y_true: &[i32], y_pred: &[i32]) -> (f64, f64) {\n    let mut tp = 0;\n    let mut fp = 0;\n    let mut fn_ = 0;\n    for (t, p) in y_true.iter().zip(y_pred.iter()) {\n        if *t == 1 && *p == 1 { tp += 1; }\n        if *t == 0 && *p == 1 { fp += 1; }\n        if *t == 1 && *p == 0 { fn_ += 1; }\n    }\n    let precision = if tp + fp > 0 { tp as f64 / (tp + fp) as f64 } else { 0.0 };\n    let recall = if tp + fn_ > 0 { tp as f64 / (tp + fn_) as f64 } else { 0.0 };\n    (precision, recall)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "r2-score",
      "title": "R2 Score",
      "path": "modules/ml/evaluation/r2-score",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# R2 Score\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nR2 measures explained variance relative to a mean baseline.\n\n## Math\n\n$$R2 = 1 - SS_res/SS_tot$$\n\n## Function\n\n```python\ndef r2_score(y: list[float], y_hat: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/r2-score/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/r2-score/python/r2_score.py",
          "language": "python",
          "content": "def r2_score(y: list[float], y_hat: list[float]) -> float:\n    mean = sum(y) / len(y)\n    ss_res = sum((a - b) ** 2 for a, b in zip(y, y_hat))\n    ss_tot = sum((a - mean) ** 2 for a in y)\n    return 1 - ss_res / ss_tot if ss_tot > 0 else 0.0\n"
        },
        {
          "path": "modules/ml/evaluation/r2-score/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn r2_score(y: &[f64], y_hat: &[f64]) -> f64 {\n    let mean: f64 = y.iter().sum::<f64>() / y.len() as f64;\n    let ss_res: f64 = y.iter().zip(y_hat.iter()).map(|(a, b)| (a - b).powi(2)).sum();\n    let ss_tot: f64 = y.iter().map(|a| (a - mean).powi(2)).sum();\n    if ss_tot > 0.0 { 1.0 - ss_res / ss_tot } else { 0.0 }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "roc-auc",
      "title": "ROC-AUC",
      "path": "modules/ml/evaluation/roc-auc",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# ROC-AUC\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nROC-AUC measures ranking quality over thresholds.\n\n## Math\n\n$$AUC = area under TPR vs FPR curve.$$\n\n## Function\n\n```python\ndef auc(fpr: list[float], tpr: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/roc-auc/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/roc-auc/python/roc_auc.py",
          "language": "python",
          "content": "def auc(fpr: list[float], tpr: list[float]) -> float:\n    area = 0.0\n    for i in range(1, len(fpr)):\n        area += (fpr[i] - fpr[i - 1]) * (tpr[i] + tpr[i - 1]) / 2\n    return area\n"
        },
        {
          "path": "modules/ml/evaluation/roc-auc/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn auc(fpr: &[f64], tpr: &[f64]) -> f64 {\n    let mut area = 0.0;\n    for i in 1..fpr.len() {\n        area += (fpr[i] - fpr[i - 1]) * (tpr[i] + tpr[i - 1]) / 2.0;\n    }\n    area\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "silhouette-score",
      "title": "Silhouette Score",
      "path": "modules/ml/evaluation/silhouette-score",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Silhouette Score\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nSilhouette compares intra-cluster to nearest-cluster distance.\n\n## Math\n\n$$s = \\frac{b - a}{\\max(a,b)}$$\n\n## Function\n\n```python\ndef silhouette(a: float, b: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/silhouette-score/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/silhouette-score/python/silhouette_score.py",
          "language": "python",
          "content": "def silhouette(a: float, b: float) -> float:\n    if max(a, b) == 0:\n        return 0.0\n    return (b - a) / max(a, b)\n"
        },
        {
          "path": "modules/ml/evaluation/silhouette-score/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn silhouette(a: f64, b: f64) -> f64 {\n    let m = a.max(b);\n    if m == 0.0 { 0.0 } else { (b - a) / m }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "beta-binomial",
      "title": "Bayesian Inference (Beta-Binomial)",
      "path": "modules/ml/fundamentals/beta-binomial",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Bayesian Inference (Beta-Binomial)\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nBeta prior and Binomial likelihood yield Beta posterior.\n\n## Math\n\n$$\\alpha' = \\alpha + \\text{successes},\\ \\beta' = \\beta + \\text{failures}$$\n\n## Function\n\n```python\ndef beta_posterior(alpha: float, beta: float, successes: int, failures: int) -> tuple[float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/beta-binomial/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/beta-binomial/python/beta_binomial.py",
          "language": "python",
          "content": "def beta_posterior(alpha: float, beta: float, successes: int, failures: int) -> tuple[float, float]:\n    return alpha + successes, beta + failures\n"
        },
        {
          "path": "modules/ml/fundamentals/beta-binomial/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn beta_posterior(alpha: f64, beta: f64, successes: i32, failures: i32) -> (f64, f64) {\n    (alpha + successes as f64, beta + failures as f64)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "convex-vs-nonconvex",
      "title": "Convex vs Non-Convex",
      "path": "modules/ml/fundamentals/convex-vs-nonconvex",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Convex vs Non-Convex\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nConvex functions have a single global minimum.\n\n## Math\n\n$$f(x)=ax^2+bx+c \\text{ is convex if } a \\ge 0$$\n\n## Function\n\n```python\ndef is_convex_quadratic(a: float) -> bool:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/convex-vs-nonconvex/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/convex-vs-nonconvex/python/convex_vs_nonconvex.py",
          "language": "python",
          "content": "def is_convex_quadratic(a: float) -> bool:\n    return a >= 0\n"
        },
        {
          "path": "modules/ml/fundamentals/convex-vs-nonconvex/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn is_convex_quadratic(a: f64) -> bool {\n    a >= 0.0\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "cosine-similarity",
      "title": "Cosine Similarity",
      "path": "modules/ml/fundamentals/cosine-similarity",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Cosine Similarity\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nCosine similarity measures angle between vectors.\n\n## Math\n\n$$cos = (a·b) / (||a|| ||b||)$$\n\n## Function\n\n```python\ndef cosine_similarity(a: list[float], b: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/cosine-similarity/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/cosine-similarity/python/cosine_similarity.py",
          "language": "python",
          "content": "import math\n\n\ndef cosine_similarity(a: list[float], b: list[float]) -> float:\n    dot = sum(x * y for x, y in zip(a, b))\n    na = math.sqrt(sum(x * x for x in a))\n    nb = math.sqrt(sum(y * y for y in b))\n    return dot / (na * nb)\n"
        },
        {
          "path": "modules/ml/fundamentals/cosine-similarity/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn cosine_similarity(a: &[f64], b: &[f64]) -> f64 {\n    let dot: f64 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();\n    let na = a.iter().map(|x| x * x).sum::<f64>().sqrt();\n    let nb = b.iter().map(|y| y * y).sum::<f64>().sqrt();\n    dot / (na * nb)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "covariance",
      "title": "Covariance",
      "path": "modules/ml/fundamentals/covariance",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Covariance\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nCovariance measures how two variables vary together.\n\n## Math\n\n$$\\mathrm{cov}(X,Y)=\\mathbb{E}[(X-\\mu_X)(Y-\\mu_Y)]$$\n\n## Function\n\n```python\ndef covariance(x: list[float], y: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/covariance/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/covariance/python/covariance.py",
          "language": "python",
          "content": "def covariance(x: list[float], y: list[float]) -> float:\n    mean_x = sum(x) / len(x)\n    mean_y = sum(y) / len(y)\n    return sum((a - mean_x) * (b - mean_y) for a, b in zip(x, y)) / len(x)\n"
        },
        {
          "path": "modules/ml/fundamentals/covariance/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn covariance(x: &[f64], y: &[f64]) -> f64 {\n    let mean_x: f64 = x.iter().sum::<f64>() / x.len() as f64;\n    let mean_y: f64 = y.iter().sum::<f64>() / y.len() as f64;\n    x.iter()\n.zip(y.iter())\n.map(|(a, b)| (a - mean_x) * (b - mean_y))\n.sum::<f64>()\n        / x.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "distributions",
      "title": "Distributions",
      "path": "modules/ml/fundamentals/distributions",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Distributions\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nA probability distribution assigns probabilities to outcomes.\n\n## Math\n\n$$\\sum_i p_i = 1$$\n\n## Function\n\n```python\ndef normalize_probs(values: list[float]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/distributions/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/distributions/python/distributions.py",
          "language": "python",
          "content": "def normalize_probs(values: list[float]) -> list[float]:\n    s = sum(values)\n    return [v / s for v in values]\n"
        },
        {
          "path": "modules/ml/fundamentals/distributions/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn normalize_probs(values: &[f64]) -> Vec<f64> {\n    let sum: f64 = values.iter().sum();\n    values.iter().map(|v| v / sum).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "elbo",
      "title": "ELBO",
      "path": "modules/ml/fundamentals/elbo",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# ELBO\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nELBO lower-bounds log-likelihood in variational inference.\n\n## Math\n\n$$\\mathrm{ELBO} = \\mathbb{E}_q[\\log p(x|z)] - \\mathrm{KL}(q\\|p)$$\n\n## Function\n\n```python\ndef elbo(recon: float, kl: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/elbo/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/elbo/python/elbo.py",
          "language": "python",
          "content": "def elbo(recon: float, kl: float) -> float:\n    return recon - kl\n"
        },
        {
          "path": "modules/ml/fundamentals/elbo/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn elbo(recon: f64, kl: f64) -> f64 {\n    recon - kl\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "empirical-pmf",
      "title": "Empirical PMF",
      "path": "modules/ml/fundamentals/empirical-pmf",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Empirical PMF\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nEmpirical PMF estimates probability from samples.\n\n## Math\n\n$$p(x)=count(x)/N$$\n\n## Function\n\n```python\ndef empirical_pmf(samples: list[int]) -> dict[int, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/empirical-pmf/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/empirical-pmf/python/empirical_pmf.py",
          "language": "python",
          "content": "from collections import Counter\n\n\ndef empirical_pmf(samples: list[int]) -> dict[int, float]:\n    counts = Counter(samples)\n    n = len(samples)\n    return {k: v / n for k, v in counts.items()}\n"
        },
        {
          "path": "modules/ml/fundamentals/empirical-pmf/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashMap;\n\npub fn empirical_pmf(samples: &[i32]) -> HashMap<i32, f64> {\n    let mut counts: HashMap<i32, usize> = HashMap::new();\n    for s in samples {\n        *counts.entry(*s).or_insert(0) += 1;\n    }\n    let n = samples.len() as f64;\n    let mut pmf = HashMap::new();\n    for (k, v) in counts {\n        pmf.insert(k, v as f64 / n);\n    }\n    pmf\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "expectation",
      "title": "Expectation",
      "path": "modules/ml/fundamentals/expectation",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Expectation\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nExpectation is the average value of a random variable.\n\n## Math\n\n$$\\mathbb{E}[X] = \\sum_i x_i p_i$$\n\n## Function\n\n```python\ndef expectation(values: list[float], probs: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/expectation/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/expectation/python/expectation.py",
          "language": "python",
          "content": "def expectation(values: list[float], probs: list[float]) -> float:\n    return sum(v * p for v, p in zip(values, probs))\n"
        },
        {
          "path": "modules/ml/fundamentals/expectation/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn expectation(values: &[f64], probs: &[f64]) -> f64 {\n    values.iter().zip(probs.iter()).map(|(v, p)| v * p).sum()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "gradient-descent",
      "title": "Gradient Descent",
      "path": "modules/ml/fundamentals/gradient-descent",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Gradient Descent\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nGradient descent updates parameters along negative gradient.\n\n## Math\n\n$$x = x - lr * grad$$\n\n## Function\n\n```python\ndef gd_step(x: float, grad: float, lr: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/gradient-descent/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/gradient-descent/python/gradient_descent.py",
          "language": "python",
          "content": "def gd_step(x: float, grad: float, lr: float) -> float:\n    return x - lr * grad\n"
        },
        {
          "path": "modules/ml/fundamentals/gradient-descent/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn gd_step(x: f64, grad: f64, lr: f64) -> f64 {\n    x - lr * grad\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "hessian",
      "title": "Hessian",
      "path": "modules/ml/fundamentals/hessian",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Hessian\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nHessian is the matrix of second derivatives.\n\n## Math\n\n$$H_ij = ∂^2 f / ∂x_i ∂x_j$$\n\n## Function\n\n```python\ndef hessian_quadratic(a: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/hessian/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/hessian/python/hessian.py",
          "language": "python",
          "content": "def hessian_quadratic(a: float) -> float:\n    return 2 * a\n"
        },
        {
          "path": "modules/ml/fundamentals/hessian/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn hessian_quadratic(a: f64) -> f64 {\n    2.0 * a\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "jacobian",
      "title": "Jacobian",
      "path": "modules/ml/fundamentals/jacobian",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Jacobian\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nJacobian contains partial derivatives of vector-valued functions.\n\n## Math\n\n$$J_ij = ∂f_i/∂x_j$$\n\n## Function\n\n```python\ndef jacobian(f1: callable, f2: callable, x: float, y: float, eps: float = 1e-5) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/jacobian/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/jacobian/python/jacobian.py",
          "language": "python",
          "content": "def jacobian(f1, f2, x: float, y: float, eps: float = 1e-5) -> list[list[float]]:\n    df1_dx = (f1(x + eps, y) - f1(x - eps, y)) / (2 * eps)\n    df1_dy = (f1(x, y + eps) - f1(x, y - eps)) / (2 * eps)\n    df2_dx = (f2(x + eps, y) - f2(x - eps, y)) / (2 * eps)\n    df2_dy = (f2(x, y + eps) - f2(x, y - eps)) / (2 * eps)\n    return [[df1_dx, df1_dy], [df2_dx, df2_dy]]\n"
        },
        {
          "path": "modules/ml/fundamentals/jacobian/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn jacobian(f1: fn(f64, f64) -> f64, f2: fn(f64, f64) -> f64, x: f64, y: f64, eps: f64) -> [[f64; 2]; 2] {\n    let df1_dx = (f1(x + eps, y) - f1(x - eps, y)) / (2.0 * eps);\n    let df1_dy = (f1(x, y + eps) - f1(x, y - eps)) / (2.0 * eps);\n    let df2_dx = (f2(x + eps, y) - f2(x - eps, y)) / (2.0 * eps);\n    let df2_dy = (f2(x, y + eps) - f2(x, y - eps)) / (2.0 * eps);\n    [[df1_dx, df1_dy], [df2_dx, df2_dy]]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "jensen-shannon-divergence",
      "title": "Jensen–Shannon Divergence",
      "path": "modules/ml/fundamentals/jensen-shannon-divergence",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Jensen–Shannon Divergence\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nJS divergence symmetrizes KL using the mean distribution.\n\n## Math\n\n$$JS(p,q) = 0.5 KL(p||m)+0.5 KL(q||m)$$\n\n## Function\n\n```python\ndef js(p: list[float], q: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/jensen-shannon-divergence/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/jensen-shannon-divergence/python/jensen_shannon_divergence.py",
          "language": "python",
          "content": "import math\n\n\ndef js(p: list[float], q: list[float]) -> float:\n    m = [(pi + qi) / 2 for pi, qi in zip(p, q)]\n    def kl(a, b):\n        return sum(ai * math.log(ai / bi) for ai, bi in zip(a, b) if ai > 0 and bi > 0)\n    return 0.5 * kl(p, m) + 0.5 * kl(q, m)\n"
        },
        {
          "path": "modules/ml/fundamentals/jensen-shannon-divergence/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn js(p: &[f64], q: &[f64]) -> f64 {\n    let m: Vec<f64> = p.iter().zip(q.iter()).map(|(a, b)| (a + b) / 2.0).collect();\n    let kl = |a: &[f64], b: &[f64]| -> f64 {\n        let mut total = 0.0;\n        for (&ai, &bi) in a.iter().zip(b.iter()) {\n            if ai > 0.0 && bi > 0.0 {\n                total += ai * (ai / bi).ln();\n            }\n        }\n        total\n    };\n    0.5 * kl(p, &m) + 0.5 * kl(q, &m)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "kl-divergence",
      "title": "KL Divergence",
      "path": "modules/ml/fundamentals/kl-divergence",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# KL Divergence\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nKL divergence measures how one distribution diverges from another.\n\n## Math\n\n$$\\mathrm{KL}(p\\|q) = \\sum_i p_i \\log\\left(\\frac{p_i}{q_i}\\right)$$\n\n## Function\n\n```python\ndef kl(p: list[float], q: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/kl-divergence/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/kl-divergence/python/kl_divergence.py",
          "language": "python",
          "content": "import math\n\n\ndef kl(p: list[float], q: list[float]) -> float:\n    total = 0.0\n    for pi, qi in zip(p, q):\n        if pi > 0 and qi > 0:\n            total += pi * math.log(pi / qi)\n    return total\n"
        },
        {
          "path": "modules/ml/fundamentals/kl-divergence/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn kl(p: &[f64], q: &[f64]) -> f64 {\n    let mut total = 0.0;\n    for (&pi, &qi) in p.iter().zip(q.iter()) {\n        if pi > 0.0 && qi > 0.0 {\n            total += pi * (pi / qi).ln();\n        }\n    }\n    total\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "markov-chains",
      "title": "Markov Chains",
      "path": "modules/ml/fundamentals/markov-chains",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Markov Chains\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nMarkov chains evolve distributions via transition matrices.\n\n## Math\n\n$$p_{t+1} = p_t T$$\n\n## Function\n\n```python\ndef next_distribution(p: list[float], t: list[list[float]]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/markov-chains/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/markov-chains/python/markov_chains.py",
          "language": "python",
          "content": "def next_distribution(p: list[float], t: list[list[float]]) -> list[float]:\n    return [sum(p[j] * t[j][i] for j in range(len(p))) for i in range(len(t[0]))]\n"
        },
        {
          "path": "modules/ml/fundamentals/markov-chains/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn next_distribution(p: &[f64], t: &[Vec<f64>]) -> Vec<f64> {\n    let mut out = vec![0.0; t[0].len()];\n    for i in 0..t[0].len() {\n        let mut sum = 0.0;\n        for j in 0..p.len() {\n            sum += p[j] * t[j][i];\n        }\n        out[i] = sum;\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "mutual-information",
      "title": "Mutual Information",
      "path": "modules/ml/fundamentals/mutual-information",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Mutual Information\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nMutual information measures shared information between variables.\n\n## Math\n\n$$I(X;Y)=\\sum_{x,y} p(x,y)\\log\\left(\\frac{p(x,y)}{p(x)p(y)}\\right)$$\n\n## Function\n\n```python\ndef mutual_information(joint: list[list[float]]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/mutual-information/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/mutual-information/python/mutual_information.py",
          "language": "python",
          "content": "import math\n\n\ndef mutual_information(joint: list[list[float]]) -> float:\n    px = [sum(row) for row in joint]\n    py = [sum(joint[i][j] for i in range(len(joint))) for j in range(len(joint[0]))]\n    mi = 0.0\n    for i in range(len(joint)):\n        for j in range(len(joint[0])):\n            pxy = joint[i][j]\n            if pxy > 0 and px[i] > 0 and py[j] > 0:\n                mi += pxy * math.log(pxy / (px[i] * py[j]))\n    return mi\n"
        },
        {
          "path": "modules/ml/fundamentals/mutual-information/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn mutual_information(joint: &[Vec<f64>]) -> f64 {\n    let px: Vec<f64> = joint.iter().map(|row| row.iter().sum()).collect();\n    let mut py = vec![0.0; joint[0].len()];\n    for i in 0..joint.len() {\n        for j in 0..joint[0].len() {\n            py[j] += joint[i][j];\n        }\n    }\n    let mut mi = 0.0;\n    for i in 0..joint.len() {\n        for j in 0..joint[0].len() {\n            let pxy = joint[i][j];\n            if pxy > 0.0 && px[i] > 0.0 && py[j] > 0.0 {\n                mi += pxy * (pxy / (px[i] * py[j])).ln();\n            }\n        }\n    }\n    mi\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "newtons-method",
      "title": "Newton's Method",
      "path": "modules/ml/fundamentals/newtons-method",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Newton's Method\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nNewton's method uses second derivatives for faster convergence.\n\n## Math\n\n$$x = x - f'(x)/f''(x)$$\n\n## Function\n\n```python\ndef newton_step(x: float, f_prime: float, f_double_prime: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/newtons-method/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/newtons-method/python/newtons_method.py",
          "language": "python",
          "content": "def newton_step(x: float, f_prime: float, f_double_prime: float) -> float:\n    return x - f_prime / f_double_prime\n"
        },
        {
          "path": "modules/ml/fundamentals/newtons-method/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn newton_step(x: f64, f_prime: f64, f_double_prime: f64) -> f64 {\n    x - f_prime / f_double_prime\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "svd",
      "title": "SVD",
      "path": "modules/ml/fundamentals/svd",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# SVD\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nSVD factorizes a matrix into $U \\Sigma V^{\\top}$.\n\n## Math\n\n$$A = U \\Sigma V^{\\top}$$\n\n## Function\n\n```python\ndef singular_values_2x2(a: list[list[float]]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/svd/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/svd/python/svd.py",
          "language": "python",
          "content": "import math\n\n\ndef singular_values_2x2(a: list[list[float]]) -> list[float]:\n    # compute eigenvalues of A^T A\n    at_a = [\n        [a[0][0] ** 2 + a[1][0] ** 2, a[0][0] * a[0][1] + a[1][0] * a[1][1]],\n        [a[0][0] * a[0][1] + a[1][0] * a[1][1], a[0][1] ** 2 + a[1][1] ** 2],\n    ]\n    trace = at_a[0][0] + at_a[1][1]\n    det = at_a[0][0] * at_a[1][1] - at_a[0][1] * at_a[1][0]\n    eig1 = trace / 2 + math.sqrt(max(0.0, (trace / 2) ** 2 - det))\n    eig2 = trace / 2 - math.sqrt(max(0.0, (trace / 2) ** 2 - det))\n    return [math.sqrt(eig1), math.sqrt(eig2)]\n"
        },
        {
          "path": "modules/ml/fundamentals/svd/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn singular_values_2x2(a: [[f64; 2]; 2]) -> [f64; 2] {\n    let at_a = [\n        [a[0][0] * a[0][0] + a[1][0] * a[1][0], a[0][0] * a[0][1] + a[1][0] * a[1][1]],\n        [a[0][0] * a[0][1] + a[1][0] * a[1][1], a[0][1] * a[0][1] + a[1][1] * a[1][1]],\n    ];\n    let trace = at_a[0][0] + at_a[1][1];\n    let det = at_a[0][0] * at_a[1][1] - at_a[0][1] * at_a[1][0];\n    let disc = ((trace / 2.0).powi(2) - det).max(0.0).sqrt();\n    let eig1 = trace / 2.0 + disc;\n    let eig2 = trace / 2.0 - disc;\n    [eig1.sqrt(), eig2.sqrt()]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "two-sample-t-test",
      "title": "Two-Sample t-test",
      "path": "modules/ml/fundamentals/two-sample-t-test",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Two-Sample t-test\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nTwo-sample t-test compares means of two groups.\n\n## Math\n\n$$t = \\frac{m_1 - m_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}$$\n\n## Function\n\n```python\ndef t_stat(x: list[float], y: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/two-sample-t-test/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/two-sample-t-test/python/two_sample_t_test.py",
          "language": "python",
          "content": "import math\n\n\ndef t_stat(x: list[float], y: list[float]) -> float:\n    m1 = sum(x) / len(x)\n    m2 = sum(y) / len(y)\n    v1 = sum((v - m1) ** 2 for v in x) / (len(x) - 1)\n    v2 = sum((v - m2) ** 2 for v in y) / (len(y) - 1)\n    return (m1 - m2) / math.sqrt(v1 / len(x) + v2 / len(y))\n"
        },
        {
          "path": "modules/ml/fundamentals/two-sample-t-test/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn t_stat(x: &[f64], y: &[f64]) -> f64 {\n    let m1: f64 = x.iter().sum::<f64>() / x.len() as f64;\n    let m2: f64 = y.iter().sum::<f64>() / y.len() as f64;\n    let v1: f64 = x.iter().map(|v| (v - m1).powi(2)).sum::<f64>() / (x.len() as f64 - 1.0);\n    let v2: f64 = y.iter().map(|v| (v - m2).powi(2)).sum::<f64>() / (y.len() as f64 - 1.0);\n    (m1 - m2) / (v1 / x.len() as f64 + v2 / y.len() as f64).sqrt()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "vectors-matrices",
      "title": "Vectors and Matrices",
      "path": "modules/ml/fundamentals/vectors-matrices",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Vectors and Matrices\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nVectors and matrices represent data and linear transforms.\n\n## Math\n\n$$y = A x$$\n\n## Function\n\n```python\ndef matvec(a: list[list[float]], x: list[float]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/vectors-matrices/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/vectors-matrices/python/vectors_matrices.py",
          "language": "python",
          "content": "def matvec(a: list[list[float]], x: list[float]) -> list[float]:\n    return [sum(ai * xi for ai, xi in zip(row, x)) for row in a]\n"
        },
        {
          "path": "modules/ml/fundamentals/vectors-matrices/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn matvec(a: &[Vec<f64>], x: &[f64]) -> Vec<f64> {\n    a.iter().map(|row| row.iter().zip(x.iter()).map(|(ai, xi)| ai * xi).sum()).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "generative",
      "slug": "diffusion-guidance-tradeoffs",
      "title": "Diffusion Sampling and Guidance Trade-offs",
      "path": "modules/ml/generative/diffusion-guidance-tradeoffs",
      "summary": "> Track: `ml` | Topic: `generative`",
      "readme": "# Diffusion Sampling and Guidance Trade-offs\n\n> Track: `ml` | Topic: `generative`\n\n## Concept\n\nGuidance improves fidelity but can slow sampling or reduce diversity.\n\n## Math\n\n$$guided = base + s * (cond - base)$$\n\n## Function\n\n```python\ndef guided_step(base: float, cond: float, scale: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/generative/diffusion-guidance-tradeoffs/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/generative/diffusion-guidance-tradeoffs/python/diffusion_guidance_tradeoffs.py",
          "language": "python",
          "content": "def guided_step(base: float, cond: float, scale: float) -> float:\n    return base + scale * (cond - base)\n"
        },
        {
          "path": "modules/ml/generative/diffusion-guidance-tradeoffs/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn guided_step(base: f64, cond: f64, scale: f64) -> f64 {\n    base + scale * (cond - base)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "generative",
      "slug": "diffusion-models",
      "title": "Diffusion Models",
      "path": "modules/ml/generative/diffusion-models",
      "summary": "> Track: `ml` | Topic: `generative`",
      "readme": "# Diffusion Models\n\n> Track: `ml` | Topic: `generative`\n\n## Concept\n\nDiffusion adds noise forward and denoises backward.\n\n## Math\n\n$$x_t = \\sqrt{\\alpha}\\, x_{t-1} + \\sqrt{1-\\alpha}\\, \\epsilon$$\n\n## Function\n\n```python\ndef add_noise(x: float, noise: float, alpha: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/generative/diffusion-models/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/generative/diffusion-models/python/diffusion_models.py",
          "language": "python",
          "content": "import math\n\n\ndef add_noise(x: float, noise: float, alpha: float) -> float:\n    return math.sqrt(alpha) * x + math.sqrt(1 - alpha) * noise\n"
        },
        {
          "path": "modules/ml/generative/diffusion-models/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn add_noise(x: f64, noise: f64, alpha: f64) -> f64 {\n    alpha.sqrt() * x + (1.0 - alpha).sqrt() * noise\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "generative",
      "slug": "gan-mode-collapse",
      "title": "GAN Instability and Mode Collapse",
      "path": "modules/ml/generative/gan-mode-collapse",
      "summary": "> Track: `ml` | Topic: `generative`",
      "readme": "# GAN Instability and Mode Collapse\n\n> Track: `ml` | Topic: `generative`\n\n## Concept\n\nMode collapse happens when the generator outputs limited modes.\n\n## Math\n\n$$Low diversity ⇒ entropy of samples drops.$$\n\n## Function\n\n```python\ndef diversity_score(samples: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/generative/gan-mode-collapse/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/generative/gan-mode-collapse/python/gan_mode_collapse.py",
          "language": "python",
          "content": "def diversity_score(samples: list[float]) -> float:\n    return len(set(samples)) / len(samples) if samples else 0.0\n"
        },
        {
          "path": "modules/ml/generative/gan-mode-collapse/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashSet;\n\npub fn diversity_score(samples: &[i32]) -> f64 {\n    if samples.is_empty() { return 0.0; }\n    let set: HashSet<i32> = samples.iter().copied().collect();\n    set.len() as f64 / samples.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "generative",
      "slug": "gan",
      "title": "GAN",
      "path": "modules/ml/generative/gan",
      "summary": "> Track: `ml` | Topic: `generative`",
      "readme": "# GAN\n\n> Track: `ml` | Topic: `generative`\n\n## Concept\n\nGANs train a generator and discriminator adversarially.\n\n## Math\n\n$$L_D = -\\log D(x) - \\log\\left(1 - D(G(z))\\right)$$\n\n## Function\n\n```python\ndef gan_loss(d_real: float, d_fake: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/generative/gan/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/generative/gan/python/gan.py",
          "language": "python",
          "content": "import math\n\n\ndef gan_loss(d_real: float, d_fake: float) -> float:\n    return -math.log(d_real) - math.log(1 - d_fake)\n"
        },
        {
          "path": "modules/ml/generative/gan/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn gan_loss(d_real: f64, d_fake: f64) -> f64 {\n    -(d_real.ln()) - (1.0 - d_fake).ln()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "generative",
      "slug": "model-selection",
      "title": "Choose GAN vs VAE vs Diffusion",
      "path": "modules/ml/generative/model-selection",
      "summary": "> Track: `ml` | Topic: `generative`",
      "readme": "# Choose GAN vs VAE vs Diffusion\n\n> Track: `ml` | Topic: `generative`\n\n## Concept\n\nPick a model based on fidelity, diversity, and speed trade-offs.\n\n## Math\n\n$$\\text{Selection is heuristic, not a fixed formula.}$$\n\n## Function\n\n```python\ndef choose_model(priority: str) -> str:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/generative/model-selection/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/generative/model-selection/python/model_selection.py",
          "language": "python",
          "content": "def choose_model(priority: str) -> str:\n    priority = priority.lower()\n    if priority == \"speed\":\n        return \"gan\"\n    if priority == \"diversity\":\n        return \"diffusion\"\n    return \"vae\"\n"
        },
        {
          "path": "modules/ml/generative/model-selection/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn choose_model(priority: &str) -> String {\n    match priority.to_lowercase().as_str() {\n        \"speed\" => \"gan\".to_string(),\n        \"diversity\" => \"diffusion\".to_string(),\n        _ => \"vae\".to_string(),\n    }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "generative",
      "slug": "vae-posterior-collapse",
      "title": "VAE Blurry Samples and Posterior Collapse",
      "path": "modules/ml/generative/vae-posterior-collapse",
      "summary": "> Track: `ml` | Topic: `generative`",
      "readme": "# VAE Blurry Samples and Posterior Collapse\n\n> Track: `ml` | Topic: `generative`\n\n## Concept\n\nPosterior collapse happens when KL goes to zero.\n\n## Math\n\n$$KL → 0 indicates z ignored.$$\n\n## Function\n\n```python\ndef kl_is_low(kl: float, threshold: float = 0.1) -> bool:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/generative/vae-posterior-collapse/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/generative/vae-posterior-collapse/python/vae_posterior_collapse.py",
          "language": "python",
          "content": "def kl_is_low(kl: float, threshold: float = 0.1) -> bool:\n    return kl < threshold\n"
        },
        {
          "path": "modules/ml/generative/vae-posterior-collapse/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn kl_is_low(kl: f64, threshold: f64) -> bool {\n    kl < threshold\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "generative",
      "slug": "vae",
      "title": "VAE",
      "path": "modules/ml/generative/vae",
      "summary": "> Track: `ml` | Topic: `generative`",
      "readme": "# VAE\n\n> Track: `ml` | Topic: `generative`\n\n## Concept\n\nVAEs maximize ELBO = reconstruction - KL.\n\n## Math\n\n$$\\mathrm{ELBO} = \\mathbb{E}_q[\\log p(x|z)] - \\mathrm{KL}(q(z)\\|p(z))$$\n\n## Function\n\n```python\ndef elbo(recon: float, kl: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/generative/vae/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/generative/vae/python/vae.py",
          "language": "python",
          "content": "def elbo(recon: float, kl: float) -> float:\n    return recon - kl\n"
        },
        {
          "path": "modules/ml/generative/vae/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn elbo(recon: f64, kl: f64) -> f64 {\n    recon - kl\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "attention-causal",
      "title": "Masked Attention (Causal Mask)",
      "path": "modules/ml/llm/attention-causal",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Masked Attention (Causal Mask)\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nIn autoregressive language models, each token can only attend to itself and\nprevious tokens. This is enforced by a **causal (lower-triangular) mask** applied\nto the attention scores before softmax.\n\nWithout the mask, token at position _i_ could \"see\" tokens at positions _i+1, i+2, …_,\nleaking future information during training. The mask sets those attention scores\nto −∞ so that softmax drives them to zero.\n\nMulti-head attention applies the same mask to every head independently.\nThe mask is a constant for a given sequence length and can be precomputed once.\n\n## Math\n\n$$\\text{Scaled dot-product attention with causal mask:}$$\n\n```\nscores = Q @ K^T / sqrt(d_k)           # (seq_len, seq_len)\n\nmask[i][j] = 0      if j <= i\n           = -inf    if j >  i\n\nmasked = scores + mask\nweights = softmax(masked, dim=-1)       # (seq_len, seq_len)\noutput  = weights @ V                   # (seq_len, d_v)\n```\n\n- $\\text{\\texttt{Q}, \\texttt{K} — queries and keys, shape \\texttt{(seq\\_len, d\\_k)}}$\n- $\\text{\\texttt{V} — values, shape \\texttt{(seq\\_len, d\\_v)}}$\n- $\\text{\\texttt{d\\_k} — key dimension (scaling factor prevents large dot products)}$\n\n## Function\n\n```python\ndef causal_self_attention(Q, K, V) -> np.ndarray\n```\n\n- `Q` — query matrix, shape `(seq_len, d_k)`\n- `K` — key matrix, shape `(seq_len, d_k)`\n- `V` — value matrix, shape `(seq_len, d_v)`\n- Returns — output of shape `(seq_len, d_v)`, each row attending only to current and earlier positions\n\nHelper functions:\n\n```python\ndef causal_mask(seq_len) -> np.ndarray    # upper-tri of -inf\ndef softmax(x, axis=-1) -> np.ndarray     # numerically stable\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/attention-causal/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/attention-causal/python/attention.py",
          "language": "python",
          "content": "\"\"\"Causal (masked) self-attention from scratch using NumPy.\"\"\"\n\nimport numpy as np\n\n\ndef softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n    \"\"\"Numerically stable softmax.\"\"\"\n    x_max = np.max(x, axis=axis, keepdims=True)\n    e_x = np.exp(x - x_max)\n    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n\n\ndef causal_mask(seq_len: int) -> np.ndarray:\n    \"\"\"Return an upper-triangular matrix of -inf (causal mask).\n\n    Shape: (seq_len, seq_len). mask[i][j] = -inf if j > i, else 0.\n    \"\"\"\n    mask = np.zeros((seq_len, seq_len))\n    mask[np.triu_indices(seq_len, k=1)] = -np.inf\n    return mask\n\n\ndef causal_self_attention(\n    Q: np.ndarray, K: np.ndarray, V: np.ndarray\n) -> np.ndarray:\n    \"\"\"Compute causal self-attention.\n\n    Args:\n        Q: queries, shape (seq_len, d_k)\n        K: keys,    shape (seq_len, d_k)\n        V: values,  shape (seq_len, d_v)\n\n    Returns:\n        output of shape (seq_len, d_v)\n    \"\"\"\n    seq_len, d_k = Q.shape\n    scores = Q @ K.T / np.sqrt(d_k)\n    scores = scores + causal_mask(seq_len)\n    weights = softmax(scores, axis=-1)\n    return weights @ V\n"
        },
        {
          "path": "modules/ml/llm/attention-causal/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn causal_mask(seq_len: usize) -> Vec<Vec<f64>> {\n    let mut mask = vec![vec![0.0; seq_len]; seq_len];\n    for i in 0..seq_len {\n        for j in (i + 1)..seq_len {\n            mask[i][j] = f64::NEG_INFINITY;\n        }\n    }\n    mask\n}\n\nfn softmax(row: &[f64]) -> Vec<f64> {\n    let m = row\n.iter()\n.copied()\n.fold(f64::NEG_INFINITY, f64::max);\n    let exps: Vec<f64> = row.iter().map(|x| (x - m).exp()).collect();\n    let sum: f64 = exps.iter().sum();\n    exps.iter().map(|e| e / sum).collect()\n}\n\nfn matmul(a: &[Vec<f64>], b: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = b[0].len();\n    let mut out = vec![vec![0.0; cols]; rows];\n    for i in 0..rows {\n        for j in 0..cols {\n            let mut acc = 0.0;\n            for k in 0..b.len() {\n                acc += a[i][k] * b[k][j];\n            }\n            out[i][j] = acc;\n        }\n    }\n    out\n}\n\nfn transpose(a: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = a[0].len();\n    let mut out = vec![vec![0.0; rows]; cols];\n    for i in 0..rows {\n        for j in 0..cols {\n            out[j][i] = a[i][j];\n        }\n    }\n    out\n}\n\npub fn causal_self_attention(q: &[Vec<f64>], k: &[Vec<f64>], v: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let dk = k[0].len() as f64;\n    let scores = matmul(q, &transpose(k));\n    let mask = causal_mask(scores.len());\n    let mut masked = scores.clone();\n    for i in 0..scores.len() {\n        for j in 0..scores[0].len() {\n            masked[i][j] = scores[i][j] / dk.sqrt() + mask[i][j];\n        }\n    }\n    let weights: Vec<Vec<f64>> = masked.iter().map(|row| softmax(row)).collect();\n    matmul(&weights, v)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "dpo",
      "title": "DPO (Direct Preference Optimization)",
      "path": "modules/ml/llm/dpo",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# DPO (Direct Preference Optimization)\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nDPO optimizes policy preferences directly using a reference model.\n\n## Math\n\n$$\\text{Demo loss: } L = -\\log\\left(\\sigma\\left(\\beta(\\Delta \\log \\pi - \\Delta \\log \\pi_{\\text{ref}})\\right)\\right)$$\n\n## Function\n\n```python\ndef dpo_loss(delta_logp: float, delta_logp_ref: float, beta: float = 0.1) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/dpo/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/dpo/python/dpo.py",
          "language": "python",
          "content": "import math\n\n\ndef dpo_loss(delta_logp: float, delta_logp_ref: float, beta: float = 0.1) -> float:\n    diff = beta * (delta_logp - delta_logp_ref)\n    return -math.log(1 / (1 + math.exp(-diff)))\n"
        },
        {
          "path": "modules/ml/llm/dpo/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn dpo_loss(delta_logp: f64, delta_logp_ref: f64, beta: f64) -> f64 {\n    let diff = beta * (delta_logp - delta_logp_ref);\n    -1.0 / (1.0 + (-diff).exp()).ln()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "embeddings",
      "title": "Embeddings",
      "path": "modules/ml/llm/embeddings",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Embeddings\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nEmbeddings map discrete token ids to continuous vectors used by the model.\n\n## Math\n\n$$Given embedding matrix \\texttt{E ∈ R\\^\\{\\}\\{V×d\\}}, the embedding for token \\texttt{i} is \\texttt{E[i]}.$$\n\n## Function\n\n```python\ndef embed(tokens: list[int], embeddings: list[list[float]]) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/embeddings/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/embeddings/python/embeddings.py",
          "language": "python",
          "content": "from __future__ import annotations\n\n\ndef embed(tokens: list[int], embeddings: list[list[float]]) -> list[list[float]]:\n    return [embeddings[i] for i in tokens]\n"
        },
        {
          "path": "modules/ml/llm/embeddings/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn embed(tokens: &[usize], embeddings: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    tokens.iter().map(|&i| embeddings[i].clone()).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "fp16-bf16-fp8",
      "title": "FP16/BF16/FP8 Precision",
      "path": "modules/ml/llm/fp16-bf16-fp8",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# FP16/BF16/FP8 Precision\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nLower-precision formats trade accuracy for speed and memory.\n\n## Math\n\n$$\\text{Quantize a float by rounding mantissa bits (demo).}$$\n\n## Function\n\n```python\ndef quantize_fp(x: float, mantissa_bits: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/fp16-bf16-fp8/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/fp16-bf16-fp8/python/fp16_bf16_fp8.py",
          "language": "python",
          "content": "import math\n\n\ndef quantize_fp(x: float, mantissa_bits: int) -> float:\n    if x == 0.0:\n        return 0.0\n    exp = int(math.floor(math.log2(abs(x))))\n    scale = 2 ** (mantissa_bits - exp)\n    return round(x * scale) / scale\n"
        },
        {
          "path": "modules/ml/llm/fp16-bf16-fp8/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn quantize_fp(x: f64, mantissa_bits: i32) -> f64 {\n    if x == 0.0 {\n        return 0.0;\n    }\n    let exp = x.abs().log2().floor() as i32;\n    let scale = 2f64.powi(mantissa_bits - exp);\n    (x * scale).round() / scale\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "inference-head-pruning",
      "title": "Inference Head Pruning",
      "path": "modules/ml/llm/inference-head-pruning",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Inference Head Pruning\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nHead pruning removes less useful attention heads to reduce compute at inference.\n\n## Math\n\n$$\\text{Prune by slicing head blocks from weight matrices.}$$\n\n## Function\n\n```python\ndef prune_heads(weights: list[list[float]], keep: list[int], head_dim: int) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/inference-head-pruning/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/inference-head-pruning/python/inference_head_pruning.py",
          "language": "python",
          "content": "from __future__ import annotations\n\n\ndef prune_heads(weights: list[list[float]], keep: list[int], head_dim: int) -> list[list[float]]:\n    pruned = []\n    for row in weights:\n        new_row = []\n        for h in keep:\n            start = h * head_dim\n            end = start + head_dim\n            new_row.extend(row[start:end])\n        pruned.append(new_row)\n    return pruned\n"
        },
        {
          "path": "modules/ml/llm/inference-head-pruning/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn prune_heads(weights: &[Vec<f64>], keep: &[usize], head_dim: usize) -> Vec<Vec<f64>> {\n    let mut out = Vec::with_capacity(weights.len());\n    for row in weights {\n        let mut new_row = Vec::new();\n        for &h in keep {\n            let start = h * head_dim;\n            let end = start + head_dim;\n            new_row.extend_from_slice(&row[start..end]);\n        }\n        out.push(new_row);\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "int8-int4-quantization",
      "title": "INT8/INT4 Quantization",
      "path": "modules/ml/llm/int8-int4-quantization",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# INT8/INT4 Quantization\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nInteger quantization maps floats to a small integer range with a scale.\n\n## Math\n\n$$Quantize: q = round(x / s), dequantize: x̂ = q * s.$$\n\n## Function\n\n```python\ndef quantize_int(x: float, bits: int, scale: float) -> int:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/int8-int4-quantization/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/int8-int4-quantization/python/int8_int4_quantization.py",
          "language": "python",
          "content": "def quantize_int(x: float, bits: int, scale: float) -> int:\n    qmax = 2 ** (bits - 1) - 1\n    q = int(round(x / scale))\n    return max(-qmax, min(qmax, q))\n"
        },
        {
          "path": "modules/ml/llm/int8-int4-quantization/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn quantize_int(x: f64, bits: i32, scale: f64) -> i32 {\n    let qmax = (1 << (bits - 1)) - 1;\n    let q = (x / scale).round() as i32;\n    q.max(-qmax).min(qmax)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "kl-regularization",
      "title": "KL Regularization",
      "path": "modules/ml/llm/kl-regularization",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# KL Regularization\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nKL regularization keeps a policy close to a reference distribution.\n\n## Math\n\n$$L = \\beta\\, \\mathrm{KL}(p\\|q)$$\n\n## Function\n\n```python\ndef kl_penalty(p: list[float], q: list[float], beta: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/kl-regularization/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/kl-regularization/python/kl_regularization.py",
          "language": "python",
          "content": "import math\n\n\ndef kl_penalty(p: list[float], q: list[float], beta: float) -> float:\n    kl = 0.0\n    for pi, qi in zip(p, q):\n        if pi > 0 and qi > 0:\n            kl += pi * math.log(pi / qi)\n    return beta * kl\n"
        },
        {
          "path": "modules/ml/llm/kl-regularization/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn kl_penalty(p: &[f64], q: &[f64], beta: f64) -> f64 {\n    let mut kl = 0.0;\n    for (&pi, &qi) in p.iter().zip(q.iter()) {\n        if pi > 0.0 && qi > 0.0 {\n            kl += pi * (pi / qi).ln();\n        }\n    }\n    beta * kl\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "lora",
      "title": "LoRA",
      "path": "modules/ml/llm/lora",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# LoRA\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nLoRA applies a low-rank update $\\Delta W = AB$ while keeping base weights frozen.\n\n## Math\n\n$$W' = W + \\frac{\\alpha}{r} AB$$\n\n## Function\n\n```python\ndef lora_update(w: list[list[float]], a: list[list[float]], b: list[list[float]], alpha: float) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/lora/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/lora/python/lora.py",
          "language": "python",
          "content": "from __future__ import annotations\n\n\ndef _matmul(a: list[list[float]], b: list[list[float]]) -> list[list[float]]:\n    out = []\n    for i in range(len(a)):\n        row = []\n        for j in range(len(b[0])):\n            row.append(sum(a[i][k] * b[k][j] for k in range(len(b))))\n        out.append(row)\n    return out\n\n\ndef lora_update(w: list[list[float]], a: list[list[float]], b: list[list[float]], alpha: float) -> list[list[float]]:\n    r = len(b)\n    delta = _matmul(a, b)\n    scale = alpha / r\n    return [[w[i][j] + scale * delta[i][j] for j in range(len(w[0]))] for i in range(len(w))]\n"
        },
        {
          "path": "modules/ml/llm/lora/rust/src/lib.rs",
          "language": "rust",
          "content": "fn matmul(a: &[Vec<f64>], b: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = b[0].len();\n    let mut out = vec![vec![0.0; cols]; rows];\n    for i in 0..rows {\n        for j in 0..cols {\n            let mut acc = 0.0;\n            for k in 0..b.len() {\n                acc += a[i][k] * b[k][j];\n            }\n            out[i][j] = acc;\n        }\n    }\n    out\n}\n\npub fn lora_update(w: &[Vec<f64>], a: &[Vec<f64>], b: &[Vec<f64>], alpha: f64) -> Vec<Vec<f64>> {\n    let r = b.len() as f64;\n    let delta = matmul(a, b);\n    let scale = alpha / r;\n    let mut out = w.to_vec();\n    for i in 0..w.len() {\n        for j in 0..w[0].len() {\n            out[i][j] += scale * delta[i][j];\n        }\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "moe-routing",
      "title": "MoE Routing",
      "path": "modules/ml/llm/moe-routing",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# MoE Routing\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nMixture-of-Experts routes tokens to top-k experts based on gating scores.\n\n## Math\n\n$$Select top-k gates g_i; output = sum(g_i * expert_i).$$\n\n## Function\n\n```python\ndef moe_combine(experts: list[list[float]], gates: list[float], k: int) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/moe-routing/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/moe-routing/python/moe_routing.py",
          "language": "python",
          "content": "def moe_combine(experts: list[list[float]], gates: list[float], k: int) -> list[float]:\n    pairs = sorted(enumerate(gates), key=lambda x: x[1], reverse=True)[:k]\n    total = sum(score for _, score in pairs)\n    weights = [(idx, score / total) for idx, score in pairs]\n    out = [0.0 for _ in experts[0]]\n    for idx, w in weights:\n        for j, val in enumerate(experts[idx]):\n            out[j] += w * val\n    return out\n"
        },
        {
          "path": "modules/ml/llm/moe-routing/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn moe_combine(experts: &[Vec<f64>], gates: &[f64], k: usize) -> Vec<f64> {\n    let mut pairs: Vec<(usize, f64)> = gates.iter().copied().enumerate().collect();\n    pairs.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());\n    let top = &pairs[..k];\n    let total: f64 = top.iter().map(|(_, s)| *s).sum();\n    let mut out = vec![0.0; experts[0].len()];\n    for (idx, score) in top {\n        let weight = score / total;\n        for j in 0..experts[*idx].len() {\n            out[j] += weight * experts[*idx][j];\n        }\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "multi-head-attention",
      "title": "Multi-Head Attention",
      "path": "modules/ml/llm/multi-head-attention",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Multi-Head Attention\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nMulti-head attention runs attention in parallel subspaces and concatenates results.\n\n## Math\n\n$$Split Q,K,V into heads: head_i = Attention(Q_i,K_i,V_i).$$\n\n## Function\n\n```python\ndef multi_head_attention(q: list[list[float]], k: list[list[float]], v: list[list[float]], heads: int) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/multi-head-attention/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/multi-head-attention/python/multi_head_attention.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nimport math\n\n\ndef _softmax(row: list[float]) -> list[float]:\n    m = max(row)\n    exps = [math.exp(x - m) for x in row]\n    s = sum(exps)\n    return [e / s for e in exps]\n\n\ndef _matmul(a: list[list[float]], b: list[list[float]]) -> list[list[float]]:\n    out = []\n    for i in range(len(a)):\n        row = []\n        for j in range(len(b[0])):\n            row.append(sum(a[i][k] * b[k][j] for k in range(len(b))))\n        out.append(row)\n    return out\n\n\ndef _transpose(a: list[list[float]]) -> list[list[float]]:\n    return [list(col) for col in zip(*a)]\n\n\ndef _attention(q: list[list[float]], k: list[list[float]], v: list[list[float]]) -> list[list[float]]:\n    dk = len(k[0])\n    scores = _matmul(q, _transpose(k))\n    scaled = [[val / math.sqrt(dk) for val in row] for row in scores]\n    weights = [_softmax(row) for row in scaled]\n    return _matmul(weights, v)\n\n\ndef multi_head_attention(q: list[list[float]], k: list[list[float]], v: list[list[float]], heads: int) -> list[list[float]]:\n    d_model = len(q[0])\n    head_dim = d_model // heads\n    outputs = []\n    for h in range(heads):\n        q_h = [row[h * head_dim:(h + 1) * head_dim] for row in q]\n        k_h = [row[h * head_dim:(h + 1) * head_dim] for row in k]\n        v_h = [row[h * head_dim:(h + 1) * head_dim] for row in v]\n        outputs.append(_attention(q_h, k_h, v_h))\n    merged = []\n    for i in range(len(q)):\n        merged_row = []\n        for h in range(heads):\n            merged_row.extend(outputs[h][i])\n        merged.append(merged_row)\n    return merged\n"
        },
        {
          "path": "modules/ml/llm/multi-head-attention/rust/src/lib.rs",
          "language": "rust",
          "content": "fn softmax(row: &[f64]) -> Vec<f64> {\n    let m = row.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let exps: Vec<f64> = row.iter().map(|x| (x - m).exp()).collect();\n    let sum: f64 = exps.iter().sum();\n    exps.iter().map(|e| e / sum).collect()\n}\n\nfn matmul(a: &[Vec<f64>], b: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = b[0].len();\n    let mut out = vec![vec![0.0; cols]; rows];\n    for i in 0..rows {\n        for j in 0..cols {\n            let mut acc = 0.0;\n            for k in 0..b.len() {\n                acc += a[i][k] * b[k][j];\n            }\n            out[i][j] = acc;\n        }\n    }\n    out\n}\n\nfn transpose(a: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = a[0].len();\n    let mut out = vec![vec![0.0; rows]; cols];\n    for i in 0..rows {\n        for j in 0..cols {\n            out[j][i] = a[i][j];\n        }\n    }\n    out\n}\n\nfn attention(q: &[Vec<f64>], k: &[Vec<f64>], v: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let dk = k[0].len() as f64;\n    let scores = matmul(q, &transpose(k));\n    let scaled: Vec<Vec<f64>> = scores\n.iter()\n.map(|row| row.iter().map(|x| x / dk.sqrt()).collect())\n.collect();\n    let weights: Vec<Vec<f64>> = scaled.iter().map(|row| softmax(row)).collect();\n    matmul(&weights, v)\n}\n\npub fn multi_head_attention(q: &[Vec<f64>], k: &[Vec<f64>], v: &[Vec<f64>], heads: usize) -> Vec<Vec<f64>> {\n    let d_model = q[0].len();\n    let head_dim = d_model / heads;\n    let mut outputs: Vec<Vec<Vec<f64>>> = Vec::with_capacity(heads);\n    for h in 0..heads {\n        let start = h * head_dim;\n        let end = (h + 1) * head_dim;\n        let q_h: Vec<Vec<f64>> = q.iter().map(|row| row[start..end].to_vec()).collect();\n        let k_h: Vec<Vec<f64>> = k.iter().map(|row| row[start..end].to_vec()).collect();\n        let v_h: Vec<Vec<f64>> = v.iter().map(|row| row[start..end].to_vec()).collect();\n        outputs.push(attention(&q_h, &k_h, &v_h));\n    }\n    let mut merged = vec![vec![0.0; d_model]; q.len()];\n    for i in 0..q.len() {\n        let mut offset = 0;\n        for h in 0..heads {\n            for val in &outputs[h][i] {\n                merged[i][offset] = *val;\n                offset += 1;\n            }\n        }\n    }\n    merged\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "positional-encoding",
      "title": "Positional Encoding",
      "path": "modules/ml/llm/positional-encoding",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Positional Encoding\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nPositional encodings add order information to token embeddings.\n\n## Math\n\n$$Sinusoidal: \\texttt{PE[pos,2i]=sin(pos/10000\\^\\{\\}\\{2i/d\\})}, \\texttt{PE[pos,2i+1]=cos(pos/10000\\^\\{\\}\\{2i/d\\})}.$$\n\n## Function\n\n```python\ndef sinusoidal_position(pos: int, d_model: int) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/positional-encoding/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/positional-encoding/python/positional_encoding.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nimport math\n\n\ndef sinusoidal_position(pos: int, d_model: int) -> list[float]:\n    values = []\n    for i in range(d_model):\n        angle = pos / (10000 ** (2 * (i // 2) / d_model))\n        if i % 2 == 0:\n            values.append(math.sin(angle))\n        else:\n            values.append(math.cos(angle))\n    return values\n"
        },
        {
          "path": "modules/ml/llm/positional-encoding/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn sinusoidal_position(pos: usize, d_model: usize) -> Vec<f64> {\n    let mut values = Vec::with_capacity(d_model);\n    for i in 0..d_model {\n        let angle = (pos as f64) / 10000f64.powf(2.0 * ((i / 2) as f64) / d_model as f64);\n        if i % 2 == 0 {\n            values.push(angle.sin());\n        } else {\n            values.push(angle.cos());\n        }\n    }\n    values\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "preference-learning",
      "title": "Preference Learning",
      "path": "modules/ml/llm/preference-learning",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Preference Learning\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nPreference learning optimizes models with pairwise comparisons between outputs.\n\n## Math\n\n$$\\text{Pairwise logistic loss: } L = -\\log\\left(\\sigma(\\text{score}_{\\text{chosen}} - \\text{score}_{\\text{rejected}})\\right)$$\n\n## Function\n\n```python\ndef preference_loss(score_chosen: float, score_rejected: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/preference-learning/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/preference-learning/python/preference_learning.py",
          "language": "python",
          "content": "import math\n\n\ndef preference_loss(score_chosen: float, score_rejected: float) -> float:\n    diff = score_chosen - score_rejected\n    return -math.log(1 / (1 + math.exp(-diff)))\n"
        },
        {
          "path": "modules/ml/llm/preference-learning/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn preference_loss(score_chosen: f64, score_rejected: f64) -> f64 {\n    let diff = score_chosen - score_rejected;\n    -1.0 / (1.0 + (-diff).exp()).ln()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "pretraining",
      "title": "Pretraining (Next-Token Loss)",
      "path": "modules/ml/llm/pretraining",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Pretraining (Next-Token Loss)\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nPretraining optimizes next-token prediction on large corpora.\n\n## Math\n\n$$\\text{Cross-entropy: } L = -\\log\\left(\\mathrm{softmax}(\\text{logits})_{\\text{target}}\\right)$$\n\n## Function\n\n```python\ndef next_token_loss(logits: list[list[float]], targets: list[int]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/pretraining/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/pretraining/python/pretraining.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nimport math\n\n\ndef _softmax(row: list[float]) -> list[float]:\n    m = max(row)\n    exps = [math.exp(x - m) for x in row]\n    s = sum(exps)\n    return [e / s for e in exps]\n\n\ndef next_token_loss(logits: list[list[float]], targets: list[int]) -> float:\n    total = 0.0\n    for row, tgt in zip(logits, targets):\n        probs = _softmax(row)\n        total += -math.log(probs[tgt])\n    return total / len(targets)\n"
        },
        {
          "path": "modules/ml/llm/pretraining/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn next_token_loss(logits: &[Vec<f64>], targets: &[usize]) -> f64 {\n    let mut total = 0.0;\n    for (row, &tgt) in logits.iter().zip(targets.iter()) {\n        let m = row.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n        let exps: Vec<f64> = row.iter().map(|x| (x - m).exp()).collect();\n        let sum: f64 = exps.iter().sum();\n        let prob = exps[tgt] / sum;\n        total += -prob.ln();\n    }\n    total / targets.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "ptx-anchoring",
      "title": "PTX Anchoring",
      "path": "modules/ml/llm/ptx-anchoring",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# PTX Anchoring\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nPTX anchoring mixes a small amount of pretraining loss during alignment to retain base behavior.\n\n## Math\n\n$$L = (1-\\alpha) L_{\\text{align}} + \\alpha L_{\\text{ptx}}$$\n\n## Function\n\n```python\ndef anchored_loss(align_loss: float, ptx_loss: float, alpha: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/ptx-anchoring/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/ptx-anchoring/python/ptx_anchoring.py",
          "language": "python",
          "content": "def anchored_loss(align_loss: float, ptx_loss: float, alpha: float) -> float:\n    return (1 - alpha) * align_loss + alpha * ptx_loss\n"
        },
        {
          "path": "modules/ml/llm/ptx-anchoring/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn anchored_loss(align_loss: f64, ptx_loss: f64, alpha: f64) -> f64 {\n    (1.0 - alpha) * align_loss + alpha * ptx_loss\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "qlora",
      "title": "QLoRA",
      "path": "modules/ml/llm/qlora",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# QLoRA\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nQLoRA combines low-rank updates with quantized base weights.\n\n## Math\n\n$$\\text{Quantize base weights, then apply LoRA-style low-rank update.}$$\n\n## Function\n\n```python\ndef qlora_update(w: list[list[float]], a: list[list[float]], b: list[list[float]], alpha: float, scale: float) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/qlora/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/qlora/python/qlora.py",
          "language": "python",
          "content": "from __future__ import annotations\n\n\ndef _matmul(a: list[list[float]], b: list[list[float]]) -> list[list[float]]:\n    out = []\n    for i in range(len(a)):\n        row = []\n        for j in range(len(b[0])):\n            row.append(sum(a[i][k] * b[k][j] for k in range(len(b))))\n        out.append(row)\n    return out\n\n\ndef _quantize(w: list[list[float]], scale: float) -> list[list[float]]:\n    return [[round(val / scale) * scale for val in row] for row in w]\n\n\ndef qlora_update(w: list[list[float]], a: list[list[float]], b: list[list[float]], alpha: float, scale: float) -> list[list[float]]:\n    w_q = _quantize(w, scale)\n    r = len(b)\n    delta = _matmul(a, b)\n    factor = alpha / r\n    return [[w_q[i][j] + factor * delta[i][j] for j in range(len(w_q[0]))] for i in range(len(w_q))]\n"
        },
        {
          "path": "modules/ml/llm/qlora/rust/src/lib.rs",
          "language": "rust",
          "content": "fn matmul(a: &[Vec<f64>], b: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = b[0].len();\n    let mut out = vec![vec![0.0; cols]; rows];\n    for i in 0..rows {\n        for j in 0..cols {\n            let mut acc = 0.0;\n            for k in 0..b.len() {\n                acc += a[i][k] * b[k][j];\n            }\n            out[i][j] = acc;\n        }\n    }\n    out\n}\n\nfn quantize(w: &[Vec<f64>], scale: f64) -> Vec<Vec<f64>> {\n    w.iter()\n.map(|row| row.iter().map(|v| (v / scale).round() * scale).collect())\n.collect()\n}\n\npub fn qlora_update(w: &[Vec<f64>], a: &[Vec<f64>], b: &[Vec<f64>], alpha: f64, scale: f64) -> Vec<Vec<f64>> {\n    let w_q = quantize(w, scale);\n    let r = b.len() as f64;\n    let delta = matmul(a, b);\n    let factor = alpha / r;\n    let mut out = w_q;\n    for i in 0..out.len() {\n        for j in 0..out[0].len() {\n            out[i][j] += factor * delta[i][j];\n        }\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "rlhf",
      "title": "RLHF (Reward Model Loss)",
      "path": "modules/ml/llm/rlhf",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# RLHF (Reward Model Loss)\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nRLHF uses a reward model trained on preference pairs before policy optimization.\n\n## Math\n\n$$\\text{Reward model loss uses the pairwise logistic objective.}$$\n\n## Function\n\n```python\ndef reward_model_loss(chosen: float, rejected: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/rlhf/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/rlhf/python/rlhf.py",
          "language": "python",
          "content": "import math\n\n\ndef reward_model_loss(chosen: float, rejected: float) -> float:\n    diff = chosen - rejected\n    return -math.log(1 / (1 + math.exp(-diff)))\n"
        },
        {
          "path": "modules/ml/llm/rlhf/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn reward_model_loss(chosen: f64, rejected: f64) -> f64 {\n    let diff = chosen - rejected;\n    -1.0 / (1.0 + (-diff).exp()).ln()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "self-attention",
      "title": "Self-Attention",
      "path": "modules/ml/llm/self-attention",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Self-Attention\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nSelf-attention lets each token attend to all others by comparing queries and keys.\n\n## Math\n\n$$\\mathrm{Attention}(Q,K,V) = \\mathrm{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V$$\n\n## Function\n\n```python\ndef self_attention(q: list[list[float]], k: list[list[float]], v: list[list[float]]) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/self-attention/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/self-attention/python/self_attention.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nimport math\n\n\ndef _softmax(row: list[float]) -> list[float]:\n    m = max(row)\n    exps = [math.exp(x - m) for x in row]\n    s = sum(exps)\n    return [e / s for e in exps]\n\n\ndef _matmul(a: list[list[float]], b: list[list[float]]) -> list[list[float]]:\n    out = []\n    for i in range(len(a)):\n        row = []\n        for j in range(len(b[0])):\n            row.append(sum(a[i][k] * b[k][j] for k in range(len(b))))\n        out.append(row)\n    return out\n\n\ndef _transpose(a: list[list[float]]) -> list[list[float]]:\n    return [list(col) for col in zip(*a)]\n\n\ndef self_attention(q: list[list[float]], k: list[list[float]], v: list[list[float]]) -> list[list[float]]:\n    dk = len(k[0])\n    scores = _matmul(q, _transpose(k))\n    scaled = [[val / math.sqrt(dk) for val in row] for row in scores]\n    weights = [_softmax(row) for row in scaled]\n    return _matmul(weights, v)\n"
        },
        {
          "path": "modules/ml/llm/self-attention/rust/src/lib.rs",
          "language": "rust",
          "content": "fn softmax(row: &[f64]) -> Vec<f64> {\n    let m = row.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let exps: Vec<f64> = row.iter().map(|x| (x - m).exp()).collect();\n    let sum: f64 = exps.iter().sum();\n    exps.iter().map(|e| e / sum).collect()\n}\n\nfn matmul(a: &[Vec<f64>], b: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = b[0].len();\n    let mut out = vec![vec![0.0; cols]; rows];\n    for i in 0..rows {\n        for j in 0..cols {\n            let mut acc = 0.0;\n            for k in 0..b.len() {\n                acc += a[i][k] * b[k][j];\n            }\n            out[i][j] = acc;\n        }\n    }\n    out\n}\n\nfn transpose(a: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = a[0].len();\n    let mut out = vec![vec![0.0; rows]; cols];\n    for i in 0..rows {\n        for j in 0..cols {\n            out[j][i] = a[i][j];\n        }\n    }\n    out\n}\n\npub fn self_attention(q: &[Vec<f64>], k: &[Vec<f64>], v: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let dk = k[0].len() as f64;\n    let scores = matmul(q, &transpose(k));\n    let scaled: Vec<Vec<f64>> = scores\n.iter()\n.map(|row| row.iter().map(|x| x / dk.sqrt()).collect())\n.collect();\n    let weights: Vec<Vec<f64>> = scaled.iter().map(|row| softmax(row)).collect();\n    matmul(&weights, v)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "sparse-attention",
      "title": "Sparse Attention",
      "path": "modules/ml/llm/sparse-attention",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Sparse Attention\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nSparse attention limits attention to a local window to reduce complexity.\n\n## Math\n\n$$Mask scores outside a window so softmax only sees nearby positions.$$\n\n## Function\n\n```python\ndef window_mask(seq_len: int, window: int) -> list[list[int]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/sparse-attention/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/sparse-attention/python/sparse_attention.py",
          "language": "python",
          "content": "def window_mask(seq_len: int, window: int) -> list[list[int]]:\n    mask = [[0 for _ in range(seq_len)] for _ in range(seq_len)]\n    for i in range(seq_len):\n        for j in range(seq_len):\n            if abs(i - j) <= window:\n                mask[i][j] = 1\n    return mask\n"
        },
        {
          "path": "modules/ml/llm/sparse-attention/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn window_mask(seq_len: usize, window: usize) -> Vec<Vec<i32>> {\n    let mut mask = vec![vec![0; seq_len]; seq_len];\n    for i in 0..seq_len {\n        for j in 0..seq_len {\n            let dist = if i > j { i - j } else { j - i };\n            if dist <= window {\n                mask[i][j] = 1;\n            }\n        }\n    }\n    mask\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "supervised-fine-tuning",
      "title": "Supervised Fine-Tuning",
      "path": "modules/ml/llm/supervised-fine-tuning",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Supervised Fine-Tuning\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nSFT trains on curated prompt-response pairs with teacher forcing.\n\n## Math\n\n$$\\text{Masked cross-entropy over supervised target tokens.}$$\n\n## Function\n\n```python\ndef sft_loss(logits: list[list[float]], targets: list[int], mask: list[int]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/supervised-fine-tuning/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/supervised-fine-tuning/python/supervised_fine_tuning.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nimport math\n\n\ndef _softmax(row: list[float]) -> list[float]:\n    m = max(row)\n    exps = [math.exp(x - m) for x in row]\n    s = sum(exps)\n    return [e / s for e in exps]\n\n\ndef sft_loss(logits: list[list[float]], targets: list[int], mask: list[int]) -> float:\n    total = 0.0\n    count = 0\n    for row, tgt, keep in zip(logits, targets, mask):\n        if not keep:\n            continue\n        probs = _softmax(row)\n        total += -math.log(probs[tgt])\n        count += 1\n    return total / max(count, 1)\n"
        },
        {
          "path": "modules/ml/llm/supervised-fine-tuning/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn sft_loss(logits: &[Vec<f64>], targets: &[usize], mask: &[u8]) -> f64 {\n    let mut total = 0.0;\n    let mut count = 0;\n    for ((row, &tgt), &keep) in logits.iter().zip(targets.iter()).zip(mask.iter()) {\n        if keep == 0 {\n            continue;\n        }\n        let m = row.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n        let exps: Vec<f64> = row.iter().map(|x| (x - m).exp()).collect();\n        let sum: f64 = exps.iter().sum();\n        let prob = exps[tgt] / sum;\n        total += -prob.ln();\n        count += 1;\n    }\n    if count == 0 { 0.0 } else { total / count as f64 }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "tokenization",
      "title": "Tokenization",
      "path": "modules/ml/llm/tokenization",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Tokenization\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nTokenization maps raw text into discrete tokens that a model can embed.\n\n## Math\n\n$$Let \\texttt{V} be the vocabulary. Tokenization is a mapping \\texttt{f: text -> [id\\_1,..., id\\_n]}.$$\n\n## Function\n\n```python\ndef tokenize(text: str, vocab: dict[str, int]) -> list[int]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/tokenization/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/tokenization/python/tokenization.py",
          "language": "python",
          "content": "from __future__ import annotations\n\ndef build_vocab(texts: list[str]) -> dict[str, int]:\n    vocab: dict[str, int] = {}\n    for text in texts:\n        for token in text.lower().split():\n            if token not in vocab:\n                vocab[token] = len(vocab)\n    return vocab\n\n\ndef tokenize(text: str, vocab: dict[str, int]) -> list[int]:\n    return [vocab[token] for token in text.lower().split() if token in vocab]\n"
        },
        {
          "path": "modules/ml/llm/tokenization/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashMap;\n\npub fn build_vocab(texts: &[&str]) -> HashMap<String, usize> {\n    let mut vocab = HashMap::new();\n    for text in texts {\n        for token in text.to_lowercase().split_whitespace() {\n            if !vocab.contains_key(token) {\n                let id = vocab.len();\n                vocab.insert(token.to_string(), id);\n            }\n        }\n    }\n    vocab\n}\n\npub fn tokenize(text: &str, vocab: &HashMap<String, usize>) -> Vec<usize> {\n    text.to_lowercase()\n.split_whitespace()\n.filter_map(|tok| vocab.get(tok).copied())\n.collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "transformer",
      "title": "Transformer Block",
      "path": "modules/ml/llm/transformer",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Transformer Block\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nA transformer block applies self-attention, a feed-forward network, and residual connections.\n\n## Math\n\n$$Block(x) = x + Attention(x); then x = x + FFN(x).$$\n\n## Function\n\n```python\ndef transformer_block(x: list[list[float]], w1: list[list[float]], w2: list[list[float]]) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/transformer/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/transformer/python/transformer.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nimport math\n\n\ndef _softmax(row: list[float]) -> list[float]:\n    m = max(row)\n    exps = [math.exp(x - m) for x in row]\n    s = sum(exps)\n    return [e / s for e in exps]\n\n\ndef _matmul(a: list[list[float]], b: list[list[float]]) -> list[list[float]]:\n    out = []\n    for i in range(len(a)):\n        row = []\n        for j in range(len(b[0])):\n            row.append(sum(a[i][k] * b[k][j] for k in range(len(b))))\n        out.append(row)\n    return out\n\n\ndef _transpose(a: list[list[float]]) -> list[list[float]]:\n    return [list(col) for col in zip(*a)]\n\n\ndef _self_attention(x: list[list[float]]) -> list[list[float]]:\n    dk = len(x[0])\n    scores = _matmul(x, _transpose(x))\n    scaled = [[val / math.sqrt(dk) for val in row] for row in scores]\n    weights = [_softmax(row) for row in scaled]\n    return _matmul(weights, x)\n\n\ndef _relu_row(row: list[float]) -> list[float]:\n    return [max(0.0, x) for x in row]\n\n\ndef transformer_block(x: list[list[float]], w1: list[list[float]], w2: list[list[float]]) -> list[list[float]]:\n    attn = _self_attention(x)\n    res1 = [[x[i][j] + attn[i][j] for j in range(len(x[0]))] for i in range(len(x))]\n    ffn_hidden = _matmul(res1, w1)\n    ffn_hidden = [_relu_row(row) for row in ffn_hidden]\n    ffn = _matmul(ffn_hidden, w2)\n    res2 = [[res1[i][j] + ffn[i][j] for j in range(len(ffn[0]))] for i in range(len(ffn))]\n    return res2\n"
        },
        {
          "path": "modules/ml/llm/transformer/rust/src/lib.rs",
          "language": "rust",
          "content": "fn softmax(row: &[f64]) -> Vec<f64> {\n    let m = row.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let exps: Vec<f64> = row.iter().map(|x| (x - m).exp()).collect();\n    let sum: f64 = exps.iter().sum();\n    exps.iter().map(|e| e / sum).collect()\n}\n\nfn matmul(a: &[Vec<f64>], b: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = b[0].len();\n    let mut out = vec![vec![0.0; cols]; rows];\n    for i in 0..rows {\n        for j in 0..cols {\n            let mut acc = 0.0;\n            for k in 0..b.len() {\n                acc += a[i][k] * b[k][j];\n            }\n            out[i][j] = acc;\n        }\n    }\n    out\n}\n\nfn transpose(a: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = a[0].len();\n    let mut out = vec![vec![0.0; rows]; cols];\n    for i in 0..rows {\n        for j in 0..cols {\n            out[j][i] = a[i][j];\n        }\n    }\n    out\n}\n\nfn relu(row: &[f64]) -> Vec<f64> {\n    row.iter().map(|x| x.max(0.0)).collect()\n}\n\nfn self_attention(x: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let dk = x[0].len() as f64;\n    let scores = matmul(x, &transpose(x));\n    let scaled: Vec<Vec<f64>> = scores\n.iter()\n.map(|row| row.iter().map(|x| x / dk.sqrt()).collect())\n.collect();\n    let weights: Vec<Vec<f64>> = scaled.iter().map(|row| softmax(row)).collect();\n    matmul(&weights, x)\n}\n\npub fn transformer_block(x: &[Vec<f64>], w1: &[Vec<f64>], w2: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let attn = self_attention(x);\n    let mut res1 = x.to_vec();\n    for i in 0..res1.len() {\n        for j in 0..res1[0].len() {\n            res1[i][j] += attn[i][j];\n        }\n    }\n    let ffn_hidden = matmul(&res1, w1);\n    let ffn_hidden: Vec<Vec<f64>> = ffn_hidden.iter().map(|row| relu(row)).collect();\n    let ffn = matmul(&ffn_hidden, w2);\n    let mut res2 = res1.clone();\n    for i in 0..res2.len() {\n        for j in 0..res2[0].len() {\n            res2[i][j] += ffn[i][j];\n        }\n    }\n    res2\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "ab-testing",
      "title": "A/B Testing",
      "path": "modules/ml/mlops/ab-testing",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# A/B Testing\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nA/B testing compares conversion rates between variants.\n\n## Math\n\n$$rate = conversions / trials$$\n\n## Function\n\n```python\ndef conversion_rate(conversions: int, trials: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/ab-testing/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/ab-testing/python/ab_testing.py",
          "language": "python",
          "content": "def conversion_rate(conversions: int, trials: int) -> float:\n    return conversions / trials if trials > 0 else 0.0\n"
        },
        {
          "path": "modules/ml/mlops/ab-testing/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn conversion_rate(conversions: i32, trials: i32) -> f64 {\n    if trials > 0 { conversions as f64 / trials as f64 } else { 0.0 }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "batch-vs-realtime",
      "title": "Batch vs Real-Time Inference",
      "path": "modules/ml/mlops/batch-vs-realtime",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# Batch vs Real-Time Inference\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nBatching trades latency for throughput.\n\n## Math\n\n$$throughput_batch > throughput_realtime$$\n\n## Function\n\n```python\ndef choose_mode(batch_size: int) -> str:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/batch-vs-realtime/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/batch-vs-realtime/python/batch_vs_realtime.py",
          "language": "python",
          "content": "def choose_mode(batch_size: int) -> str:\n    return \"batch\" if batch_size > 1 else \"realtime\"\n"
        },
        {
          "path": "modules/ml/mlops/batch-vs-realtime/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn choose_mode(batch_size: usize) -> String {\n    if batch_size > 1 { \"batch\".to_string() } else { \"realtime\".to_string() }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "canary-deployment",
      "title": "Canary Deployment",
      "path": "modules/ml/mlops/canary-deployment",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# Canary Deployment\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nCanary routes a small fraction of traffic to a new model.\n\n## Math\n\n$$new_pct = new / total$$\n\n## Function\n\n```python\ndef split_traffic(total: int, canary_pct: float) -> tuple[int, int]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/canary-deployment/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/canary-deployment/python/canary_deployment.py",
          "language": "python",
          "content": "def split_traffic(total: int, canary_pct: float) -> tuple[int, int]:\n    canary = int(total * canary_pct)\n    return canary, total - canary\n"
        },
        {
          "path": "modules/ml/mlops/canary-deployment/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn split_traffic(total: i32, canary_pct: f64) -> (i32, i32) {\n    let canary = (total as f64 * canary_pct) as i32;\n    (canary, total - canary)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "data-quality-checks",
      "title": "Data Quality Checks",
      "path": "modules/ml/mlops/data-quality-checks",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# Data Quality Checks\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nCheck missing values or out-of-range features.\n\n## Math\n\n$$missing_rate = missing / N$$\n\n## Function\n\n```python\ndef missing_rate(values: list[float | None]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/data-quality-checks/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/data-quality-checks/python/data_quality_checks.py",
          "language": "python",
          "content": "from typing import Optional\n\n\ndef missing_rate(values: list[Optional[float]]) -> float:\n    missing = sum(1 for v in values if v is None)\n    return missing / len(values)\n"
        },
        {
          "path": "modules/ml/mlops/data-quality-checks/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn missing_rate(values: &[Option<f64>]) -> f64 {\n    let missing = values.iter().filter(|v| v.is_none()).count();\n    missing as f64 / values.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "etl-pipeline",
      "title": "ETL Pipeline",
      "path": "modules/ml/mlops/etl-pipeline",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# ETL Pipeline\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nETL extracts raw data, transforms it, and loads it for training or serving.\n\n## Math\n\n$$x' = \\frac{x - \\mu}{\\sigma}$$\n\n## Function\n\n```python\ndef normalize(values: list[float]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/etl-pipeline/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/etl-pipeline/python/etl_pipeline.py",
          "language": "python",
          "content": "import math\n\n\ndef normalize(values: list[float]) -> list[float]:\n    mean = sum(values) / len(values)\n    var = sum((v - mean) ** 2 for v in values) / len(values)\n    std = math.sqrt(var) if var > 0 else 1.0\n    return [(v - mean) / std for v in values]\n"
        },
        {
          "path": "modules/ml/mlops/etl-pipeline/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn normalize(values: &[f64]) -> Vec<f64> {\n    let mean: f64 = values.iter().sum::<f64>() / values.len() as f64;\n    let var: f64 = values.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / values.len() as f64;\n    let std = if var > 0.0 { var.sqrt() } else { 1.0 };\n    values.iter().map(|v| (v - mean) / std).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "feature-drift-psi",
      "title": "Feature Drift Detection (PSI)",
      "path": "modules/ml/mlops/feature-drift-psi",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# Feature Drift Detection (PSI)\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nPSI compares expected vs actual distributions.\n\n## Math\n\n$$\\mathrm{PSI} = \\sum_i (a_i - e_i) \\ln\\left(\\frac{a_i}{e_i}\\right)$$\n\n## Function\n\n```python\ndef psi(expected: list[float], actual: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/feature-drift-psi/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/feature-drift-psi/python/feature_drift_psi.py",
          "language": "python",
          "content": "import math\n\n\ndef psi(expected: list[float], actual: list[float]) -> float:\n    total = 0.0\n    for e, a in zip(expected, actual):\n        if e > 0 and a > 0:\n            total += (a - e) * math.log(a / e)\n    return total\n"
        },
        {
          "path": "modules/ml/mlops/feature-drift-psi/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn psi(expected: &[f64], actual: &[f64]) -> f64 {\n    let mut total = 0.0;\n    for (&e, &a) in expected.iter().zip(actual.iter()) {\n        if e > 0.0 && a > 0.0 {\n            total += (a - e) * (a / e).ln();\n        }\n    }\n    total\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "offline-online-inference",
      "title": "Offline vs Online Inference",
      "path": "modules/ml/mlops/offline-online-inference",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# Offline vs Online Inference\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nOffline runs in batches; online responds per request.\n\n## Math\n\n$$latency_online << latency_batch$$\n\n## Function\n\n```python\ndef is_online(latency_ms: float, threshold_ms: float = 100.0) -> bool:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/offline-online-inference/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/offline-online-inference/python/offline_online_inference.py",
          "language": "python",
          "content": "def is_online(latency_ms: float, threshold_ms: float = 100.0) -> bool:\n    return latency_ms <= threshold_ms\n"
        },
        {
          "path": "modules/ml/mlops/offline-online-inference/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn is_online(latency_ms: f64, threshold_ms: f64) -> bool {\n    latency_ms <= threshold_ms\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "prediction-monitoring",
      "title": "Prediction Distribution Monitoring",
      "path": "modules/ml/mlops/prediction-monitoring",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# Prediction Distribution Monitoring\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nTrack shifts in prediction mean or variance over time.\n\n## Math\n\n$$\\Delta \\mu = \\left|\\mu_{\\text{new}} - \\mu_{\\text{old}}\\right|$$\n\n## Function\n\n```python\ndef mean_shift(old: list[float], new: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/prediction-monitoring/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/prediction-monitoring/python/prediction_monitoring.py",
          "language": "python",
          "content": "def mean_shift(old: list[float], new: list[float]) -> float:\n    mean_old = sum(old) / len(old)\n    mean_new = sum(new) / len(new)\n    return abs(mean_new - mean_old)\n"
        },
        {
          "path": "modules/ml/mlops/prediction-monitoring/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn mean_shift(old: &[f64], new: &[f64]) -> f64 {\n    let mean_old: f64 = old.iter().sum::<f64>() / old.len() as f64;\n    let mean_new: f64 = new.iter().sum::<f64>() / new.len() as f64;\n    (mean_new - mean_old).abs()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "request-batching",
      "title": "Request Batching",
      "path": "modules/ml/mlops/request-batching",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# Request Batching\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nBatch requests to improve throughput.\n\n## Math\n\n$$batches = ceil(n / batch_size)$$\n\n## Function\n\n```python\ndef batch_requests(n: int, batch_size: int) -> int:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/request-batching/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/request-batching/python/request_batching.py",
          "language": "python",
          "content": "import math\n\n\ndef batch_requests(n: int, batch_size: int) -> int:\n    return math.ceil(n / batch_size)\n"
        },
        {
          "path": "modules/ml/mlops/request-batching/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn batch_requests(n: usize, batch_size: usize) -> usize {\n    (n + batch_size - 1) / batch_size\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "sla-metrics",
      "title": "SLA Metrics",
      "path": "modules/ml/mlops/sla-metrics",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# SLA Metrics\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nSLA metrics track latency or error thresholds.\n\n## Math\n\n$$violation_rate = violations / total$$\n\n## Function\n\n```python\ndef violation_rate(violations: int, total: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/sla-metrics/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/sla-metrics/python/sla_metrics.py",
          "language": "python",
          "content": "def violation_rate(violations: int, total: int) -> float:\n    return violations / total if total > 0 else 0.0\n"
        },
        {
          "path": "modules/ml/mlops/sla-metrics/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn violation_rate(violations: i32, total: i32) -> f64 {\n    if total > 0 { violations as f64 / total as f64 } else { 0.0 }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "adaboost",
      "title": "AdaBoost",
      "path": "modules/ml/models/adaboost",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# AdaBoost\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nAdaBoost reweights samples to focus on errors.\n\n## Math\n\n$$w_i \\leftarrow w_i \\cdot \\exp\\left(\\alpha \\mathbb{I}[\\text{misclassified}]\\right)$$\n\n## Function\n\n```python\ndef update_weights(weights: list[float], errors: list[int], alpha: float) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/adaboost/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/adaboost/python/adaboost.py",
          "language": "python",
          "content": "import math\n\n\ndef update_weights(weights: list[float], errors: list[int], alpha: float) -> list[float]:\n    updated = [w * math.exp(alpha * e) for w, e in zip(weights, errors)]\n    s = sum(updated)\n    return [w / s for w in updated]\n"
        },
        {
          "path": "modules/ml/models/adaboost/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn update_weights(weights: &[f64], errors: &[i32], alpha: f64) -> Vec<f64> {\n    let mut updated: Vec<f64> = weights\n.iter()\n.zip(errors.iter())\n.map(|(w, e)| w * (alpha * (*e as f64)).exp())\n.collect();\n    let sum: f64 = updated.iter().sum();\n    for w in updated.iter_mut() {\n        *w /= sum;\n    }\n    updated\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "bernoulli-naive-bayes",
      "title": "Bernoulli Naive Bayes",
      "path": "modules/ml/models/bernoulli-naive-bayes",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Bernoulli Naive Bayes\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nBernoulli NB models binary features per class.\n\n## Math\n\n$$p(x|y)=∏ p_i^{x_i} (1-p_i)^{1-x_i}$$\n\n## Function\n\n```python\ndef bernoulli_log_likelihood(x: list[int], prob: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/bernoulli-naive-bayes/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/bernoulli-naive-bayes/python/bernoulli_naive_bayes.py",
          "language": "python",
          "content": "import math\n\n\ndef bernoulli_log_likelihood(x: list[int], prob: list[float]) -> float:\n    ll = 0.0\n    for xi, p in zip(x, prob):\n        ll += xi * math.log(p) + (1 - xi) * math.log(1 - p)\n    return ll\n"
        },
        {
          "path": "modules/ml/models/bernoulli-naive-bayes/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn bernoulli_log_likelihood(x: &[i32], prob: &[f64]) -> f64 {\n    let mut ll = 0.0;\n    for (&xi, &p) in x.iter().zip(prob.iter()) {\n        ll += (xi as f64) * p.ln() + (1.0 - xi as f64) * (1.0 - p).ln();\n    }\n    ll\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "dbscan",
      "title": "DBSCAN",
      "path": "modules/ml/models/dbscan",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# DBSCAN\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nDBSCAN groups points by density using eps-neighborhoods.\n\n## Math\n\n$$\\mathrm{neighbors}(x) = \\{y \\mid \\mathrm{dist}(x,y) \\le \\varepsilon\\}$$\n\n## Function\n\n```python\ndef neighbors(points: list[list[float]], idx: int, eps: float) -> list[int]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/dbscan/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/dbscan/python/dbscan.py",
          "language": "python",
          "content": "def neighbors(points: list[list[float]], idx: int, eps: float) -> list[int]:\n    base = points[idx]\n    out = []\n    for i, p in enumerate(points):\n        dist = sum((a - b) ** 2 for a, b in zip(base, p)) ** 0.5\n        if dist <= eps:\n            out.append(i)\n    return out\n"
        },
        {
          "path": "modules/ml/models/dbscan/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn neighbors(points: &[Vec<f64>], idx: usize, eps: f64) -> Vec<usize> {\n    let base = &points[idx];\n    let mut out = Vec::new();\n    for (i, p) in points.iter().enumerate() {\n        let dist = base\n.iter()\n.zip(p.iter())\n.map(|(a, b)| (a - b).powi(2))\n.sum::<f64>()\n.sqrt();\n        if dist <= eps {\n            out.push(i);\n        }\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "decision-trees",
      "title": "Decision Trees",
      "path": "modules/ml/models/decision-trees",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Decision Trees\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nDecision trees split data to reduce impurity.\n\n## Math\n\n$$\\mathrm{Gini} = 1 - \\sum_c p_c^2$$\n\n## Function\n\n```python\ndef gini_impurity(labels: list[int]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/decision-trees/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/decision-trees/python/decision_trees.py",
          "language": "python",
          "content": "from collections import Counter\n\n\ndef gini_impurity(labels: list[int]) -> float:\n    counts = Counter(labels)\n    n = len(labels)\n    return 1 - sum((c / n) ** 2 for c in counts.values())\n"
        },
        {
          "path": "modules/ml/models/decision-trees/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashMap;\n\npub fn gini_impurity(labels: &[i32]) -> f64 {\n    let mut counts: HashMap<i32, usize> = HashMap::new();\n    for label in labels {\n        *counts.entry(*label).or_insert(0) += 1;\n    }\n    let n = labels.len() as f64;\n    let mut sum_sq = 0.0;\n    for cnt in counts.values() {\n        let p = *cnt as f64 / n;\n        sum_sq += p * p;\n    }\n    1.0 - sum_sq\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "elastic-net",
      "title": "Elastic Net",
      "path": "modules/ml/models/elastic-net",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Elastic Net\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nElastic Net combines L1 (Lasso) and L2 (Ridge) penalties to balance sparsity and stability.\n\n## Math\n\n$$L = L_0 + \\lambda_1 \\lVert w \\rVert_1 + \\lambda_2 \\lVert w \\rVert_2^2$$\n\n## Function\n\n```python\ndef elastic_net_penalty(w: list[float], l1: float, l2: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/elastic-net/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/elastic-net/python/elastic_net.py",
          "language": "python",
          "content": "def elastic_net_penalty(w: list[float], l1: float, l2: float) -> float:\n    return l1 * sum(abs(v) for v in w) + l2 * sum(v * v for v in w)\n"
        },
        {
          "path": "modules/ml/models/elastic-net/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn elastic_net_penalty(w: &[f64], l1: f64, l2: f64) -> f64 {\n    let l1_term: f64 = w.iter().map(|v| v.abs()).sum();\n    let l2_term: f64 = w.iter().map(|v| v * v).sum();\n    l1 * l1_term + l2 * l2_term\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "gaussian-naive-bayes",
      "title": "Gaussian Naive Bayes",
      "path": "modules/ml/models/gaussian-naive-bayes",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Gaussian Naive Bayes\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nGaussian NB assumes features are independent Gaussians per class.\n\n## Math\n\n$$p(x|y)=\\prod_i \\mathcal{N}(x_i;\\mu_i,\\sigma_i^2)$$\n\n## Function\n\n```python\ndef gaussian_log_likelihood(x: list[float], mean: list[float], var: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/gaussian-naive-bayes/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/gaussian-naive-bayes/python/gaussian_naive_bayes.py",
          "language": "python",
          "content": "import math\n\n\ndef gaussian_log_likelihood(x: list[float], mean: list[float], var: list[float]) -> float:\n    ll = 0.0\n    for xi, mu, v in zip(x, mean, var):\n        ll += -0.5 * (math.log(2 * math.pi * v) + ((xi - mu) ** 2) / v)\n    return ll\n"
        },
        {
          "path": "modules/ml/models/gaussian-naive-bayes/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn gaussian_log_likelihood(x: &[f64], mean: &[f64], var: &[f64]) -> f64 {\n    let mut ll = 0.0;\n    for ((xi, mu), v) in x.iter().zip(mean.iter()).zip(var.iter()) {\n        ll += -0.5 * ((2.0 * std::f64::consts::PI * v).ln() + (xi - mu).powi(2) / v);\n    }\n    ll\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "gaussian-process-regression",
      "title": "Gaussian Process Regression",
      "path": "modules/ml/models/gaussian-process-regression",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Gaussian Process Regression\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nGPR predicts using a kernel to measure similarity.\n\n## Math\n\n$$k(x,x') = \\exp\\left(-\\frac{\\lVert x-x' \\rVert^2}{2\\ell^2}\\right)$$\n\n## Function\n\n```python\ndef rbf_kernel(x: list[float], y: list[float], length_scale: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/gaussian-process-regression/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/gaussian-process-regression/python/gaussian_process_regression.py",
          "language": "python",
          "content": "import math\n\n\ndef rbf_kernel(x: list[float], y: list[float], length_scale: float) -> float:\n    dist2 = sum((a - b) ** 2 for a, b in zip(x, y))\n    return math.exp(-dist2 / (2 * length_scale ** 2))\n"
        },
        {
          "path": "modules/ml/models/gaussian-process-regression/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn rbf_kernel(x: &[f64], y: &[f64], length_scale: f64) -> f64 {\n    let dist2: f64 = x.iter().zip(y.iter()).map(|(a, b)| (a - b).powi(2)).sum();\n    (-dist2 / (2.0 * length_scale * length_scale)).exp()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "k-means",
      "title": "K-Means",
      "path": "modules/ml/models/k-means",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# K-Means\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nK-Means assigns points to nearest centroid.\n\n## Math\n\n$$c_i = \\arg\\min_j \\lVert x_i - \\mu_j \\rVert$$\n\n## Function\n\n```python\ndef assign(points: list[list[float]], centroids: list[list[float]]) -> list[int]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/k-means/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/k-means/python/k_means.py",
          "language": "python",
          "content": "def assign(points: list[list[float]], centroids: list[list[float]]) -> list[int]:\n    assignments = []\n    for p in points:\n        dists = [sum((pi - ci) ** 2 for pi, ci in zip(p, c)) for c in centroids]\n        assignments.append(min(range(len(dists)), key=lambda i: dists[i]))\n    return assignments\n"
        },
        {
          "path": "modules/ml/models/k-means/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn assign(points: &[Vec<f64>], centroids: &[Vec<f64>]) -> Vec<usize> {\n    let mut assignments = Vec::with_capacity(points.len());\n    for p in points {\n        let mut best = 0;\n        let mut best_dist = f64::INFINITY;\n        for (i, c) in centroids.iter().enumerate() {\n            let dist: f64 = p.iter().zip(c.iter()).map(|(pi, ci)| (pi - ci).powi(2)).sum();\n            if dist < best_dist {\n                best_dist = dist;\n                best = i;\n            }\n        }\n        assignments.push(best);\n    }\n    assignments\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "knn",
      "title": "K-Nearest Neighbors",
      "path": "modules/ml/models/knn",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# K-Nearest Neighbors\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nKNN predicts by majority vote of nearest points.\n\n## Math\n\n$$\\hat{c} = \\arg\\max_c \\sum_{i \\in \\mathcal{N}_k} \\mathbb{I}[y_i = c]$$\n\n## Function\n\n```python\ndef knn_predict(distances: list[float], labels: list[int], k: int) -> int:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/knn/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/knn/python/knn.py",
          "language": "python",
          "content": "from collections import Counter\n\n\ndef knn_predict(distances: list[float], labels: list[int], k: int) -> int:\n    idxs = sorted(range(len(distances)), key=lambda i: distances[i])[:k]\n    counts = Counter(labels[i] for i in idxs)\n    return counts.most_common(1)[0][0]\n"
        },
        {
          "path": "modules/ml/models/knn/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashMap;\n\npub fn knn_predict(distances: &[f64], labels: &[i32], k: usize) -> i32 {\n    let mut idxs: Vec<usize> = (0..distances.len()).collect();\n    idxs.sort_by(|&a, &b| distances[a].partial_cmp(&distances[b]).unwrap());\n    let mut counts: HashMap<i32, usize> = HashMap::new();\n    for &i in idxs.iter().take(k) {\n        *counts.entry(labels[i]).or_insert(0) += 1;\n    }\n    counts.into_iter().max_by_key(|(_, c)| *c).unwrap().0\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "linear-regression",
      "title": "Linear Regression",
      "path": "modules/ml/models/linear-regression",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Linear Regression\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nLinear regression predicts a target as a linear function of features.\n\n## Math\n\n$$y = w · x + b$$\n\n## Function\n\n```python\ndef predict(x: list[float], w: list[float], b: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/linear-regression/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/linear-regression/python/linear_regression.py",
          "language": "python",
          "content": "def predict(x: list[float], w: list[float], b: float) -> float:\n    return sum(wi * xi for wi, xi in zip(w, x)) + b\n"
        },
        {
          "path": "modules/ml/models/linear-regression/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn predict(x: &[f64], w: &[f64], b: f64) -> f64 {\n    let mut out = b;\n    for (wi, xi) in w.iter().zip(x.iter()) {\n        out += wi * xi;\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "logistic-regression",
      "title": "Logistic Regression",
      "path": "modules/ml/models/logistic-regression",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Logistic Regression\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nLogistic regression models class probability with a sigmoid.\n\n## Math\n\n$$p = 1/(1+e^{-(w·x+b)})$$\n\n## Function\n\n```python\ndef predict_proba(x: list[float], w: list[float], b: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/logistic-regression/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/logistic-regression/python/logistic_regression.py",
          "language": "python",
          "content": "import math\n\n\ndef predict_proba(x: list[float], w: list[float], b: float) -> float:\n    z = sum(wi * xi for wi, xi in zip(w, x)) + b\n    return 1 / (1 + math.exp(-z))\n"
        },
        {
          "path": "modules/ml/models/logistic-regression/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn predict_proba(x: &[f64], w: &[f64], b: f64) -> f64 {\n    let mut z = b;\n    for (wi, xi) in w.iter().zip(x.iter()) {\n        z += wi * xi;\n    }\n    1.0 / (1.0 + (-z).exp())\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "pca",
      "title": "PCA",
      "path": "modules/ml/models/pca",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# PCA\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nPCA finds directions of maximum variance.\n\n## Math\n\n$$v_1 = \\arg\\max_{\\lVert v \\rVert = 1} v^\\top \\Sigma v$$\n\n## Function\n\n```python\ndef pca_first_component_2d(points: list[list[float]]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/pca/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/pca/python/pca.py",
          "language": "python",
          "content": "import math\n\n\ndef pca_first_component_2d(points: list[list[float]]) -> list[float]:\n    n = len(points)\n    mean_x = sum(p[0] for p in points) / n\n    mean_y = sum(p[1] for p in points) / n\n    cov_xx = sum((p[0] - mean_x) ** 2 for p in points) / n\n    cov_yy = sum((p[1] - mean_y) ** 2 for p in points) / n\n    cov_xy = sum((p[0] - mean_x) * (p[1] - mean_y) for p in points) / n\n    # eigenvector for largest eigenvalue of 2x2\n    trace = cov_xx + cov_yy\n    det = cov_xx * cov_yy - cov_xy * cov_xy\n    eig1 = trace / 2 + math.sqrt(max(0.0, (trace / 2) ** 2 - det))\n    if cov_xy == 0:\n        vec = [1.0, 0.0] if cov_xx >= cov_yy else [0.0, 1.0]\n    else:\n        vec = [eig1 - cov_yy, cov_xy]\n    norm = math.sqrt(vec[0] ** 2 + vec[1] ** 2)\n    return [vec[0] / norm, vec[1] / norm]\n"
        },
        {
          "path": "modules/ml/models/pca/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn pca_first_component_2d(points: &[Vec<f64>]) -> Vec<f64> {\n    let n = points.len() as f64;\n    let mean_x: f64 = points.iter().map(|p| p[0]).sum::<f64>() / n;\n    let mean_y: f64 = points.iter().map(|p| p[1]).sum::<f64>() / n;\n    let cov_xx: f64 = points.iter().map(|p| (p[0] - mean_x).powi(2)).sum::<f64>() / n;\n    let cov_yy: f64 = points.iter().map(|p| (p[1] - mean_y).powi(2)).sum::<f64>() / n;\n    let cov_xy: f64 = points.iter().map(|p| (p[0] - mean_x) * (p[1] - mean_y)).sum::<f64>() / n;\n    let trace = cov_xx + cov_yy;\n    let det = cov_xx * cov_yy - cov_xy * cov_xy;\n    let eig1 = trace / 2.0 + ((trace / 2.0).powi(2) - det).max(0.0).sqrt();\n    let (mut vx, mut vy) = if cov_xy == 0.0 {\n        if cov_xx >= cov_yy { (1.0, 0.0) } else { (0.0, 1.0) }\n    } else {\n        (eig1 - cov_yy, cov_xy)\n    };\n    let norm = (vx * vx + vy * vy).sqrt();\n    vx /= norm;\n    vy /= norm;\n    vec![vx, vy]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "random-forest",
      "title": "Random Forest (Bagging)",
      "path": "modules/ml/models/random-forest",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Random Forest (Bagging)\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nRandom forests train many trees on bootstrap samples.\n\n## Math\n\n$$Bootstrap sample size = N with replacement.$$\n\n## Function\n\n```python\ndef bootstrap_indices(n: int, seed: int = 0) -> list[int]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/random-forest/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/random-forest/python/random_forest.py",
          "language": "python",
          "content": "def bootstrap_indices(n: int, seed: int = 0) -> list[int]:\n    idxs = []\n    state = seed\n    for _ in range(n):\n        state = (1103515245 * state + 12345) % (2**31)\n        idxs.append(state % n)\n    return idxs\n"
        },
        {
          "path": "modules/ml/models/random-forest/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn bootstrap_indices(n: usize, seed: u64) -> Vec<usize> {\n    let mut idxs = Vec::with_capacity(n);\n    let mut state = seed;\n    for _ in 0..n {\n        state = (1103515245u64.wrapping_mul(state).wrapping_add(12345)) % (1u64 << 31);\n        idxs.push((state as usize) % n);\n    }\n    idxs\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "softmax-regression",
      "title": "Softmax Regression",
      "path": "modules/ml/models/softmax-regression",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Softmax Regression\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nSoftmax regression generalizes logistic regression to multiple classes.\n\n## Math\n\n$$p_i = \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)}$$\n\n## Function\n\n```python\ndef softmax(logits: list[float]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/softmax-regression/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/softmax-regression/python/softmax_regression.py",
          "language": "python",
          "content": "import math\n\n\ndef softmax(logits: list[float]) -> list[float]:\n    m = max(logits)\n    exps = [math.exp(x - m) for x in logits]\n    s = sum(exps)\n    return [e / s for e in exps]\n"
        },
        {
          "path": "modules/ml/models/softmax-regression/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn softmax(logits: &[f64]) -> Vec<f64> {\n    let m = logits.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let exps: Vec<f64> = logits.iter().map(|x| (x - m).exp()).collect();\n    let sum: f64 = exps.iter().sum();\n    exps.iter().map(|e| e / sum).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "svm-pegasos",
      "title": "SVM (Pegasos)",
      "path": "modules/ml/models/svm-pegasos",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# SVM (Pegasos)\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nPegasos is a stochastic subgradient method for SVMs.\n\n## Math\n\n$$\nw \\leftarrow\n\\begin{cases}\n(1-\\text{lr}\\,\\lambda)w + \\text{lr}\\, y x, & y(w^\\top x) < 1 \\\\\n(1-\\text{lr}\\,\\lambda)w, & \\text{otherwise}\n\\end{cases}\n$$\n\n## Function\n\n```python\ndef pegasos_step(w: list[float], x: list[float], y: int, lr: float, lam: float) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/svm-pegasos/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/svm-pegasos/python/svm_pegasos.py",
          "language": "python",
          "content": "def pegasos_step(w: list[float], x: list[float], y: int, lr: float, lam: float) -> list[float]:\n    dot = sum(wi * xi for wi, xi in zip(w, x))\n    scale = 1 - lr * lam\n    w = [wi * scale for wi in w]\n    if y * dot < 1:\n        w = [wi + lr * y * xi for wi, xi in zip(w, x)]\n    return w\n"
        },
        {
          "path": "modules/ml/models/svm-pegasos/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn pegasos_step(w: &[f64], x: &[f64], y: i32, lr: f64, lam: f64) -> Vec<f64> {\n    let dot: f64 = w.iter().zip(x.iter()).map(|(wi, xi)| wi * xi).sum();\n    let scale = 1.0 - lr * lam;\n    let mut out: Vec<f64> = w.iter().map(|wi| wi * scale).collect();\n    if (y as f64) * dot < 1.0 {\n        for i in 0..out.len() {\n            out[i] += lr * (y as f64) * x[i];\n        }\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "adagrad",
      "title": "Adagrad",
      "path": "modules/ml/optimization/adagrad",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Adagrad\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nAdagrad accumulates squared gradients for per-parameter learning rates.\n\n## Math\n\n$$\n\\begin{aligned}\nG_t &= G_{t-1} + g_t^2 \\\\\nw_{t+1} &= w_t - \\text{lr} \\frac{g_t}{\\sqrt{G_t} + \\epsilon}\n\\end{aligned}\n$$\n\n## Function\n\n```python\ndef adagrad_step(w: float, grad: float, g2: float, lr: float, eps: float) -> tuple[float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/adagrad/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/adagrad/python/adagrad.py",
          "language": "python",
          "content": "import math\n\n\ndef adagrad_step(w: float, grad: float, g2: float, lr: float, eps: float) -> tuple[float, float]:\n    g2 = g2 + grad ** 2\n    w = w - lr * grad / (math.sqrt(g2) + eps)\n    return w, g2\n"
        },
        {
          "path": "modules/ml/optimization/adagrad/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn adagrad_step(w: f64, grad: f64, g2: f64, lr: f64, eps: f64) -> (f64, f64) {\n    let g2_new = g2 + grad * grad;\n    let w_new = w - lr * grad / (g2_new.sqrt() + eps);\n    (w_new, g2_new)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "adam",
      "title": "Adam",
      "path": "modules/ml/optimization/adam",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Adam\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nAdam combines momentum and adaptive learning rates.\n\n## Math\n\n$$\n\\begin{aligned}\nm_t &= \\beta_1 m_{t-1} + (1-\\beta_1) g_t \\\\\nv_t &= \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 \\\\\nw_{t+1} &= w_t - \\text{lr} \\frac{m_t}{\\sqrt{v_t} + \\epsilon}\n\\end{aligned}\n$$\n\n## Function\n\n```python\ndef adam_step(w: float, grad: float, m: float, v: float, lr: float, beta1: float, beta2: float, eps: float) -> tuple[float, float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/adam/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/adam/python/adam.py",
          "language": "python",
          "content": "import math\n\n\ndef adam_step(w: float, grad: float, m: float, v: float, lr: float, beta1: float, beta2: float, eps: float) -> tuple[float, float, float]:\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    w = w - lr * m / (math.sqrt(v) + eps)\n    return w, m, v\n"
        },
        {
          "path": "modules/ml/optimization/adam/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn adam_step(w: f64, grad: f64, m: f64, v: f64, lr: f64, beta1: f64, beta2: f64, eps: f64) -> (f64, f64, f64) {\n    let m_new = beta1 * m + (1.0 - beta1) * grad;\n    let v_new = beta2 * v + (1.0 - beta2) * grad * grad;\n    let w_new = w - lr * m_new / (v_new.sqrt() + eps);\n    (w_new, m_new, v_new)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "adamw",
      "title": "AdamW",
      "path": "modules/ml/optimization/adamw",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# AdamW\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nAdamW decouples weight decay from adaptive updates.\n\n## Math\n\n$$w = w - lr * (m/√v) - lr*wd*w$$\n\n## Function\n\n```python\ndef adamw_step(w: float, grad: float, m: float, v: float, lr: float, wd: float, beta1: float, beta2: float, eps: float) -> tuple[float, float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/adamw/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/adamw/python/adamw.py",
          "language": "python",
          "content": "import math\n\n\ndef adamw_step(w: float, grad: float, m: float, v: float, lr: float, wd: float, beta1: float, beta2: float, eps: float) -> tuple[float, float, float]:\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    w = w - lr * m / (math.sqrt(v) + eps) - lr * wd * w\n    return w, m, v\n"
        },
        {
          "path": "modules/ml/optimization/adamw/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn adamw_step(w: f64, grad: f64, m: f64, v: f64, lr: f64, wd: f64, beta1: f64, beta2: f64, eps: f64) -> (f64, f64, f64) {\n    let m_new = beta1 * m + (1.0 - beta1) * grad;\n    let v_new = beta2 * v + (1.0 - beta2) * grad * grad;\n    let w_new = w - lr * m_new / (v_new.sqrt() + eps) - lr * wd * w;\n    (w_new, m_new, v_new)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "detect-nans",
      "title": "Detecting NaNs",
      "path": "modules/ml/optimization/detect-nans",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Detecting NaNs\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nCheck for NaNs to catch divergence early.\n\n## Math\n\n$$is_nan(x) = true if x != x$$\n\n## Function\n\n```python\ndef has_nan(values: list[float]) -> bool:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/detect-nans/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/detect-nans/python/detect_nans.py",
          "language": "python",
          "content": "import math\n\n\ndef has_nan(values: list[float]) -> bool:\n    return any(math.isnan(v) for v in values)\n"
        },
        {
          "path": "modules/ml/optimization/detect-nans/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn has_nan(values: &[f64]) -> bool {\n    values.iter().any(|v| v.is_nan())\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "gradient-clipping",
      "title": "Gradient Clipping",
      "path": "modules/ml/optimization/gradient-clipping",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Gradient Clipping\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nClip gradient norm to avoid exploding gradients.\n\n## Math\n\n$$g \\leftarrow g \\cdot \\min\\left(1, \\frac{\\text{clip}}{\\lVert g \\rVert}\\right)$$\n\n## Function\n\n```python\ndef clip_norm(grad: list[float], clip: float) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/gradient-clipping/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/gradient-clipping/python/gradient_clipping.py",
          "language": "python",
          "content": "import math\n\n\ndef clip_norm(grad: list[float], clip: float) -> list[float]:\n    norm = math.sqrt(sum(g * g for g in grad))\n    if norm == 0 or norm <= clip:\n        return grad\n    scale = clip / norm\n    return [g * scale for g in grad]\n"
        },
        {
          "path": "modules/ml/optimization/gradient-clipping/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn clip_norm(grad: &[f64], clip: f64) -> Vec<f64> {\n    let norm = grad.iter().map(|g| g * g).sum::<f64>().sqrt();\n    if norm == 0.0 || norm <= clip {\n        return grad.to_vec();\n    }\n    let scale = clip / norm;\n    grad.iter().map(|g| g * scale).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "loss-scaling",
      "title": "Loss Scaling",
      "path": "modules/ml/optimization/loss-scaling",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Loss Scaling\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nLoss scaling multiplies loss to avoid underflow in mixed precision.\n\n## Math\n\n$$scaled_grad = grad * scale$$\n\n## Function\n\n```python\ndef scale_grad(grad: float, scale: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/loss-scaling/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/loss-scaling/python/loss_scaling.py",
          "language": "python",
          "content": "def scale_grad(grad: float, scale: float) -> float:\n    return grad * scale\n"
        },
        {
          "path": "modules/ml/optimization/loss-scaling/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn scale_grad(grad: f64, scale: f64) -> f64 {\n    grad * scale\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "lr-constant",
      "title": "Constant Learning Rate",
      "path": "modules/ml/optimization/lr-constant",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Constant Learning Rate\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nConstant LR keeps the step size fixed.\n\n## Math\n\n$$\\text{lr}_t = \\text{lr}$$\n\n## Function\n\n```python\ndef constant_lr(lr: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/lr-constant/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/lr-constant/python/lr_constant.py",
          "language": "python",
          "content": "def constant_lr(lr: float) -> float:\n    return lr\n"
        },
        {
          "path": "modules/ml/optimization/lr-constant/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn constant_lr(lr: f64) -> f64 { lr }\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "lr-cosine-decay",
      "title": "Cosine Decay",
      "path": "modules/ml/optimization/lr-cosine-decay",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Cosine Decay\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nCosine decay anneals LR smoothly to zero.\n\n## Math\n\n$$\\text{lr}_t = \\text{lr} \\cdot \\frac{1}{2}\\left(1+\\cos\\left(\\frac{\\pi t}{T}\\right)\\right)$$\n\n## Function\n\n```python\ndef cosine_decay(lr: float, t: int, t_max: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/lr-cosine-decay/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/lr-cosine-decay/python/lr_cosine_decay.py",
          "language": "python",
          "content": "import math\n\n\ndef cosine_decay(lr: float, t: int, t_max: int) -> float:\n    return lr * 0.5 * (1 + math.cos(math.pi * t / t_max))\n"
        },
        {
          "path": "modules/ml/optimization/lr-cosine-decay/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn cosine_decay(lr: f64, t: i32, t_max: i32) -> f64 {\n    lr * 0.5 * (1.0 + (std::f64::consts::PI * t as f64 / t_max as f64).cos())\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "lr-exponential-decay",
      "title": "Exponential Decay",
      "path": "modules/ml/optimization/lr-exponential-decay",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Exponential Decay\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nExponential decay reduces LR continuously.\n\n## Math\n\n$$\\text{lr}_t = \\text{lr} \\cdot \\exp(-k t)$$\n\n## Function\n\n```python\ndef exp_decay(lr: float, k: float, t: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/lr-exponential-decay/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/lr-exponential-decay/python/lr_exponential_decay.py",
          "language": "python",
          "content": "import math\n\n\ndef exp_decay(lr: float, k: float, t: float) -> float:\n    return lr * math.exp(-k * t)\n"
        },
        {
          "path": "modules/ml/optimization/lr-exponential-decay/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn exp_decay(lr: f64, k: f64, t: f64) -> f64 {\n    lr * (-k * t).exp()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "lr-step-decay",
      "title": "Step Decay",
      "path": "modules/ml/optimization/lr-step-decay",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Step Decay\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nStep decay drops LR by a factor every k steps.\n\n## Math\n\n$$\\text{lr}_t = \\text{lr} \\cdot \\gamma^{\\lfloor t / \\text{step} \\rfloor}$$\n\n## Function\n\n```python\ndef step_decay(lr: float, step: int, gamma: float, t: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/lr-step-decay/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/lr-step-decay/python/lr_step_decay.py",
          "language": "python",
          "content": "import math\n\n\ndef step_decay(lr: float, step: int, gamma: float, t: int) -> float:\n    return lr * (gamma ** (t // step))\n"
        },
        {
          "path": "modules/ml/optimization/lr-step-decay/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn step_decay(lr: f64, step: i32, gamma: f64, t: i32) -> f64 {\n    let k = t / step;\n    lr * gamma.powi(k)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "lr-warmup",
      "title": "Warmup",
      "path": "modules/ml/optimization/lr-warmup",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Warmup\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nWarmup linearly ramps LR at the start of training.\n\n## Math\n\n$$\\text{lr}_t = \\text{lr} \\cdot \\min\\left(1, \\frac{t}{T_{\\text{warmup}}}\\right)$$\n\n## Function\n\n```python\ndef warmup_lr(lr: float, t: int, warmup_steps: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/lr-warmup/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/lr-warmup/python/lr_warmup.py",
          "language": "python",
          "content": "def warmup_lr(lr: float, t: int, warmup_steps: int) -> float:\n    return lr * min(1.0, t / warmup_steps)\n"
        },
        {
          "path": "modules/ml/optimization/lr-warmup/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn warmup_lr(lr: f64, t: i32, warmup_steps: i32) -> f64 {\n    let factor = (t as f64 / warmup_steps as f64).min(1.0);\n    lr * factor\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "muon-optimizer",
      "title": "Muon Optimizer",
      "path": "modules/ml/optimization/muon-optimizer",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Muon Optimizer\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nMuon is a matrix-aware optimizer: it applies momentum, then orthogonalizes the\n2D update so its directions stay well-conditioned. Practical implementations\northogonalize each matrix update (often via Newton-Schulz iterations). This\nmodule uses a tiny Gram-Schmidt approximation for a learnable demo.\n\n## Math\n\n- $m_t = beta * m_{t-1} + g_t$\n- $Q_t = orthogonalize(m_t)$\n- $W_{t+1} = W_t - lr * Q_t$\n\n## Function\n\n```python\ndef muon_step(\n    weights: list[list[float]],\n    grad: list[list[float]],\n    velocity: list[list[float]] | None,\n    lr: float = 0.1,\n    momentum: float = 0.9,\n) -> tuple[list[list[float]], list[list[float]]]:\n```\n\n## Demo code\n\n```python\nweights = [[1.0, -1.0], [0.5, 0.25]]\ngrad = [[0.2, -0.1], [0.05, 0.4]]\nnew_weights, velocity = muon_step(weights, grad, None, lr=0.1, momentum=0.9)\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/muon-optimizer/python -q\ncargo test --manifest-path modules/ml/optimization/muon-optimizer/rust/Cargo.toml\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/muon-optimizer/python/muon_optimizer.py",
          "language": "python",
          "content": "\"\"\"Muon optimizer demo using Gram-Schmidt orthogonalization.\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom typing import List, Tuple\n\n\nMatrix = List[List[float]]\n\n\ndef _dot(a: List[float], b: List[float]) -> float:\n    return sum(x * y for x, y in zip(a, b))\n\n\ndef _norm(v: List[float]) -> float:\n    return math.sqrt(_dot(v, v))\n\n\ndef gram_schmidt_rows(mat: Matrix, eps: float = 1e-12) -> Matrix:\n    \"\"\"Return an orthonormalized copy of rows via Gram-Schmidt.\"\"\"\n    out: Matrix = []\n    for row in mat:\n        v = row[:]\n        for u in out:\n            proj = _dot(v, u)\n            v = [vi - proj * ui for vi, ui in zip(v, u)]\n        n = _norm(v)\n        if n < eps:\n            out.append([0.0 for _ in v])\n        else:\n            out.append([vi / n for vi in v])\n    return out\n\n\ndef muon_step(\n    weights: Matrix,\n    grad: Matrix,\n    velocity: Matrix | None,\n    lr: float = 0.1,\n    momentum: float = 0.9,\n) -> Tuple[Matrix, Matrix]:\n    \"\"\"Apply a Muon-style update and return (new_weights, new_velocity).\"\"\"\n    if velocity is None:\n        velocity = [[0.0 for _ in row] for row in grad]\n    new_velocity = [\n        [momentum * v + g for v, g in zip(v_row, g_row)]\n        for v_row, g_row in zip(velocity, grad)\n    ]\n    ortho = gram_schmidt_rows(new_velocity)\n    new_weights = [\n        [w - lr * u for w, u in zip(w_row, u_row)]\n        for w_row, u_row in zip(weights, ortho)\n    ]\n    return new_weights, new_velocity\n"
        },
        {
          "path": "modules/ml/optimization/muon-optimizer/rust/src/lib.rs",
          "language": "rust",
          "content": "pub type Matrix = Vec<Vec<f64>>;\n\nfn dot(a: &[f64], b: &[f64]) -> f64 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\nfn norm(v: &[f64]) -> f64 {\n    dot(v, v).sqrt()\n}\n\npub fn gram_schmidt_rows(mat: &Matrix, eps: f64) -> Matrix {\n    let mut out: Matrix = Vec::new();\n    for row in mat.iter() {\n        let mut v = row.clone();\n        for u in out.iter() {\n            let proj = dot(&v, u);\n            for (vi, ui) in v.iter_mut().zip(u.iter()) {\n                *vi -= proj * ui;\n            }\n        }\n        let n = norm(&v);\n        if n < eps {\n            out.push(vec![0.0; v.len()]);\n        } else {\n            out.push(v.iter().map(|vi| vi / n).collect());\n        }\n    }\n    out\n}\n\npub fn muon_step(\n    weights: &Matrix,\n    grad: &Matrix,\n    velocity: Option<&Matrix>,\n    lr: f64,\n    momentum: f64,\n) -> (Matrix, Matrix) {\n    let mut vel = match velocity {\n        Some(v) => v.clone(),\n        None => grad.iter().map(|row| vec![0.0; row.len()]).collect(),\n    };\n    for (v_row, g_row) in vel.iter_mut().zip(grad.iter()) {\n        for (v, g) in v_row.iter_mut().zip(g_row.iter()) {\n            *v = momentum * *v + *g;\n        }\n    }\n    let ortho = gram_schmidt_rows(&vel, 1e-12);\n    let mut new_weights = weights.clone();\n    for (w_row, u_row) in new_weights.iter_mut().zip(ortho.iter()) {\n        for (w, u) in w_row.iter_mut().zip(u_row.iter()) {\n            *w -= lr * u;\n        }\n    }\n    (new_weights, vel)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "nesterov",
      "title": "Nesterov Accelerated Gradient",
      "path": "modules/ml/optimization/nesterov",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Nesterov Accelerated Gradient\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nNesterov uses a lookahead gradient for faster convergence.\n\n## Math\n\n$$\n\\begin{aligned}\nv_t &= \\mu v_{t-1} + g\\left(w_{t-1} - \\text{lr}\\,\\mu v_{t-1}\\right) \\\\\nw_{t+1} &= w_t - \\text{lr}\\, v_t\n\\end{aligned}\n$$\n\n## Function\n\n```python\ndef nesterov_step(w: float, grad: float, v: float, lr: float, mu: float) -> tuple[float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/nesterov/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/nesterov/python/nesterov.py",
          "language": "python",
          "content": "def nesterov_step(w: float, grad: float, v: float, lr: float, mu: float) -> tuple[float, float]:\n    v_new = mu * v + grad\n    w_new = w - lr * (mu * v_new + grad)\n    return w_new, v_new\n"
        },
        {
          "path": "modules/ml/optimization/nesterov/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn nesterov_step(w: f64, grad: f64, v: f64, lr: f64, mu: f64) -> (f64, f64) {\n    let v_new = mu * v + grad;\n    let w_new = w - lr * (mu * v_new + grad);\n    (w_new, v_new)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "rmsprop",
      "title": "RMSProp",
      "path": "modules/ml/optimization/rmsprop",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# RMSProp\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nRMSProp scales learning rates by running average of squared gradients.\n\n## Math\n\n$$\n\\begin{aligned}\nv_t &= \\beta v_{t-1} + (1-\\beta) g_t^2 \\\\\nw_{t+1} &= w_t - \\text{lr} \\frac{g_t}{\\sqrt{v_t} + \\epsilon}\n\\end{aligned}\n$$\n\n## Function\n\n```python\ndef rmsprop_step(w: float, grad: float, v: float, lr: float, beta: float, eps: float) -> tuple[float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/rmsprop/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/rmsprop/python/rmsprop.py",
          "language": "python",
          "content": "import math\n\n\ndef rmsprop_step(w: float, grad: float, v: float, lr: float, beta: float, eps: float) -> tuple[float, float]:\n    v = beta * v + (1 - beta) * (grad ** 2)\n    w = w - lr * grad / (math.sqrt(v) + eps)\n    return w, v\n"
        },
        {
          "path": "modules/ml/optimization/rmsprop/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn rmsprop_step(w: f64, grad: f64, v: f64, lr: f64, beta: f64, eps: f64) -> (f64, f64) {\n    let v_new = beta * v + (1.0 - beta) * grad * grad;\n    let w_new = w - lr * grad / (v_new.sqrt() + eps);\n    (w_new, v_new)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "sgd-momentum",
      "title": "SGD with Momentum",
      "path": "modules/ml/optimization/sgd-momentum",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# SGD with Momentum\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nMomentum accumulates velocity to smooth updates.\n\n## Math\n\n$$\n\\begin{aligned}\nv_t &= \\mu v_{t-1} + g_t \\\\\nw_{t+1} &= w_t - \\text{lr}\\, v_t\n\\end{aligned}\n$$\n\n## Function\n\n```python\ndef momentum_step(w: float, grad: float, v: float, lr: float, mu: float) -> tuple[float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/sgd-momentum/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/sgd-momentum/python/sgd_momentum.py",
          "language": "python",
          "content": "def momentum_step(w: float, grad: float, v: float, lr: float, mu: float) -> tuple[float, float]:\n    v = mu * v + grad\n    w = w - lr * v\n    return w, v\n"
        },
        {
          "path": "modules/ml/optimization/sgd-momentum/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn momentum_step(w: f64, grad: f64, v: f64, lr: f64, mu: f64) -> (f64, f64) {\n    let v_new = mu * v + grad;\n    let w_new = w - lr * v_new;\n    (w_new, v_new)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "sgd",
      "title": "SGD",
      "path": "modules/ml/optimization/sgd",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# SGD\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nSGD updates parameters by stepping opposite the gradient.\n\n## Math\n\n$$w = w - lr * grad$$\n\n## Function\n\n```python\ndef sgd_step(w: float, grad: float, lr: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/sgd/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/sgd/python/sgd.py",
          "language": "python",
          "content": "def sgd_step(w: float, grad: float, lr: float) -> float:\n    return w - lr * grad\n"
        },
        {
          "path": "modules/ml/optimization/sgd/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn sgd_step(w: f64, grad: f64, lr: f64) -> f64 {\n    w - lr * grad\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "bandit-epsilon-greedy",
      "title": "Epsilon-Greedy Bandit",
      "path": "modules/ml/reinforcement-learning/bandit-epsilon-greedy",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# Epsilon-Greedy Bandit\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nThe **multi-armed bandit** problem is a simplified RL setting: an agent\nrepeatedly chooses one of _K_ actions (arms) and receives a stochastic reward.\nThe goal is to maximize cumulative reward over time.\n\nThe **epsilon-greedy** strategy balances exploration and exploitation:\n\n- With probability $\\epsilon$, pick a random arm (explore)\n- With probability $1-\\epsilon$, pick the arm with the highest estimated value (exploit)\n\nThis is the simplest baseline for the explore-exploit tradeoff.\n$\\epsilon = 0$ is pure greedy; $\\epsilon = 1$ is pure random.\n\n## Math\n\n$$Action-value estimate updated incrementally after pulling arm _a_ and$$\n$$observing reward _r_:$$\n\n```\nN[a] += 1\nQ[a] += (1 / N[a]) * (r - Q[a])\n```\n\n- $\\text{\\texttt{Q[a]} — estimated value of arm \\\\_a\\\\_ (running mean of rewards)}$\n- $\\text{\\texttt{N[a]} — number of times arm \\\\_a\\\\_ has been pulled}$\n- $\\text{The update is equivalent to computing the sample mean incrementally}$\n\n$$\\text{Arm selection:}$$\n\n```\na_t = random(0..K-1)         with probability epsilon\n    = argmax_a Q[a]          with probability 1 - epsilon\n```\n\n## Function\n\n```python\nclass EpsilonGreedyBandit:\n    def __init__(self, k: int, epsilon: float = 0.1, seed: int | None = None)\n    def select_arm(self) -> int\n    def update(self, arm: int, reward: float) -> None\n```\n\n- `k` — number of arms\n- `epsilon` — exploration probability (0 to 1)\n- `select_arm()` — returns the index of the chosen arm\n- `update(arm, reward)` — updates the value estimate for the given arm\n- `q_values` — list of current estimated values per arm\n- `counts` — list of pull counts per arm\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/bandit-epsilon-greedy/python -q\ncargo test --manifest-path modules/ml/reinforcement-learning/bandit-epsilon-greedy/rust/Cargo.toml\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/bandit-epsilon-greedy/python/bandit.py",
          "language": "python",
          "content": "\"\"\"Epsilon-greedy multi-armed bandit.\"\"\"\n\nimport random\nfrom typing import List\n\n\nclass EpsilonGreedyBandit:\n    \"\"\"Epsilon-greedy agent for the multi-armed bandit problem.\"\"\"\n\n    def __init__(self, k: int, epsilon: float = 0.1, seed: int | None = None):\n        \"\"\"\n        Args:\n            k: number of arms\n            epsilon: exploration probability (0 to 1)\n            seed: optional random seed for reproducibility\n        \"\"\"\n        self.k = k\n        self.epsilon = epsilon\n        self.q_values: List[float] = [0.0] * k  # estimated action values\n        self.counts: List[int] = [0] * k  # pull counts per arm\n        self._rng = random.Random(seed)\n\n    def select_arm(self) -> int:\n        \"\"\"Choose an arm using epsilon-greedy strategy.\"\"\"\n        if self._rng.random() < self.epsilon:\n            return self._rng.randint(0, self.k - 1)\n        # Greedy: pick arm with highest Q (break ties randomly)\n        max_q = max(self.q_values)\n        best_arms = [a for a in range(self.k) if self.q_values[a] == max_q]\n        return self._rng.choice(best_arms)\n\n    def update(self, arm: int, reward: float) -> None:\n        \"\"\"Update the value estimate for the chosen arm.\"\"\"\n        self.counts[arm] += 1\n        # Incremental mean update\n        self.q_values[arm] += (reward - self.q_values[arm]) / self.counts[arm]\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/bandit-epsilon-greedy/rust/src/lib.rs",
          "language": "rust",
          "content": "struct Lcg {\n    state: u64,\n}\n\nimpl Lcg {\n    fn new(seed: u64) -> Self {\n        Self { state: seed }\n    }\n\n    fn next_u32(&mut self) -> u32 {\n        self.state = self\n.state\n.wrapping_mul(6364136223846793005)\n.wrapping_add(1);\n        (self.state >> 32) as u32\n    }\n\n    fn next_f64(&mut self) -> f64 {\n        let v = self.next_u32() as f64;\n        v / u32::MAX as f64\n    }\n\n    fn gen_range(&mut self, upper: usize) -> usize {\n        (self.next_u32() as usize) % upper\n    }\n}\n\npub struct EpsilonGreedyBandit {\n    k: usize,\n    epsilon: f64,\n    q_values: Vec<f64>,\n    counts: Vec<usize>,\n    rng: Lcg,\n}\n\nimpl EpsilonGreedyBandit {\n    pub fn new(k: usize, epsilon: f64, seed: u64) -> Self {\n        Self {\n            k,\n            epsilon,\n            q_values: vec![0.0; k],\n            counts: vec![0; k],\n            rng: Lcg::new(seed),\n        }\n    }\n\n    pub fn select_arm(&mut self) -> usize {\n        if self.rng.next_f64() < self.epsilon {\n            return self.rng.gen_range(self.k);\n        }\n        let max_q = self\n.q_values\n.iter()\n.cloned()\n.fold(f64::NEG_INFINITY, f64::max);\n        let mut best: Vec<usize> = self\n.q_values\n.iter()\n.enumerate()\n.filter(|(_, q)| **q == max_q)\n.map(|(i, _)| i)\n.collect();\n        if best.len() == 1 {\n            return best[0];\n        }\n        let idx = self.rng.gen_range(best.len());\n        best.swap_remove(idx)\n    }\n\n    pub fn update(&mut self, arm: usize, reward: f64) {\n        self.counts[arm] += 1;\n        let n = self.counts[arm] as f64;\n        self.q_values[arm] += (reward - self.q_values[arm]) / n;\n    }\n\n    pub fn q_values(&self) -> &[f64] {\n        &self.q_values\n    }\n\n    pub fn counts(&self) -> &[usize] {\n        &self.counts\n    }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "bandits",
      "title": "Bandits",
      "path": "modules/ml/reinforcement-learning/bandits",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# Bandits\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nBandits estimate action values from rewards.\n\n## Math\n\n$$Q <- Q + (1/N)(r - Q)$$\n\n## Function\n\n```python\ndef update_value(q: float, n: int, reward: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/bandits/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/bandits/python/bandits.py",
          "language": "python",
          "content": "def update_value(q: float, n: int, reward: float) -> float:\n    return q + (reward - q) / n\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/bandits/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn update_value(q: f64, n: i32, reward: f64) -> f64 {\n    q + (reward - q) / n as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "dpo-vs-ppo",
      "title": "DPO vs PPO",
      "path": "modules/ml/reinforcement-learning/dpo-vs-ppo",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# DPO vs PPO\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nCompare preference optimization to policy optimization.\n\n## Math\n\n$$\\text{DPO uses preference logits; PPO uses clipped policy ratios.}$$\n\n## Function\n\n```python\ndef compare_methods() -> list[str]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/dpo-vs-ppo/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/dpo-vs-ppo/python/dpo_vs_ppo.py",
          "language": "python",
          "content": "def compare_methods() -> list[str]:\n    return [\"dpo\", \"ppo\"]\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/dpo-vs-ppo/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn compare_methods() -> Vec<&'static str> {\n    vec![\"dpo\", \"ppo\"]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "exploration-exploitation",
      "title": "Exploration vs Exploitation",
      "path": "modules/ml/reinforcement-learning/exploration-exploitation",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# Exploration vs Exploitation\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nEpsilon controls how often to explore.\n\n## Math\n\n$$P(\\text{explore})=\\epsilon$$\n\n## Function\n\n```python\ndef explore_probability(epsilon: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/exploration-exploitation/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/exploration-exploitation/python/exploration_exploitation.py",
          "language": "python",
          "content": "def explore_probability(epsilon: float) -> float:\n    return max(0.0, min(1.0, epsilon))\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/exploration-exploitation/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn explore_probability(epsilon: f64) -> f64 {\n    if epsilon < 0.0 { 0.0 } else if epsilon > 1.0 { 1.0 } else { epsilon }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "group-based-optimization",
      "title": "Group-Based Optimization (GSPO / GRPO)",
      "path": "modules/ml/reinforcement-learning/group-based-optimization",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# Group-Based Optimization (GSPO / GRPO)\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nGroup-based policy optimization builds advantages by normalizing rewards within\na group of sampled responses. GRPO uses token-level importance ratios, while\nGSPO switches to a sequence-level ratio to reduce token-level instability.\n\n## Math\n\n- $A_i = \\frac{r_i - \\mu_r}{\\sigma_r + \\epsilon}$\n- $\\text{GRPO: } \\rho_{i,t} = \\exp(\\log \\pi_\\theta - \\log \\pi_{\\text{old}}),\\ \\text{ratio}_i = \\frac{1}{T}\\sum_t \\rho_{i,t}$\n- $\\text{GSPO: } \\text{ratio}_i = \\exp\\left(\\sum_t (\\log \\pi_\\theta - \\log \\pi_{\\text{old}})\\right)$\n- $\\text{Objective: } J = \\frac{1}{N}\\sum_i \\text{ratio}_i A_i$\n\n## Function\n\n```python\ndef group_advantages(rewards: list[float], eps: float = 1e-8) -> list[float]:\n\ndef grpo_objective(\n    old_logps: list[list[float]],\n    new_logps: list[list[float]],\n    rewards: list[float],\n) -> float:\n\ndef gspo_objective(\n    old_logps: list[list[float]],\n    new_logps: list[list[float]],\n    rewards: list[float],\n) -> float:\n```\n\n## Demo code\n\n```python\nold_logps = [[-0.1, -0.2], [-0.3, -0.1]]\nnew_logps = [[-0.15, -0.25], [-0.25, -0.05]]\nrewards = [0.2, 0.8]\nprint(grpo_objective(old_logps, new_logps, rewards))\nprint(gspo_objective(old_logps, new_logps, rewards))\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/group-based-optimization/python -q\ncargo test --manifest-path modules/ml/reinforcement-learning/group-based-optimization/rust/Cargo.toml\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/group-based-optimization/python/group_based_optimization.py",
          "language": "python",
          "content": "\"\"\"Group-based optimization demo for GRPO/GSPO.\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom typing import List\n\n\ndef group_advantages(rewards: List[float], eps: float = 1e-8) -> List[float]:\n    if not rewards:\n        return []\n    mean = sum(rewards) / len(rewards)\n    var = sum((r - mean) ** 2 for r in rewards) / len(rewards)\n    std = math.sqrt(var)\n    return [(r - mean) / (std + eps) for r in rewards]\n\n\ndef _sequence_ratio(old_logps: List[float], new_logps: List[float]) -> float:\n    return math.exp(sum(n - o for o, n in zip(old_logps, new_logps)))\n\n\ndef _token_ratio_mean(old_logps: List[float], new_logps: List[float]) -> float:\n    ratios = [math.exp(n - o) for o, n in zip(old_logps, new_logps)]\n    return sum(ratios) / len(ratios)\n\n\ndef grpo_objective(\n    old_logps: List[List[float]],\n    new_logps: List[List[float]],\n    rewards: List[float],\n) -> float:\n    advantages = group_advantages(rewards)\n    if not advantages:\n        return 0.0\n    total = 0.0\n    for old, new, adv in zip(old_logps, new_logps, advantages):\n        total += _token_ratio_mean(old, new) * adv\n    return total / len(advantages)\n\n\ndef gspo_objective(\n    old_logps: List[List[float]],\n    new_logps: List[List[float]],\n    rewards: List[float],\n) -> float:\n    advantages = group_advantages(rewards)\n    if not advantages:\n        return 0.0\n    total = 0.0\n    for old, new, adv in zip(old_logps, new_logps, advantages):\n        total += _sequence_ratio(old, new) * adv\n    return total / len(advantages)\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/group-based-optimization/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn group_advantages(rewards: &[f64]) -> Vec<f64> {\n    if rewards.is_empty() {\n        return Vec::new();\n    }\n    let mean = rewards.iter().sum::<f64>() / rewards.len() as f64;\n    let var = rewards\n.iter()\n.map(|r| (r - mean).powi(2))\n.sum::<f64>()\n        / rewards.len() as f64;\n    let std = var.sqrt();\n    rewards\n.iter()\n.map(|r| (r - mean) / (std + 1e-8))\n.collect()\n}\n\nfn sequence_ratio(old_logps: &[f64], new_logps: &[f64]) -> f64 {\n    let sum_delta: f64 = old_logps\n.iter()\n.zip(new_logps.iter())\n.map(|(o, n)| n - o)\n.sum();\n    sum_delta.exp()\n}\n\nfn token_ratio_mean(old_logps: &[f64], new_logps: &[f64]) -> f64 {\n    let ratios: f64 = old_logps\n.iter()\n.zip(new_logps.iter())\n.map(|(o, n)| (n - o).exp())\n.sum();\n    ratios / old_logps.len() as f64\n}\n\npub fn grpo_objective(old_logps: &[Vec<f64>], new_logps: &[Vec<f64>], rewards: &[f64]) -> f64 {\n    let advantages = group_advantages(rewards);\n    if advantages.is_empty() {\n        return 0.0;\n    }\n    let mut total = 0.0;\n    for ((old, new), adv) in old_logps.iter().zip(new_logps.iter()).zip(advantages.iter()) {\n        total += token_ratio_mean(old, new) * adv;\n    }\n    total / advantages.len() as f64\n}\n\npub fn gspo_objective(old_logps: &[Vec<f64>], new_logps: &[Vec<f64>], rewards: &[f64]) -> f64 {\n    let advantages = group_advantages(rewards);\n    if advantages.is_empty() {\n        return 0.0;\n    }\n    let mut total = 0.0;\n    for ((old, new), adv) in old_logps.iter().zip(new_logps.iter()).zip(advantages.iter()) {\n        total += sequence_ratio(old, new) * adv;\n    }\n    total / advantages.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "mdp",
      "title": "Markov Decision Process",
      "path": "modules/ml/reinforcement-learning/mdp",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# Markov Decision Process\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nAn MDP defines states, actions, transitions, and rewards.\n\n## Math\n\n$$\\text{P(s'|s,a) defines transition probabilities.}$$\n\n## Function\n\n```python\ndef transition_prob(transitions: dict[tuple[int, int], dict[int, float]], s: int, a: int, s_next: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/mdp/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/mdp/python/mdp.py",
          "language": "python",
          "content": "def transition_prob(transitions: dict[tuple[int, int], dict[int, float]], s: int, a: int, s_next: int) -> float:\n    return transitions.get((s, a), {}).get(s_next, 0.0)\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/mdp/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashMap;\n\npub fn transition_prob(\n    transitions: &HashMap<(i32, i32), HashMap<i32, f64>>,\n    s: i32,\n    a: i32,\n    s_next: i32,\n) -> f64 {\n    transitions\n.get(&(s, a))\n.and_then(|m| m.get(&s_next).copied())\n.unwrap_or(0.0)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "ppo",
      "title": "PPO",
      "path": "modules/ml/reinforcement-learning/ppo",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# PPO\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nPPO clips policy updates to avoid large deviations.\n\n## Math\n\n$$L = \\min\\left(r_t A_t, \\operatorname{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A_t\\right)$$\n\n## Function\n\n```python\ndef clip_ratio(ratio: float, eps: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/ppo/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/ppo/python/ppo.py",
          "language": "python",
          "content": "def clip_ratio(ratio: float, eps: float) -> float:\n    return max(1 - eps, min(1 + eps, ratio))\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/ppo/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn clip_ratio(ratio: f64, eps: f64) -> f64 {\n    let lower = 1.0 - eps;\n    let upper = 1.0 + eps;\n    ratio.max(lower).min(upper)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "q-learning",
      "title": "Q-Learning",
      "path": "modules/ml/reinforcement-learning/q-learning",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# Q-Learning\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nQ-learning updates state-action values toward TD targets.\n\n## Math\n\n$$Q \\leftarrow Q + \\alpha\\left(r + \\gamma \\max_{a'} Q' - Q\\right)$$\n\n## Function\n\n```python\ndef q_update(q: float, reward: float, next_max: float, alpha: float, gamma: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/q-learning/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/q-learning/python/q_learning.py",
          "language": "python",
          "content": "def q_update(q: float, reward: float, next_max: float, alpha: float, gamma: float) -> float:\n    return q + alpha * (reward + gamma * next_max - q)\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/q-learning/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn q_update(q: f64, reward: f64, next_max: f64, alpha: f64, gamma: f64) -> f64 {\n    q + alpha * (reward + gamma * next_max - q)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "reinforce",
      "title": "REINFORCE",
      "path": "modules/ml/reinforcement-learning/reinforce",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# REINFORCE\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nREINFORCE updates policy parameters with reward-weighted gradients.\n\n## Math\n\n$$\\Delta \\theta \\propto R \\cdot \\nabla \\log \\pi(a|s)$$\n\n## Function\n\n```python\ndef reinforce_update(grad_logp: float, reward: float, lr: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/reinforce/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/reinforce/python/reinforce.py",
          "language": "python",
          "content": "def reinforce_update(grad_logp: float, reward: float, lr: float) -> float:\n    return lr * reward * grad_logp\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/reinforce/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn reinforce_update(grad_logp: f64, reward: f64, lr: f64) -> f64 {\n    lr * reward * grad_logp\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "return-discount",
      "title": "Reward, Return, Discount",
      "path": "modules/ml/reinforcement-learning/return-discount",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# Reward, Return, Discount\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nReturn is the discounted sum of rewards.\n\n## Math\n\n$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}$$\n\n## Function\n\n```python\ndef discounted_return(rewards: list[float], gamma: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/return-discount/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/return-discount/python/return_discount.py",
          "language": "python",
          "content": "def discounted_return(rewards: list[float], gamma: float) -> float:\n    total = 0.0\n    for i, r in enumerate(rewards):\n        total += (gamma ** i) * r\n    return total\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/return-discount/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn discounted_return(rewards: &[f64], gamma: f64) -> f64 {\n    let mut total = 0.0;\n    for (i, r) in rewards.iter().enumerate() {\n        total += gamma.powi(i as i32) * r;\n    }\n    total\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "sarsa",
      "title": "SARSA",
      "path": "modules/ml/reinforcement-learning/sarsa",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# SARSA\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nSARSA uses the next action actually taken for updates.\n\n## Math\n\n$$Q \\leftarrow Q + \\alpha\\left(r + \\gamma Q' - Q\\right)$$\n\n## Function\n\n```python\ndef sarsa_update(q: float, reward: float, next_q: float, alpha: float, gamma: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/sarsa/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/sarsa/python/sarsa.py",
          "language": "python",
          "content": "def sarsa_update(q: float, reward: float, next_q: float, alpha: float, gamma: float) -> float:\n    return q + alpha * (reward + gamma * next_q - q)\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/sarsa/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn sarsa_update(q: f64, reward: f64, next_q: f64, alpha: f64, gamma: f64) -> f64 {\n    q + alpha * (reward + gamma * next_q - q)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "ucb",
      "title": "UCB",
      "path": "modules/ml/reinforcement-learning/ucb",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# UCB\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nUpper Confidence Bound balances mean reward and uncertainty.\n\n## Math\n\n$$\\mathrm{UCB} = Q + c \\sqrt{\\frac{\\ln t}{N}}$$\n\n## Function\n\n```python\ndef ucb_score(q: float, t: int, n: int, c: float = 1.0) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/ucb/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/ucb/python/ucb.py",
          "language": "python",
          "content": "import math\n\n\ndef ucb_score(q: float, t: int, n: int, c: float = 1.0) -> float:\n    return q + c * math.sqrt(math.log(t) / n)\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/ucb/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn ucb_score(q: f64, t: f64, n: f64, c: f64) -> f64 {\n    q + c * (t.ln() / n).sqrt()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "backward-pass",
      "title": "Backward Pass",
      "path": "modules/ml/systems/backward-pass",
      "summary": "> Track: `ml` | Topic: `systems`",
      "readme": "# Backward Pass\n\n> Track: `ml` | Topic: `systems`\n\n## Concept\n\nBackward pass computes gradients of loss w.r.t parameters.\n\n## Math\n\n$$dL/dw = dL/dy * x$$\n\n## Function\n\n```python\ndef backward(dy: float, x: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/systems/backward-pass/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/systems/backward-pass/python/backward_pass.py",
          "language": "python",
          "content": "def backward(dy: float, x: float) -> float:\n    return dy * x\n"
        },
        {
          "path": "modules/ml/systems/backward-pass/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn backward(dy: f64, x: f64) -> f64 {\n    dy * x\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "check-gradients",
      "title": "Check Gradients Are Flowing",
      "path": "modules/ml/systems/check-gradients",
      "summary": "> Track: `ml` | Topic: `systems`",
      "readme": "# Check Gradients Are Flowing\n\n> Track: `ml` | Topic: `systems`\n\n## Concept\n\nCheck that gradients are non-zero and finite.\n\n## Math\n\n$$ok if any(|g| > 0) and no NaN$$\n\n## Function\n\n```python\ndef gradients_ok(grads: list[float]) -> bool:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/systems/check-gradients/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/systems/check-gradients/python/check_gradients.py",
          "language": "python",
          "content": "import math\n\n\ndef gradients_ok(grads: list[float]) -> bool:\n    if any(math.isnan(g) for g in grads):\n        return False\n    return any(abs(g) > 0 for g in grads)\n"
        },
        {
          "path": "modules/ml/systems/check-gradients/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn gradients_ok(grads: &[f64]) -> bool {\n    if grads.iter().any(|g| g.is_nan()) { return false; }\n    grads.iter().any(|g| g.abs() > 0.0)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "debug-overfit-underfit",
      "title": "Debug Overfitting vs Underfitting",
      "path": "modules/ml/systems/debug-overfit-underfit",
      "summary": "> Track: `ml` | Topic: `systems`",
      "readme": "# Debug Overfitting vs Underfitting\n\n> Track: `ml` | Topic: `systems`\n\n## Concept\n\nCompare train vs validation loss to detect fit issues.\n\n## Math\n\n$$If train << val => overfit; if both high => underfit.$$\n\n## Function\n\n```python\ndef diagnose(train_loss: float, val_loss: float) -> str:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/systems/debug-overfit-underfit/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/systems/debug-overfit-underfit/python/debug_overfit_underfit.py",
          "language": "python",
          "content": "def diagnose(train_loss: float, val_loss: float) -> str:\n    if train_loss < val_loss * 0.7:\n        return \"overfit\"\n    if train_loss > 1.0 and val_loss > 1.0:\n        return \"underfit\"\n    return \"ok\"\n"
        },
        {
          "path": "modules/ml/systems/debug-overfit-underfit/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn diagnose(train_loss: f64, val_loss: f64) -> String {\n    if train_loss < val_loss * 0.7 {\n        \"overfit\".to_string()\n    } else if train_loss > 1.0 && val_loss > 1.0 {\n        \"underfit\".to_string()\n    } else {\n        \"ok\".to_string()\n    }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "forward-pass",
      "title": "Forward Pass",
      "path": "modules/ml/systems/forward-pass",
      "summary": "> Track: `ml` | Topic: `systems`",
      "readme": "# Forward Pass\n\n> Track: `ml` | Topic: `systems`\n\n## Concept\n\nForward pass computes model outputs from inputs.\n\n## Math\n\n$$y = W x + b$$\n\n## Function\n\n```python\ndef forward(x: list[float], w: list[float], b: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/systems/forward-pass/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/systems/forward-pass/python/forward_pass.py",
          "language": "python",
          "content": "def forward(x: list[float], w: list[float], b: float) -> float:\n    return sum(wi * xi for wi, xi in zip(w, x)) + b\n"
        },
        {
          "path": "modules/ml/systems/forward-pass/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn forward(x: &[f64], w: &[f64], b: f64) -> f64 {\n    let mut out = b;\n    for (wi, xi) in w.iter().zip(x.iter()) {\n        out += wi * xi;\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "gradient-accumulation",
      "title": "Gradient Accumulation",
      "path": "modules/ml/systems/gradient-accumulation",
      "summary": "> Track: `ml` | Topic: `systems`",
      "readme": "# Gradient Accumulation\n\n> Track: `ml` | Topic: `systems`\n\n## Concept\n\nAccumulate gradients across steps to simulate larger batches.\n\n## Math\n\n$$g_{\\text{total}} = \\sum_i g_i$$\n\n## Function\n\n```python\ndef accumulate(grads: list[list[float]]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/systems/gradient-accumulation/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/systems/gradient-accumulation/python/gradient_accumulation.py",
          "language": "python",
          "content": "def accumulate(grads: list[list[float]]) -> list[float]:\n    if not grads:\n        return []\n    total = [0.0 for _ in grads[0]]\n    for g in grads:\n        total = [a + b for a, b in zip(total, g)]\n    return total\n"
        },
        {
          "path": "modules/ml/systems/gradient-accumulation/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn accumulate(grads: &[Vec<f64>]) -> Vec<f64> {\n    if grads.is_empty() { return Vec::new(); }\n    let mut total = vec![0.0; grads[0].len()];\n    for g in grads {\n        for i in 0..g.len() {\n            total[i] += g[i];\n        }\n    }\n    total\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "mixed-precision",
      "title": "Mixed Precision Training",
      "path": "modules/ml/systems/mixed-precision",
      "summary": "> Track: `ml` | Topic: `systems`",
      "readme": "# Mixed Precision Training\n\n> Track: `ml` | Topic: `systems`\n\n## Concept\n\nMixed precision scales gradients to avoid underflow.\n\n## Math\n\n$$g_scaled = g * scale$$\n\n## Function\n\n```python\ndef scale_gradients(grads: list[float], scale: float) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/systems/mixed-precision/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/systems/mixed-precision/python/mixed_precision.py",
          "language": "python",
          "content": "def scale_gradients(grads: list[float], scale: float) -> list[float]:\n    return [g * scale for g in grads]\n"
        },
        {
          "path": "modules/ml/systems/mixed-precision/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn scale_gradients(grads: &[f64], scale: f64) -> Vec<f64> {\n    grads.iter().map(|g| g * scale).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "optimizer-step",
      "title": "Optimizer Step",
      "path": "modules/ml/systems/optimizer-step",
      "summary": "> Track: `ml` | Topic: `systems`",
      "readme": "# Optimizer Step\n\n> Track: `ml` | Topic: `systems`\n\n## Concept\n\nApply gradients to update parameters.\n\n## Math\n\n$$w = w - lr * g$$\n\n## Function\n\n```python\ndef step(w: float, grad: float, lr: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/systems/optimizer-step/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/systems/optimizer-step/python/optimizer_step.py",
          "language": "python",
          "content": "def step(w: float, grad: float, lr: float) -> float:\n    return w - lr * grad\n"
        },
        {
          "path": "modules/ml/systems/optimizer-step/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn step(w: f64, grad: f64, lr: f64) -> f64 {\n    w - lr * grad\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "zero-gradients",
      "title": "Zeroing Gradients",
      "path": "modules/ml/systems/zero-gradients",
      "summary": "> Track: `ml` | Topic: `systems`",
      "readme": "# Zeroing Gradients\n\n> Track: `ml` | Topic: `systems`\n\n## Concept\n\nReset gradients to zero before the next backward pass.\n\n## Math\n\n$$g = 0$$\n\n## Function\n\n```python\ndef zero_grad(grads: list[float]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/systems/zero-gradients/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/systems/zero-gradients/python/zero_gradients.py",
          "language": "python",
          "content": "def zero_grad(grads: list[float]) -> list[float]:\n    return [0.0 for _ in grads]\n"
        },
        {
          "path": "modules/ml/systems/zero-gradients/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn zero_grad(grads: &[f64]) -> Vec<f64> {\n    vec![0.0; grads.len()]\n}\n"
        }
      ]
    }
  ],
  "problems": [
    {
      "id": "dsa-arrays-two-sum",
      "slug": "two-sum",
      "title": "Two Sum",
      "track": "dsa",
      "topic": "arrays",
      "difficulty": "easy",
      "tags": [
        "hashmap"
      ],
      "languages": [
        "python",
        "rust"
      ],
      "path": "problems/dsa/arrays/two-sum",
      "summary": "Given an array of integers `nums` and an integer `target`, return the indices of the two numbers that add up to `target`.",
      "statement": "# Two Sum\n\n## Problem\n\nGiven an array of integers `nums` and an integer `target`, return the indices\nof the two numbers that add up to `target`.\n\nYou may assume that each input has **exactly one solution**, and you may not\nuse the same element twice. You can return the answer in any order.\n\n## Examples\n\n```\nInput: nums = [2, 7, 11, 15], target = 9\nOutput: [0, 1]\nExplanation: nums[0] + nums[1] == 9\n```\n\n```\nInput: nums = [3, 2, 4], target = 6\nOutput: [1, 2]\n```\n\n```\nInput: nums = [3, 3], target = 6\nOutput: [0, 1]\n```\n\n## Constraints\n\n- 2 <= len(nums) <= 10^4\n- -10^9 <= nums[i] <= 10^9\n- -10^9 <= target <= 10^9\n- Exactly one valid answer exists\n"
    }
  ],
  "docs": [
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "computer-vision",
      "title": "Computer Vision",
      "path": "docs/ml/computer-vision/README.md",
      "summary": "Vision preprocessing and CNN architecture basics. Each bullet maps to a module under `modules/ml/computer-vision/`.",
      "content": "# Computer Vision\n\nVision preprocessing and CNN architecture basics.\nEach bullet maps to a module under `modules/ml/computer-vision/`.\n\n## CNN Fundamentals\n\n- CNNs (`modules/ml/computer-vision/cnn-basics`)\n- Convolution layers (`modules/ml/computer-vision/convolution-layer`)\n- Pooling (max, average) (`modules/ml/computer-vision/pooling`)\n- 2D vs 3D CNN (`modules/ml/computer-vision/cnn-2d-vs-3d`)\n- Image preprocessing (`modules/ml/computer-vision/image-preprocessing`)\n- RGB to grayscale (`modules/ml/computer-vision/rgb-to-grayscale`)\n- Contrast and brightness (`modules/ml/computer-vision/contrast-brightness`)\n- Bilinear resizing (`modules/ml/computer-vision/bilinear-resizing`)\n- Sobel edge detection (`modules/ml/computer-vision/sobel-edge-detection`)\n- Optical flow (EPE) (`modules/ml/computer-vision/optical-flow-epe`)\n- Data augmentation (`modules/ml/computer-vision/data-augmentation`)\n- Non-maximum suppression (`modules/ml/computer-vision/non-maximum-suppression`)\n\n## Classic Architectures\n\n- LeNet-5 (`modules/ml/computer-vision/lenet-5`)\n- AlexNet (`modules/ml/computer-vision/alexnet`)\n- VGGNet (`modules/ml/computer-vision/vggnet`)\n- ResNet (`modules/ml/computer-vision/resnet`)\n"
    },
    {
      "track": "ml",
      "topic": "data",
      "slug": "data",
      "title": "Data and Splitting",
      "path": "docs/ml/data/README.md",
      "summary": "Reliable data setup and evaluation splits. Each bullet maps to a module under `modules/ml/data/`.",
      "content": "# Data and Splitting\n\nReliable data setup and evaluation splits.\nEach bullet maps to a module under `modules/ml/data/`.\n\n## Concepts\n\n- Dataset vs batch vs epoch (`modules/ml/data/dataset-batch-epoch`)\n- Batch iterator (`modules/ml/data/batch-iterator`)\n- Train / validation / test split (`modules/ml/data/train-validation-test-split`)\n- Stratified split (`modules/ml/data/stratified-split`)\n- Data leakage (common failure modes) (`modules/ml/data/data-leakage`)\n- Polynomial feature expansion (`modules/ml/data/polynomial-features`)\n- Handling class imbalance (`modules/ml/data/class-imbalance`)\n"
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "deep-learning",
      "title": "Foundations of Deep Learning",
      "path": "docs/ml/deep-learning/README.md",
      "summary": "Neural networks and training mechanics from first principles. Each bullet maps to a module under `modules/ml/deep-learning/` unless noted.",
      "content": "# Foundations of Deep Learning\n\nNeural networks and training mechanics from first principles.\nEach bullet maps to a module under `modules/ml/deep-learning/` unless noted.\n\n## Neural Networks and Backprop\n\n- Feedforward neural networks (`modules/ml/deep-learning/feedforward-neural-network`)\n- Neuron, weights, bias, activation (`modules/ml/deep-learning/neuron-weights-bias-activation`)\n- Backpropagation (chain rule, local gradients) (`modules/ml/deep-learning/backpropagation`)\n- Automatic differentiation (forward vs backward mode) (`modules/ml/deep-learning/automatic-differentiation`)\n- Vanishing and exploding gradients (`modules/ml/deep-learning/vanishing-exploding-gradients`)\n- Gradient checking (finite differences) (`modules/ml/deep-learning/gradient-checking`)\n\n## Activation Functions\n\n- Sigmoid, Tanh, Hard Sigmoid, Dynamic Tanh, Hardtanh (`modules/ml/deep-learning/activations-sigmoid-tanh`)\n- ReLU family: ReLU, Leaky ReLU, ELU, PReLU (`modules/ml/deep-learning/activations-relu-family`)\n- Modern: GeLU, Swish, SwiGLU, Mish (`modules/ml/deep-learning/activations-modern`)\n- Softmax, Softplus, Softsign (`modules/ml/deep-learning/activations-softmax-softplus-softsign`)\n- Failure modes (dead ReLU, saturation) (`modules/ml/deep-learning/activation-failure-modes`)\n\n## Initialization\n\n- Xavier / Glorot: symmetric activations (`modules/ml/deep-learning/xavier-initialization`)\n- He initialization: ReLU (`modules/ml/deep-learning/he-initialization`)\n- Effect on gradient flow (`modules/ml/deep-learning/gradient-flow`)\n\n## Regularization\n\n- L1 (Lasso) (`modules/ml/deep-learning/l1-regularization`)\n- L2 (Ridge) (`modules/ml/deep-learning/l2-regularization`)\n- Weight decay (`modules/ml/deep-learning/weight-decay`)\n- Dropout (`modules/ml/deep-learning/dropout`)\n- Early stopping (`modules/ml/deep-learning/early-stopping`)\n- LoRA (conceptual regularization effect; see `modules/ml/llm/lora`)\n\n## Loss Functions\n\n### Classification\n\n- Cross-entropy (`modules/ml/deep-learning/cross-entropy`)\n- Hinge loss (`modules/ml/deep-learning/hinge-loss`)\n- Focal loss (imbalanced data) (`modules/ml/deep-learning/focal-loss`)\n\n### Regression\n\n- MSE (`modules/ml/deep-learning/mse-loss`)\n- MAE (`modules/ml/deep-learning/mae-loss`)\n- RMSE (`modules/ml/deep-learning/rmse-loss`)\n- Huber loss (`modules/ml/deep-learning/huber-loss`)\n\n### Special\n\n- Knowledge distillation loss (`modules/ml/deep-learning/knowledge-distillation-loss`)\n- KL divergence (see `modules/ml/fundamentals/kl-divergence`)\n\n## Normalization\n\n- BatchNorm (train vs eval behavior) (`modules/ml/deep-learning/batchnorm`)\n- LayerNorm (`modules/ml/deep-learning/layernorm`)\n- RMSNorm (`modules/ml/deep-learning/rmsnorm`)\n- GroupNorm (`modules/ml/deep-learning/groupnorm`)\n- InstanceNorm (`modules/ml/deep-learning/instancenorm`)\n- Why BatchNorm is bad for Transformers (`modules/ml/deep-learning/batchnorm-transformers`)\n"
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "evaluation",
      "title": "Metrics and Evaluation",
      "path": "docs/ml/evaluation/README.md",
      "summary": "Metrics and evaluation patterns for common tasks. Each bullet maps to a module under `modules/ml/evaluation/` unless noted.",
      "content": "# Metrics and Evaluation\n\nMetrics and evaluation patterns for common tasks.\nEach bullet maps to a module under `modules/ml/evaluation/` unless noted.\n\n## Classification\n\n- Accuracy (when it fails) (`modules/ml/evaluation/accuracy`)\n- Precision / Recall (`modules/ml/evaluation/precision-recall`)\n- F1 (`modules/ml/evaluation/f1-score`)\n- ROC-AUC (`modules/ml/evaluation/roc-auc`)\n- Confusion matrix (`modules/ml/evaluation/confusion-matrix`)\n- Matthews correlation coefficient (`modules/ml/evaluation/matthews-correlation`)\n- Jaccard index (`modules/ml/evaluation/jaccard-index`)\n- Dice score (`modules/ml/evaluation/dice-score`)\n- Gini impurity (`modules/ml/evaluation/gini-impurity`)\n\n## Clustering\n\n- Silhouette score (`modules/ml/evaluation/silhouette-score`)\n- Davies-Bouldin index (`modules/ml/evaluation/davies-bouldin`)\n- Calinski-Harabasz index (`modules/ml/evaluation/calinski-harabasz`)\n\n## Regression\n\n- MAE vs MSE (`modules/ml/evaluation/mae-vs-mse`)\n- RMSE (see `modules/ml/deep-learning/rmse-loss`)\n- R2 (pitfalls) (`modules/ml/evaluation/r2-score`)\n"
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "fundamentals",
      "title": "Math for ML",
      "path": "docs/ml/fundamentals/README.md",
      "summary": "Core math for models and training. Each bullet maps to a module under `modules/ml/fundamentals/`.",
      "content": "# Math for ML\n\nCore math for models and training.\nEach bullet maps to a module under `modules/ml/fundamentals/`.\n\n## Linear Algebra\n\n- Vectors, matrices (`modules/ml/fundamentals/vectors-matrices`)\n- Jacobian (`modules/ml/fundamentals/jacobian`)\n- Hessian (`modules/ml/fundamentals/hessian`)\n- SVD (`modules/ml/fundamentals/svd`)\n- Cosine similarity (`modules/ml/fundamentals/cosine-similarity`)\n\n## Probability and Statistics\n\n- Distributions (`modules/ml/fundamentals/distributions`)\n- Empirical PMF (`modules/ml/fundamentals/empirical-pmf`)\n- Expectation (`modules/ml/fundamentals/expectation`)\n- Covariance (`modules/ml/fundamentals/covariance`)\n- Markov chains (`modules/ml/fundamentals/markov-chains`)\n- KL divergence (`modules/ml/fundamentals/kl-divergence`)\n- Jensen-Shannon divergence (`modules/ml/fundamentals/jensen-shannon-divergence`)\n- Mutual information (`modules/ml/fundamentals/mutual-information`)\n- Two-sample t-test (`modules/ml/fundamentals/two-sample-t-test`)\n- Bayesian inference (Beta-Binomial) (`modules/ml/fundamentals/beta-binomial`)\n\n## Optimization\n\n- Gradient descent (`modules/ml/fundamentals/gradient-descent`)\n- Newton's method (`modules/ml/fundamentals/newtons-method`)\n- Convex vs non-convex intuition (`modules/ml/fundamentals/convex-vs-nonconvex`)\n- ELBO (variational inference) (`modules/ml/fundamentals/elbo`)\n"
    },
    {
      "track": "ml",
      "topic": "generative",
      "slug": "generative",
      "title": "Generative Models",
      "path": "docs/ml/generative/README.md",
      "summary": "Core generative families and trade-offs. Each bullet maps to a module under `modules/ml/generative/`.",
      "content": "# Generative Models\n\nCore generative families and trade-offs.\nEach bullet maps to a module under `modules/ml/generative/`.\n\n## Core Families\n\n- GAN (generator vs discriminator, mode collapse) (`modules/ml/generative/gan`)\n- VAE (ELBO, reconstruction + KL) (`modules/ml/generative/vae`)\n- Diffusion models (forward noise / reverse denoise, training objective) (`modules/ml/generative/diffusion-models`)\n\n## Interview-Level Skills\n\n- Choose GAN vs VAE vs Diffusion for a task (trade-offs) (`modules/ml/generative/model-selection`)\n- GAN instability, mode collapse (`modules/ml/generative/gan-mode-collapse`)\n- VAE blurry samples, posterior collapse (`modules/ml/generative/vae-posterior-collapse`)\n- Diffusion slow sampling, guidance trade-offs (`modules/ml/generative/diffusion-guidance-tradeoffs`)\n"
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "llm",
      "title": "NLP and LLMs",
      "path": "docs/ml/llm/README.md",
      "summary": "Transformers, training stages, and alignment for LLMs. Each bullet maps to a module under `modules/ml/llm/`.",
      "content": "# NLP and LLMs\n\nTransformers, training stages, and alignment for LLMs.\nEach bullet maps to a module under `modules/ml/llm/`.\n\n## Core Concepts\n\n- Tokenization (`modules/ml/llm/tokenization`)\n- Embeddings (`modules/ml/llm/embeddings`)\n- Positional encoding (`modules/ml/llm/positional-encoding`)\n- Transformer (`modules/ml/llm/transformer`)\n- Self-attention (`modules/ml/llm/self-attention`)\n- Multi-head attention (`modules/ml/llm/multi-head-attention`)\n- Masked attention (`modules/ml/llm/attention-causal`)\n\n## Training Stages\n\n- Pretraining (next-token prediction / PTX loss) (`modules/ml/llm/pretraining`)\n- Supervised fine-tuning (SFT) (`modules/ml/llm/supervised-fine-tuning`)\n- Alignment / preference learning (`modules/ml/llm/preference-learning`)\n\n## Alignment and Optimization\n\n- RLHF (`modules/ml/llm/rlhf`)\n- DPO (`modules/ml/llm/dpo`)\n- KL regularization (`modules/ml/llm/kl-regularization`)\n- PTX anchoring (`modules/ml/llm/ptx-anchoring`)\n\n## Efficiency and Systems\n\n- LoRA (`modules/ml/llm/lora`) / QLoRA (`modules/ml/llm/qlora`)\n- Inference head pruning (`modules/ml/llm/inference-head-pruning`)\n- Sparse attention (`modules/ml/llm/sparse-attention`)\n- MoE (Top-K routing, Noisy gating) (`modules/ml/llm/moe-routing`)\n- FP16 / BF16 / FP8 (`modules/ml/llm/fp16-bf16-fp8`)\n- INT8 / INT4 (`modules/ml/llm/int8-int4-quantization`)\n"
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "mlops",
      "title": "MLOps and Production ML",
      "path": "docs/ml/mlops/README.md",
      "summary": "Operational concerns for production ML. Each bullet maps to a module under `modules/ml/mlops/`.",
      "content": "# MLOps and Production ML\n\nOperational concerns for production ML.\nEach bullet maps to a module under `modules/ml/mlops/`.\n\n## Production Concepts\n\n- ETL pipeline (`modules/ml/mlops/etl-pipeline`)\n- Offline vs online inference (`modules/ml/mlops/offline-online-inference`)\n- Batch vs real-time inference (`modules/ml/mlops/batch-vs-realtime`)\n- Data quality checks (`modules/ml/mlops/data-quality-checks`)\n- Feature drift detection (PSI) (`modules/ml/mlops/feature-drift-psi`)\n- Prediction distribution monitoring (`modules/ml/mlops/prediction-monitoring`)\n- Canary deployment (`modules/ml/mlops/canary-deployment`)\n- A/B testing (`modules/ml/mlops/ab-testing`)\n- SLA metrics (`modules/ml/mlops/sla-metrics`)\n- Request batching (`modules/ml/mlops/request-batching`)\n"
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "models",
      "title": "Models and Classical ML",
      "path": "docs/ml/models/README.md",
      "summary": "Classical ML models and baselines. Each bullet maps to a module under `modules/ml/models/`.",
      "content": "# Models and Classical ML\n\nClassical ML models and baselines.\nEach bullet maps to a module under `modules/ml/models/`.\n\n## Core Models\n\n- Linear regression (`modules/ml/models/linear-regression`)\n- Logistic regression (`modules/ml/models/logistic-regression`)\n- Softmax regression (`modules/ml/models/softmax-regression`)\n- Elastic Net (`modules/ml/models/elastic-net`)\n- Decision Trees (`modules/ml/models/decision-trees`)\n- Random Forest (bagging) (`modules/ml/models/random-forest`)\n- AdaBoost (`modules/ml/models/adaboost`)\n- KNN (`modules/ml/models/knn`)\n- K-Means (`modules/ml/models/k-means`)\n- DBSCAN (`modules/ml/models/dbscan`)\n- PCA (`modules/ml/models/pca`)\n- Gaussian Naive Bayes (`modules/ml/models/gaussian-naive-bayes`)\n- Bernoulli Naive Bayes (`modules/ml/models/bernoulli-naive-bayes`)\n- SVM (Pegasos) (`modules/ml/models/svm-pegasos`)\n- Gaussian Process Regression (GPR) (`modules/ml/models/gaussian-process-regression`)\n"
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "optimization",
      "title": "Optimization and Training Dynamics",
      "path": "docs/ml/optimization/README.md",
      "summary": "How gradients become stable updates. Each bullet maps to a module under `modules/ml/optimization/`.",
      "content": "# Optimization and Training Dynamics\n\nHow gradients become stable updates.\nEach bullet maps to a module under `modules/ml/optimization/`.\n\n## Optimizers\n\n- SGD (`modules/ml/optimization/sgd`)\n- SGD with Momentum (`modules/ml/optimization/sgd-momentum`)\n- Nesterov Accelerated Gradient (`modules/ml/optimization/nesterov`)\n- Adam (`modules/ml/optimization/adam`)\n- AdamW (decoupled weight decay) (`modules/ml/optimization/adamw`)\n- RMSProp (`modules/ml/optimization/rmsprop`)\n- Adagrad (`modules/ml/optimization/adagrad`)\n- Muon (high-level intuition) (`modules/ml/optimization/muon-optimizer`)\n\n## Learning Rate Strategies\n\n- Constant LR (`modules/ml/optimization/lr-constant`)\n- Step decay (`modules/ml/optimization/lr-step-decay`)\n- Exponential decay (`modules/ml/optimization/lr-exponential-decay`)\n- Warmup (`modules/ml/optimization/lr-warmup`)\n- Cosine decay (`modules/ml/optimization/lr-cosine-decay`)\n\n## Training Stability\n\n- Gradient clipping (global norm) (`modules/ml/optimization/gradient-clipping`)\n- Loss scaling (mixed precision) (`modules/ml/optimization/loss-scaling`)\n- Detecting NaNs / divergence (`modules/ml/optimization/detect-nans`)\n"
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "reinforcement-learning",
      "title": "Reinforcement Learning",
      "path": "docs/ml/reinforcement-learning/README.md",
      "summary": "Agents learning from rewards and exploration. Each bullet maps to a module under `modules/ml/reinforcement-learning/`.",
      "content": "# Reinforcement Learning\n\nAgents learning from rewards and exploration.\nEach bullet maps to a module under `modules/ml/reinforcement-learning/`.\n\n## Core Ideas\n\n- MDPs (`modules/ml/reinforcement-learning/mdp`)\n- Reward, return, discount factor (`modules/ml/reinforcement-learning/return-discount`)\n- Exploration vs exploitation (`modules/ml/reinforcement-learning/exploration-exploitation`)\n\n## Algorithms\n\n- Bandits (`modules/ml/reinforcement-learning/bandits`)\n- epsilon-greedy (`modules/ml/reinforcement-learning/bandit-epsilon-greedy`)\n- UCB (`modules/ml/reinforcement-learning/ucb`)\n- REINFORCE (`modules/ml/reinforcement-learning/reinforce`)\n- PPO (why clipping helps) (`modules/ml/reinforcement-learning/ppo`)\n- Q-Learning (`modules/ml/reinforcement-learning/q-learning`)\n- SARSA (`modules/ml/reinforcement-learning/sarsa`)\n\n## LLM-related RL\n\n- DPO vs PPO (`modules/ml/reinforcement-learning/dpo-vs-ppo`)\n- Group-based optimization (GSPO / GRPO) (`modules/ml/reinforcement-learning/group-based-optimization`)\n"
    },
    {
      "track": "ml",
      "topic": "representation",
      "slug": "representation",
      "title": "Representation Learning",
      "path": "docs/ml/representation/README.md",
      "summary": "Cross-cutting representation items from.",
      "content": "# Representation Learning\n\nCross-cutting representation items from.\n\n## Core Concepts\n\n- Embeddings\n- Positional encoding\n- Cosine similarity\n- PCA\n- SVD\n- Polynomial feature expansion\n"
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "systems",
      "title": "Training Loop Mechanics",
      "path": "docs/ml/systems/README.md",
      "summary": "Training loop mechanics and debugging signals. Each bullet maps to a module under `modules/ml/systems/`.",
      "content": "# Training Loop Mechanics\n\nTraining loop mechanics and debugging signals.\nEach bullet maps to a module under `modules/ml/systems/`.\n\n## Core Steps\n\n- Zeroing gradients (`modules/ml/systems/zero-gradients`)\n- Forward pass (`modules/ml/systems/forward-pass`)\n- Backward pass (`modules/ml/systems/backward-pass`)\n- Optimizer step (`modules/ml/systems/optimizer-step`)\n- Gradient accumulation (`modules/ml/systems/gradient-accumulation`)\n- Mixed precision training (`modules/ml/systems/mixed-precision`)\n- Check gradients are flowing (`modules/ml/systems/check-gradients`)\n- Debug overfitting vs underfitting (`modules/ml/systems/debug-overfit-underfit`)\n"
    }
  ]
}
