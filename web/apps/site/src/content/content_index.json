{
  "generated_at": "2026-02-02T01:30:22.758Z",
  "tracks": [
    {
      "id": "dsa",
      "name": "Data Structures & Algorithms",
      "description": "Core patterns, data structures, and algorithmic analysis.",
      "accent": "#1f77b4",
      "accentVar": "--track-dsa",
      "topicCount": 11,
      "moduleCount": 172
    },
    {
      "id": "ml",
      "name": "Machine Learning",
      "description": "Math foundations, models, optimization, and systems.",
      "accent": "#ff7f0e",
      "accentVar": "--track-ml",
      "topicCount": 13,
      "moduleCount": 178
    },
    {
      "id": "ai-agents",
      "name": "AI Agents",
      "description": "Prompting, tool use, memory, and evaluation.",
      "accent": "#2ca02c",
      "accentVar": "--track-ai-agents",
      "topicCount": 9,
      "moduleCount": 0
    },
    {
      "id": "databases",
      "name": "Databases",
      "description": "Schema design, indexing, transactions, and query plans.",
      "accent": "#17becf",
      "accentVar": "--track-databases",
      "topicCount": 10,
      "moduleCount": 0
    },
    {
      "id": "software-engineering",
      "name": "Software Engineering",
      "description": "APIs, performance, testing, and system design.",
      "accent": "#d62728",
      "accentVar": "--track-se",
      "topicCount": 10,
      "moduleCount": 0
    }
  ],
  "topics": [
    {
      "track": "software-engineering",
      "topic": "apis",
      "name": "APIs",
      "path": "/track/software-engineering/apis",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "dsa",
      "topic": "array",
      "name": "Array",
      "path": "/track/dsa/array",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 7
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "name": "Backtracking",
      "path": "/track/dsa/backtracking",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 19
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "name": "Binary Tree",
      "path": "/track/dsa/binary-tree",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 30
    },
    {
      "track": "databases",
      "topic": "caching",
      "name": "Caching",
      "path": "/track/databases/caching",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "name": "Computer Vision",
      "path": "/track/ml/computer-vision",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 16
    },
    {
      "track": "software-engineering",
      "topic": "concurrency",
      "name": "Concurrency",
      "path": "/track/software-engineering/concurrency",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "ml",
      "topic": "data",
      "name": "Data",
      "path": "/track/ml/data",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 7
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "name": "Deep Learning",
      "path": "/track/ml/deep-learning",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 33
    },
    {
      "track": "software-engineering",
      "topic": "design-patterns",
      "name": "Design Patterns",
      "path": "/track/software-engineering/design-patterns",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "dsa",
      "topic": "double-pointers",
      "name": "Double Pointers",
      "path": "/track/dsa/double-pointers",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 10
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "name": "Dynamic Programming",
      "path": "/track/dsa/dynamic-programming",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 47
    },
    {
      "track": "ai-agents",
      "topic": "evals",
      "name": "Evals",
      "path": "/track/ai-agents/evals",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "name": "Evaluation",
      "path": "/track/ml/evaluation",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 14
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "name": "Fundamentals",
      "path": "/track/ml/fundamentals",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 19
    },
    {
      "track": "ml",
      "topic": "generative",
      "name": "Generative",
      "path": "/track/ml/generative",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 7
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "name": "Greedy Algorithm",
      "path": "/track/dsa/greedy-algorithm",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 20
    },
    {
      "track": "ai-agents",
      "topic": "guardrails",
      "name": "Guardrails",
      "path": "/track/ai-agents/guardrails",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "dsa",
      "topic": "hash-tables",
      "name": "Hash Tables",
      "path": "/track/dsa/hash-tables",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 10
    },
    {
      "track": "databases",
      "topic": "indexing",
      "name": "Indexing",
      "path": "/track/databases/indexing",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "dsa",
      "topic": "linked-list",
      "name": "Linked List",
      "path": "/track/dsa/linked-list",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 9
    },
    {
      "track": "ml",
      "topic": "llm",
      "name": "LLM",
      "path": "/track/ml/llm",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 21
    },
    {
      "track": "ai-agents",
      "topic": "memory",
      "name": "Memory",
      "path": "/track/ai-agents/memory",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "ml",
      "topic": "mlops",
      "name": "MLOps",
      "path": "/track/ml/mlops",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 10
    },
    {
      "track": "ml",
      "topic": "models",
      "name": "Models",
      "path": "/track/ml/models",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 15
    },
    {
      "track": "dsa",
      "topic": "monotonic-stack",
      "name": "Monotonic Stack",
      "path": "/track/dsa/monotonic-stack",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 5
    },
    {
      "track": "ai-agents",
      "topic": "multi-agent",
      "name": "Multi Agent",
      "path": "/track/ai-agents/multi-agent",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "databases",
      "topic": "nosql",
      "name": "Nosql",
      "path": "/track/databases/nosql",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "ai-agents",
      "topic": "observability",
      "name": "Observability",
      "path": "/track/ai-agents/observability",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "ml",
      "topic": "optimization",
      "name": "Optimization",
      "path": "/track/ml/optimization",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 16
    },
    {
      "track": "software-engineering",
      "topic": "performance",
      "name": "Performance",
      "path": "/track/software-engineering/performance",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "ai-agents",
      "topic": "planning",
      "name": "Planning",
      "path": "/track/ai-agents/planning",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "ai-agents",
      "topic": "prompting",
      "name": "Prompting",
      "path": "/track/ai-agents/prompting",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "software-engineering",
      "topic": "python",
      "name": "Python",
      "path": "/track/software-engineering/python",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "databases",
      "topic": "query-plans",
      "name": "Query Plans",
      "path": "/track/databases/query-plans",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "ai-agents",
      "topic": "rag",
      "name": "Rag",
      "path": "/track/ai-agents/rag",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "name": "Reinforcement Learning",
      "path": "/track/ml/reinforcement-learning",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 12
    },
    {
      "track": "databases",
      "topic": "relational",
      "name": "Relational",
      "path": "/track/databases/relational",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "ml",
      "topic": "representation",
      "name": "Representation",
      "path": "/track/ml/representation",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 0
    },
    {
      "track": "software-engineering",
      "topic": "rust",
      "name": "Rust",
      "path": "/track/software-engineering/rust",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "databases",
      "topic": "schema-design",
      "name": "Schema Design",
      "path": "/track/databases/schema-design",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "software-engineering",
      "topic": "security-basics",
      "name": "Security Basics",
      "path": "/track/software-engineering/security-basics",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "databases",
      "topic": "sql-patterns",
      "name": "SQL Patterns",
      "path": "/track/databases/sql-patterns",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "dsa",
      "topic": "stack-and-queue",
      "name": "Stack And Queue",
      "path": "/track/dsa/stack-and-queue",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 9
    },
    {
      "track": "databases",
      "topic": "streaming",
      "name": "Streaming",
      "path": "/track/databases/streaming",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "dsa",
      "topic": "string",
      "name": "String",
      "path": "/track/dsa/string",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 6
    },
    {
      "track": "software-engineering",
      "topic": "system-design",
      "name": "System Design",
      "path": "/track/software-engineering/system-design",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "ml",
      "topic": "systems",
      "name": "Systems",
      "path": "/track/ml/systems",
      "hasDoc": true,
      "docCount": 1,
      "moduleCount": 8
    },
    {
      "track": "software-engineering",
      "topic": "testing",
      "name": "Testing",
      "path": "/track/software-engineering/testing",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "ai-agents",
      "topic": "tool-use",
      "name": "Tool Use",
      "path": "/track/ai-agents/tool-use",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "software-engineering",
      "topic": "tooling",
      "name": "Tooling",
      "path": "/track/software-engineering/tooling",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "databases",
      "topic": "transactions",
      "name": "Transactions",
      "path": "/track/databases/transactions",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    },
    {
      "track": "databases",
      "topic": "vector-db",
      "name": "Vector Db",
      "path": "/track/databases/vector-db",
      "hasDoc": false,
      "docCount": 0,
      "moduleCount": 0
    }
  ],
  "modules": [
    {
      "track": "dsa",
      "topic": "array",
      "slug": "209-minimum-size-subarray-sum",
      "title": "209.Minimum Size Subarray Sum",
      "path": "modules/dsa/array/209-minimum-size-subarray-sum",
      "summary": "> Track: `dsa` | Topic: `array`",
      "readme": "# 209.Minimum Size Subarray Sum\n\n> Track: `dsa` | Topic: `array`\n\n## Concept\n\nUse a sliding window to keep the smallest window with sum >= target.\n\n## Function\n\n```python\nclass Solution:\n    def minSubArrayLen(self, target: int, nums: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/array/209-minimum-size-subarray-sum/python/minimum_size_subarray_sum.py",
          "language": "python",
          "content": "from typing import List\n\n# (Version 1) Sliding window\nclass Solution:\n    def minSubArrayLen(self, target: int, nums: List[int]) -> int:\n        left = 0\n        right = 0\n        length = float('inf')\n        sum = 0\n        while right < len(nums):\n            sum += nums[right]\n            while sum >= target:\n                length = min(length, right - left + 1)\n                sum -= nums[left]\n                left += 1\n            right += 1\n        return length if length != float('inf') else 0\n\n# (Version 2) Brute force\nclass Solution:\n    def minSubArrayLen(self, target: int, nums: List[int]) -> int:\n        length = float('inf')\n        for i in range(len(nums)):\n            sum = 0\n            for j in range(i, len(nums)):\n                sum += nums[j]\n                if sum >= target:\n                    length = min(length, j - i + 1)\n                    break\n        return length if length != float('inf') else 0\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "array",
      "slug": "27-remove-element",
      "title": "27.Remove Element",
      "path": "modules/dsa/array/27-remove-element",
      "summary": "> Track: `dsa` | Topic: `array`",
      "readme": "# 27.Remove Element\n\n> Track: `dsa` | Topic: `array`\n\n## Concept\n\nRemove target values in-place and return the new length.\n\n## Function\n\n```python\nclass Solution:\n    def removeElement(self, nums: List[int], val: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/array/27-remove-element/python/remove_element.py",
          "language": "python",
          "content": "from typing import List\n\n# (Version 1) Fast-slow pointer\nclass Solution:\n    def removeElement(self, nums: List[int], val: int) -> int:\n        # slow pointer\n        slowIndex = 0\n        # fast pointer\n        for fastIndex in range(len(nums)):\n            if val != nums[fastIndex]:\n                nums[slowIndex] = nums[fastIndex]\n                slowIndex += 1\n        return slowIndex\n# (Version 2) Brute force\nclass Solution:\n    def removeElement(self, nums: List[int], val: int) -> int:\n        i = 0\n        size = len(nums)\n        while i < size:\n            if nums[i] == val:\n                for j in range(i + 1, size):\n                    nums[j - 1] = nums[j]\n                size -= 1\n                i -= 1\n            i += 1\n        return size\n# (Version 3) Two pointers, remove elements from both ends\nclass Solution:\n    def removeElement(self, nums: List[int], val: int) -> int:\n        # left pointer\n        left = 0\n        # right pointer\n        right = len(nums) - 1\n        while left <= right:\n            # find left pointer position where element equals val\n            while left <= right and nums[left] != val:\n                left += 1\n            # find right pointer position where element not equals val\n            while left <= right and nums[right] == val:\n                right -= 1\n            # overwrite left pointer position with right pointer's element\n            if left < right:\n                nums[left] = nums[right]\n                left += 1\n                right -= 1\n        return left\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "array",
      "slug": "59-spiral-matrix-ii",
      "title": "59.Spiral Matrix II",
      "path": "modules/dsa/array/59-spiral-matrix-ii",
      "summary": "> Track: `dsa` | Topic: `array`",
      "readme": "# 59.Spiral Matrix II\n\n> Track: `dsa` | Topic: `array`\n\n## Concept\n\nFill an n x n grid layer by layer while incrementing a counter.\n\n## Function\n\n```python\nclass Solution:\n    def generateMatrix(self, n: int) -> List[List[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/array/59-spiral-matrix-ii/python/spiral_matrix_ii.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def generateMatrix(self, n: int) -> List[List[int]]:\n        nums = [[0] * n for _ in range(n)]\n        startx, starty = 0, 0  # Starting point\n        loop, mid = n // 2, n // 2  # Number of loops and middle position\n        count = 1  # Counter\n\n        for offset in range(1, loop + 1):\n            for i in range(starty, n - offset):\n                nums[startx][i] = count\n                count += 1\n            for i in range(startx, n - offset):\n                nums[i][n - offset] = count\n                count += 1\n            for i in range(n - offset, starty, -1):\n                nums[n - offset][i] = count\n                count += 1\n            for i in range(n - offset, startx, -1):\n                nums[i][starty] = count\n                count += 1\n            startx += 1\n            starty += 1\n        if n % 2 != 0:\n            nums[mid][mid] = count\n        return nums\n# Version 2\nclass Solution(object):\n    def generateMatrix(self, n):\n        nums = [[0] * n for _ in range(n)]\n        # Define boundaries\n        left, right = 0, n - 1\n        top, bottom = 0, n - 1\n        num = 1\n\n        while left <= right and top <= bottom:\n            for i in range(left, right + 1):\n                nums[top][i] = num\n                num += 1\n            for i in range(top + 1, bottom + 1):\n                nums[i][right] = num\n                num += 1\n            if left < right and top < bottom:\n                for i in range(right - 1, left, -1):\n                    nums[bottom][i] = num\n                    num += 1\n                for i in range(bottom, top, -1):\n                    nums[i][left] = num\n                    num += 1\n            left += 1\n            right -= 1\n            top += 1\n            bottom -= 1\n        return nums\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "array",
      "slug": "704-binary-search",
      "title": "704.Binary Search",
      "path": "modules/dsa/array/704-binary-search",
      "summary": "> Track: `dsa` | Topic: `array`",
      "readme": "# 704.Binary Search\n\n> Track: `dsa` | Topic: `array`\n\n## Concept\n\nUse binary search on a sorted list to find the target index or -1.\n\n## Function\n\n```python\nclass Solution:\n    def search(self, nums: List[int], target: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/array/704-binary-search/python/binary_search.py",
          "language": "python",
          "content": "from typing import List\n\n# (Version 1) Closed Interval, [left, right]\nclass Solution:\n    def search(self, nums: List[int], target: int) -> int:\n        left, right = 0, len(nums) - 1 # Define the target in the left-closed right-closed interval, [left, right]\n        while left <= right:\n            middle = (left + right) // 2\n            if nums[middle] > target:\n                right = middle - 1\n            elif nums[middle] < target:\n                left = middle + 1\n            else:\n                return middle\n        return -1\n# (Version 2) Left-closed right-open interval, [left, right)\nclass Solution:\n    def search(self, nums: List[int], target: int) -> int:\n        left, right = 0, len(nums) # Define the target in the left-closed right-open interval, [left, right)\n        while left < right:\n            middle = (left + right) // 2\n            if nums[middle] > target:\n                right = middle\n            elif nums[middle] < target:\n                left = middle + 1\n            else:\n                return middle\n        return -1\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "array",
      "slug": "977-squares-of-a-sorted-array",
      "title": "977.Squares of a Sorted Array",
      "path": "modules/dsa/array/977-squares-of-a-sorted-array",
      "summary": "> Track: `dsa` | Topic: `array`",
      "readme": "# 977.Squares of a Sorted Array\n\n> Track: `dsa` | Topic: `array`\n\n## Concept\n\nUse two pointers to fill the output from largest square to smallest in O(n).\n\n## Function\n\n```python\nclass Solution:\n    def sortedSquares(self, nums: List[int]) -> List[int]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/array/977-squares-of-a-sorted-array/python/squares_of_a_sorted_array.py",
          "language": "python",
          "content": "from typing import List\n\n# (Version 1) Two pointers\nclass Solution:\n    def sortedSquares(self, nums: List[int]) -> List[int]:\n        # array is sorted, in increasing order\n        n = len(nums)\n        # define a new array to store result\n        result = [0] * n\n        # left pointer\n        i = 0\n        # right pointer\n        j = n - 1\n        # result array pointer\n        k = n - 1\n        while i <= j:\n            if nums[i] ** 2 < nums[j] ** 2:\n                result[k] = nums[j] ** 2\n                j -= 1\n            else:\n                result[k] = nums[i] ** 2\n                i += 1\n            k -= 1\n        return result\n# (Version 2) Sort after square\nclass Solution:\n    def sortedSquares(self, nums: List[int]) -> List[int]:\n        for i in range(len(nums)):\n            nums[i] **= 2\n        nums.sort()\n        return nums\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "array",
      "slug": "array-summary",
      "title": "Array Summary",
      "path": "modules/dsa/array/array-summary",
      "summary": "> Track: `dsa` | Topic: `array`",
      "readme": "# Array Summary\n\n> Track: `dsa` | Topic: `array`\n\n## Concept\n\nReview array patterns: binary search, two pointers, sliding window, and matrix traversal.\n",
      "sources": []
    },
    {
      "track": "dsa",
      "topic": "array",
      "slug": "introduction-to-arrays",
      "title": "Introduction to Arrays",
      "path": "modules/dsa/array/introduction-to-arrays",
      "summary": "> Track: `dsa` | Topic: `array`",
      "readme": "# Introduction to Arrays\n\n> Track: `dsa` | Topic: `array`\n\n## Concept\n\nArrays provide contiguous storage and O(1) index access.\n",
      "sources": []
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "131-palindrome-partitioning",
      "title": "131.Palindrome Partitioning",
      "path": "modules/dsa/backtracking/131-palindrome-partitioning",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# 131.Palindrome Partitioning\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nBacktrack over substrings and keep only palindromic segments.\n\n## Function\n\n```python\nclass Solution:\n    def partition(self, s: str) -> list[list[str]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/backtracking/131-palindrome-partitioning/python/problem_131_palindrome_partitioning.py",
          "language": "python",
          "content": "class Solution:\n    def partition(self, s: str) -> list[list[str]]:\n        result: list[list[str]] = []\n        path: list[str] = []\n\n        def is_palindrome(left: int, right: int) -> bool:\n            while left < right:\n                if s[left] != s[right]:\n                    return False\n                left += 1\n                right -= 1\n            return True\n\n        def backtrack(start: int) -> None:\n            if start == len(s):\n                result.append(path.copy())\n                return\n            for end in range(start, len(s)):\n                if is_palindrome(start, end):\n                    path.append(s[start : end + 1])\n                    backtrack(end + 1)\n                    path.pop()\n\n        backtrack(0)\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "17-letter-combinations-of-a-phone-number",
      "title": "17.Letter Combinations of a Phone Number",
      "path": "modules/dsa/backtracking/17-letter-combinations-of-a-phone-number",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# 17.Letter Combinations of a Phone Number\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nMap digits to letters and backtrack through each position.\n\n## Function\n\n```python\nclass Solution:\n    def letterCombinations(self, digits: str) -> list[str]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/backtracking/17-letter-combinations-of-a-phone-number/python/problem_17_letter_combinations_of_a_phone_number.py",
          "language": "python",
          "content": "class Solution:\n    def letterCombinations(self, digits: str) -> list[str]:\n        if not digits:\n            return []\n        mapping = {\n            \"2\": \"abc\",\n            \"3\": \"def\",\n            \"4\": \"ghi\",\n            \"5\": \"jkl\",\n            \"6\": \"mno\",\n            \"7\": \"pqrs\",\n            \"8\": \"tuv\",\n            \"9\": \"wxyz\",\n        }\n        result: list[str] = []\n        path: list[str] = []\n\n        def backtrack(index: int) -> None:\n            if index == len(digits):\n                result.append(\"\".join(path))\n                return\n            for ch in mapping[digits[index]]:\n                path.append(ch)\n                backtrack(index + 1)\n                path.pop()\n\n        backtrack(0)\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "216-combination-sum-iii",
      "title": "216.Combination Sum III",
      "path": "modules/dsa/backtracking/216-combination-sum-iii",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# 216.Combination Sum III\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nChoose k distinct numbers from 1..9 that sum to n.\n\n## Function\n\n```python\nclass Solution:\n    def combinationSum3(self, k: int, n: int) -> list[list[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/backtracking/216-combination-sum-iii/python/problem_216_combination_sum_iii.py",
          "language": "python",
          "content": "class Solution:\n    def combinationSum3(self, k: int, n: int) -> list[list[int]]:\n        result: list[list[int]] = []\n        path: list[int] = []\n\n        def backtrack(start: int, remaining: int) -> None:\n            if len(path) == k:\n                if remaining == 0:\n                    result.append(path.copy())\n                return\n            for i in range(start, 10):\n                if i > remaining:\n                    break\n                path.append(i)\n                backtrack(i + 1, remaining - i)\n                path.pop()\n\n        backtrack(1, n)\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "332-reconstruct-itinerary",
      "title": "332.Reconstruct Itinerary",
      "path": "modules/dsa/backtracking/332-reconstruct-itinerary",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# 332.Reconstruct Itinerary\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nUse backtracking with lexicographic ordering to build a valid itinerary.\n\n## Function\n\n```python\nclass Solution:\n    def findItinerary(self, tickets: list[list[str]]) -> list[str]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/backtracking/332-reconstruct-itinerary/python/problem_332_reconstruct_itinerary.py",
          "language": "python",
          "content": "from collections import defaultdict\n\nclass Solution:\n    def findItinerary(self, tickets: list[list[str]]) -> list[str]:\n        graph: dict[str, list[str]] = defaultdict(list)\n        for src, dst in sorted(tickets, reverse=True):\n            graph[src].append(dst)\n        route: list[str] = []\n\n        def visit(airport: str) -> None:\n            while graph[airport]:\n                visit(graph[airport].pop())\n            route.append(airport)\n\n        visit(\"JFK\")\n        return route[::-1]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "37-sudoku-solver",
      "title": "37.Sudoku Solver",
      "path": "modules/dsa/backtracking/37-sudoku-solver",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# 37.Sudoku Solver\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nFill empty cells with valid digits using backtracking and constraint sets.\n\n## Function\n\n```python\nclass Solution:\n    def solveSudoku(self, board: list[list[str]]) -> None:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/backtracking/37-sudoku-solver/python/problem_37_sudoku_solver.py",
          "language": "python",
          "content": "class Solution:\n    def solveSudoku(self, board: list[list[str]]) -> None:\n        rows = [set() for _ in range(9)]\n        cols = [set() for _ in range(9)]\n        boxes = [set() for _ in range(9)]\n        empties: list[tuple[int, int]] = []\n\n        for r in range(9):\n            for c in range(9):\n                value = board[r][c]\n                if value == \".\":\n                    empties.append((r, c))\n                else:\n                    rows[r].add(value)\n                    cols[c].add(value)\n                    boxes[(r // 3) * 3 + (c // 3)].add(value)\n\n        def backtrack(index: int) -> bool:\n            if index == len(empties):\n                return True\n            r, c = empties[index]\n            b = (r // 3) * 3 + (c // 3)\n            for digit in map(str, range(1, 10)):\n                if digit in rows[r] or digit in cols[c] or digit in boxes[b]:\n                    continue\n                board[r][c] = digit\n                rows[r].add(digit)\n                cols[c].add(digit)\n                boxes[b].add(digit)\n                if backtrack(index + 1):\n                    return True\n                board[r][c] = \".\"\n                rows[r].remove(digit)\n                cols[c].remove(digit)\n                boxes[b].remove(digit)\n            return False\n\n        backtrack(0)\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "39-combination-sum",
      "title": "39.Combination Sum",
      "path": "modules/dsa/backtracking/39-combination-sum",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# 39.Combination Sum\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nChoose candidates with repetition to reach the target sum.\n\n## Function\n\n```python\nclass Solution:\n    def combinationSum(self, candidates: List[int], target: int) -> List[List[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/backtracking/39-combination-sum/python/problem_39_combination_sum.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def combinationSum(self, candidates: List[int], target: int) -> List[List[int]]:\n        candidates.sort()\n        result: list[list[int]] = []\n        path: list[int] = []\n\n        def backtrack(start: int, remaining: int) -> None:\n            if remaining == 0:\n                result.append(path.copy())\n                return\n            for i in range(start, len(candidates)):\n                value = candidates[i]\n                if value > remaining:\n                    break\n                path.append(value)\n                backtrack(i, remaining - value)\n                path.pop()\n\n        backtrack(0, target)\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "40-combination-sum-ii",
      "title": "40.Combination Sum II",
      "path": "modules/dsa/backtracking/40-combination-sum-ii",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# 40.Combination Sum II\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nUse backtracking with sorting to skip duplicates and use each element once.\n\n## Function\n\n```python\nclass Solution:\n    def combinationSum2(self, candidates: List[int], target: int) -> List[List[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/backtracking/40-combination-sum-ii/python/problem_40_combination_sum_ii.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def combinationSum2(self, candidates: List[int], target: int) -> List[List[int]]:\n        candidates.sort()\n        result: list[list[int]] = []\n        path: list[int] = []\n\n        def backtrack(start: int, remaining: int) -> None:\n            if remaining == 0:\n                result.append(path.copy())\n                return\n            for i in range(start, len(candidates)):\n                if i > start and candidates[i] == candidates[i - 1]:\n                    continue\n                value = candidates[i]\n                if value > remaining:\n                    break\n                path.append(value)\n                backtrack(i + 1, remaining - value)\n                path.pop()\n\n        backtrack(0, target)\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "46-permutations",
      "title": "46.Permutations",
      "path": "modules/dsa/backtracking/46-permutations",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# 46.Permutations\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nBuild permutations by choosing unused numbers at each step.\n\n## Function\n\n```python\nclass Solution:\n    def permute(self, nums: List[int]) -> List[List[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/backtracking/46-permutations/python/problem_46_permutations.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def permute(self, nums: List[int]) -> List[List[int]]:\n        result: list[list[int]] = []\n        path: list[int] = []\n        used = [False] * len(nums)\n\n        def backtrack() -> None:\n            if len(path) == len(nums):\n                result.append(path.copy())\n                return\n            for i, value in enumerate(nums):\n                if used[i]:\n                    continue\n                used[i] = True\n                path.append(value)\n                backtrack()\n                path.pop()\n                used[i] = False\n\n        backtrack()\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "47-permutations-ii",
      "title": "47.Permutations II",
      "path": "modules/dsa/backtracking/47-permutations-ii",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# 47.Permutations II\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nSort and skip duplicate choices to avoid repeated permutations.\n\n## Function\n\n```python\nclass Solution:\n    def permuteUnique(self, nums: List[int]) -> List[List[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/backtracking/47-permutations-ii/python/problem_47_permutations_ii.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def permuteUnique(self, nums: List[int]) -> List[List[int]]:\n        nums.sort()\n        result: list[list[int]] = []\n        path: list[int] = []\n        used = [False] * len(nums)\n\n        def backtrack() -> None:\n            if len(path) == len(nums):\n                result.append(path.copy())\n                return\n            for i in range(len(nums)):\n                if used[i]:\n                    continue\n                if i > 0 and nums[i] == nums[i - 1] and not used[i - 1]:\n                    continue\n                used[i] = True\n                path.append(nums[i])\n                backtrack()\n                path.pop()\n                used[i] = False\n\n        backtrack()\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "491-increasing-subsequences",
      "title": "491.Increasing Subsequences",
      "path": "modules/dsa/backtracking/491-increasing-subsequences",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# 491.Increasing Subsequences\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nTrack non-decreasing subsequences while avoiding duplicates per depth.\n\n## Function\n\n```python\nclass Solution:\n    def findSubsequences(self, nums: list[int]) -> list[list[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/backtracking/491-increasing-subsequences/python/problem_491_increasing_subsequences.py",
          "language": "python",
          "content": "class Solution:\n    def findSubsequences(self, nums: list[int]) -> list[list[int]]:\n        result: list[list[int]] = []\n        path: list[int] = []\n\n        def backtrack(start: int) -> None:\n            if len(path) >= 2:\n                result.append(path.copy())\n            used: set[int] = set()\n            for i in range(start, len(nums)):\n                if path and nums[i] < path[-1]:\n                    continue\n                if nums[i] in used:\n                    continue\n                used.add(nums[i])\n                path.append(nums[i])\n                backtrack(i + 1)\n                path.pop()\n\n        backtrack(0)\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "51-n-queens",
      "title": "51.N-Queens",
      "path": "modules/dsa/backtracking/51-n-queens",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# 51.N-Queens\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nPlace queens row by row while tracking columns and diagonals.\n\n## Function\n\n```python\nclass Solution:\n    def solveNQueens(self, n: int) -> list[list[str]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/backtracking/51-n-queens/python/problem_51_n_queens.py",
          "language": "python",
          "content": "class Solution:\n    def solveNQueens(self, n: int) -> list[list[str]]:\n        result: list[list[str]] = []\n        cols = set()\n        diag1 = set()\n        diag2 = set()\n        board = [[\".\"] * n for _ in range(n)]\n\n        def backtrack(row: int) -> None:\n            if row == n:\n                result.append([\"\".join(r) for r in board])\n                return\n            for col in range(n):\n                if col in cols or (row - col) in diag1 or (row + col) in diag2:\n                    continue\n                cols.add(col)\n                diag1.add(row - col)\n                diag2.add(row + col)\n                board[row][col] = \"Q\"\n                backtrack(row + 1)\n                board[row][col] = \".\"\n                cols.remove(col)\n                diag1.remove(row - col)\n                diag2.remove(row + col)\n\n        backtrack(0)\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "77-combinations-optimized",
      "title": "77.Combinations (Optimized)",
      "path": "modules/dsa/backtracking/77-combinations-optimized",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# 77.Combinations (Optimized)\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nPrune the upper bound to avoid exploring impossible branches.\n\n## Function\n\n```python\nclass Solution:\n    def combine(self, n: int, k: int) -> List[List[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/backtracking/77-combinations-optimized/python/problem_77_combinations_optimized.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def combine(self, n: int, k: int) -> List[List[int]]:\n        result: list[list[int]] = []\n        path: list[int] = []\n\n        def backtrack(start: int) -> None:\n            if len(path) == k:\n                result.append(path.copy())\n                return\n            max_start = n - (k - len(path)) + 1\n            for i in range(start, max_start + 1):\n                path.append(i)\n                backtrack(i + 1)\n                path.pop()\n\n        backtrack(1)\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "77-combinations",
      "title": "77.Combinations",
      "path": "modules/dsa/backtracking/77-combinations",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# 77.Combinations\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nChoose k numbers in ascending order to form combinations.\n\n## Function\n\n```python\nclass Solution:\n    def combine(self, n: int, k: int) -> List[List[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/backtracking/77-combinations/python/problem_77_combinations.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def combine(self, n: int, k: int) -> List[List[int]]:\n        result: list[list[int]] = []\n        path: list[int] = []\n\n        def backtrack(start: int) -> None:\n            if len(path) == k:\n                result.append(path.copy())\n                return\n            for i in range(start, n + 1):\n                path.append(i)\n                backtrack(i + 1)\n                path.pop()\n\n        backtrack(1)\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "78-subsets",
      "title": "78.Subsets",
      "path": "modules/dsa/backtracking/78-subsets",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# 78.Subsets\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nInclude or skip each element to build all subsets.\n\n## Function\n\n```python\nclass Solution:\n    def subsets(self, nums: List[int]) -> List[List[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/backtracking/78-subsets/python/problem_78_subsets.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def subsets(self, nums: List[int]) -> List[List[int]]:\n        result: list[list[int]] = []\n        path: list[int] = []\n\n        def backtrack(start: int) -> None:\n            result.append(path.copy())\n            for i in range(start, len(nums)):\n                path.append(nums[i])\n                backtrack(i + 1)\n                path.pop()\n\n        backtrack(0)\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "90-subsets-ii",
      "title": "90.Subsets II",
      "path": "modules/dsa/backtracking/90-subsets-ii",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# 90.Subsets II\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nSort and skip duplicates at each depth to avoid repeated subsets.\n\n## Function\n\n```python\nclass Solution:\n    def subsetsWithDup(self, nums: List[int]) -> List[List[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/backtracking/90-subsets-ii/python/problem_90_subsets_ii.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def subsetsWithDup(self, nums: List[int]) -> List[List[int]]:\n        nums.sort()\n        result: list[list[int]] = []\n        path: list[int] = []\n\n        def backtrack(start: int) -> None:\n            result.append(path.copy())\n            for i in range(start, len(nums)):\n                if i > start and nums[i] == nums[i - 1]:\n                    continue\n                path.append(nums[i])\n                backtrack(i + 1)\n                path.pop()\n\n        backtrack(0)\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "93-restore-ip-addresses",
      "title": "93.Restore IP Addresses",
      "path": "modules/dsa/backtracking/93-restore-ip-addresses",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# 93.Restore IP Addresses\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nSplit the string into four valid segments with backtracking.\n\n## Function\n\n```python\nclass Solution:\n    def restoreIpAddresses(self, s: str) -> list[str]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/backtracking/93-restore-ip-addresses/python/problem_93_restore_ip_addresses.py",
          "language": "python",
          "content": "class Solution:\n    def restoreIpAddresses(self, s: str) -> list[str]:\n        result: list[str] = []\n        path: list[str] = []\n\n        def is_valid(segment: str) -> bool:\n            if not segment:\n                return False\n            if len(segment) > 1 and segment[0] == \"0\":\n                return False\n            return 0 <= int(segment) <= 255\n\n        def backtrack(start: int) -> None:\n            if len(path) == 4:\n                if start == len(s):\n                    result.append(\".\".join(path))\n                return\n            for end in range(start + 1, min(start + 4, len(s) + 1)):\n                segment = s[start:end]\n                if is_valid(segment):\n                    path.append(segment)\n                    backtrack(end)\n                    path.pop()\n\n        backtrack(0)\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "alternative-deduplication-in-backtracking",
      "title": "Alternative Deduplication in Backtracking",
      "path": "modules/dsa/backtracking/alternative-deduplication-in-backtracking",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# Alternative Deduplication in Backtracking\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nUse a local set at each depth to skip duplicate choices.\n\n## Function\n\n```python\nclass Solution:\n    def subsetsWithDup(self, nums: List[int]) -> List[List[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/backtracking/alternative-deduplication-in-backtracking/python/alternative_deduplication_in_backtracking.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def subsetsWithDup(self, nums: List[int]) -> List[List[int]]:\n        result: list[list[int]] = []\n        path: list[int] = []\n\n        def backtrack(start: int) -> None:\n            result.append(path.copy())\n            used: set[int] = set()\n            for i in range(start, len(nums)):\n                if nums[i] in used:\n                    continue\n                used.add(nums[i])\n                path.append(nums[i])\n                backtrack(i + 1)\n                path.pop()\n\n        backtrack(0)\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "backtracking-summary",
      "title": "Backtracking Summary",
      "path": "modules/dsa/backtracking/backtracking-summary",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# Backtracking Summary\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nReview backtracking templates for combinations, permutations, subsets, and pruning.\n",
      "sources": []
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "introduction-to-backtracking",
      "title": "Introduction to Backtracking",
      "path": "modules/dsa/backtracking/introduction-to-backtracking",
      "summary": "> Track: `dsa` | Topic: `backtracking`",
      "readme": "# Introduction to Backtracking\n\n> Track: `dsa` | Topic: `backtracking`\n\n## Concept\n\nBacktracking builds solutions by exploring choices, then undoing them when a path is invalid.\n",
      "sources": []
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "101-symmetric-tree",
      "title": "101.Symmetric Tree",
      "path": "modules/dsa/binary-tree/101-symmetric-tree",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 101.Symmetric Tree\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nCheck whether the left and right subtrees are mirror images.\n\n## Function\n\n```python\nclass Solution:\n    def isSymmetric(self, root: Optional[TreeNode]) -> bool:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/101-symmetric-tree/python/problem_101_symmetric_tree.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def isSymmetric(self, root: Optional[TreeNode]) -> bool:\n        def mirror(left: Optional[TreeNode], right: Optional[TreeNode]) -> bool:\n            if left is None and right is None:\n                return True\n            if left is None or right is None:\n                return False\n            if left.val != right.val:\n                return False\n            return mirror(left.left, right.right) and mirror(left.right, right.left)\n\n        return mirror(root, root)\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "102-binary-tree-level-order-traversal",
      "title": "102.Binary Tree Level Order Traversal",
      "path": "modules/dsa/binary-tree/102-binary-tree-level-order-traversal",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 102.Binary Tree Level Order Traversal\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nPerform level-order traversal with a queue.\n\n## Function\n\n```python\nclass Solution:\n    def levelOrder(self, root: Optional[TreeNode]) -> List[List[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/102-binary-tree-level-order-traversal/python/problem_102_binary_tree_level_order_traversal.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom collections import deque\nfrom typing import Optional, List\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def levelOrder(self, root: Optional[TreeNode]) -> List[List[int]]:\n        if root is None:\n            return []\n        result: list[list[int]] = []\n        queue: deque[TreeNode] = deque([root])\n        while queue:\n            level: list[int] = []\n            for _ in range(len(queue)):\n                node = queue.popleft()\n                level.append(node.val)\n                if node.left:\n                    queue.append(node.left)\n                if node.right:\n                    queue.append(node.right)\n            result.append(level)\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "104-maximum-depth-of-binary-tree",
      "title": "104.Maximum Depth of Binary Tree",
      "path": "modules/dsa/binary-tree/104-maximum-depth-of-binary-tree",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 104.Maximum Depth of Binary Tree\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nCompute the maximum depth via recursion.\n\n## Function\n\n```python\nclass Solution:\n    def maxDepth(self, root: Optional[TreeNode]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/104-maximum-depth-of-binary-tree/python/problem_104_maximum_depth_of_binary_tree.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def maxDepth(self, root: Optional[TreeNode]) -> int:\n        if root is None:\n            return 0\n        return 1 + max(self.maxDepth(root.left), self.maxDepth(root.right))\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "106-construct-binary-tree-from-inorder-and-postorder-traversal",
      "title": "106.Construct Binary Tree from Inorder and Postorder Traversal",
      "path": "modules/dsa/binary-tree/106-construct-binary-tree-from-inorder-and-postorder-traversal",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 106.Construct Binary Tree from Inorder and Postorder Traversal\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nUse postorder's last element as root and split inorder ranges.\n\n## Function\n\n```python\nclass Solution:\n    def buildTree(self, inorder: List[int], postorder: List[int]) -> Optional[TreeNode]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/106-construct-binary-tree-from-inorder-and-postorder-traversal/python/problem_106_construct_binary_tree_from_inorder_and_postorder_traversal.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional, List\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def buildTree(self, inorder: List[int], postorder: List[int]) -> Optional[TreeNode]:\n        if not inorder or not postorder:\n            return None\n        index_map = {value: i for i, value in enumerate(inorder)}\n\n        def helper(left: int, right: int) -> Optional[TreeNode]:\n            if left > right:\n                return None\n            root_val = postorder.pop()\n            root = TreeNode(root_val)\n            idx = index_map[root_val]\n            root.right = helper(idx + 1, right)\n            root.left = helper(left, idx - 1)\n            return root\n\n        return helper(0, len(inorder) - 1)\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "108-convert-sorted-array-to-bst",
      "title": "108.Convert Sorted Array to BST",
      "path": "modules/dsa/binary-tree/108-convert-sorted-array-to-bst",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 108.Convert Sorted Array to BST\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nPick the middle value as root to build a balanced BST.\n\n## Function\n\n```python\nclass Solution:\n    def sortedArrayToBST(self, nums: List[int]) -> Optional[TreeNode]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/108-convert-sorted-array-to-bst/python/problem_108_convert_sorted_array_to_bst.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional, List\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def sortedArrayToBST(self, nums: List[int]) -> Optional[TreeNode]:\n        def helper(left: int, right: int) -> Optional[TreeNode]:\n            if left > right:\n                return None\n            mid = (left + right) // 2\n            root = TreeNode(nums[mid])\n            root.left = helper(left, mid - 1)\n            root.right = helper(mid + 1, right)\n            return root\n\n        return helper(0, len(nums) - 1)\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "110-balanced-binary-tree",
      "title": "110.Balanced Binary Tree",
      "path": "modules/dsa/binary-tree/110-balanced-binary-tree",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 110.Balanced Binary Tree\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nTrack subtree heights and detect imbalance early.\n\n## Function\n\n```python\nclass Solution:\n    def isBalanced(self, root: Optional[TreeNode]) -> bool:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/110-balanced-binary-tree/python/problem_110_balanced_binary_tree.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def isBalanced(self, root: Optional[TreeNode]) -> bool:\n        def height(node: Optional[TreeNode]) -> int:\n            if node is None:\n                return 0\n            left = height(node.left)\n            if left == -1:\n                return -1\n            right = height(node.right)\n            if right == -1:\n                return -1\n            if abs(left - right) > 1:\n                return -1\n            return max(left, right) + 1\n\n        return height(root) != -1\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "111-minimum-depth-of-binary-tree",
      "title": "111.Minimum Depth of Binary Tree",
      "path": "modules/dsa/binary-tree/111-minimum-depth-of-binary-tree",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 111.Minimum Depth of Binary Tree\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nHandle missing children when computing minimum depth.\n\n## Function\n\n```python\nclass Solution:\n    def minDepth(self, root: Optional[TreeNode]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/111-minimum-depth-of-binary-tree/python/problem_111_minimum_depth_of_binary_tree.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def minDepth(self, root: Optional[TreeNode]) -> int:\n        if root is None:\n            return 0\n        if root.left is None:\n            return 1 + self.minDepth(root.right)\n        if root.right is None:\n            return 1 + self.minDepth(root.left)\n        return 1 + min(self.minDepth(root.left), self.minDepth(root.right))\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "112-path-sum",
      "title": "112.Path Sum",
      "path": "modules/dsa/binary-tree/112-path-sum",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 112.Path Sum\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nCheck for a root-to-leaf path with the target sum.\n\n## Function\n\n```python\nclass Solution:\n    def hasPathSum(self, root: Optional[TreeNode], targetSum: int) -> bool:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/112-path-sum/python/problem_112_path_sum.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def hasPathSum(self, root: Optional[TreeNode], targetSum: int) -> bool:\n        if root is None:\n            return False\n        if root.left is None and root.right is None:\n            return root.val == targetSum\n        return self.hasPathSum(root.left, targetSum - root.val) or self.hasPathSum(\n            root.right, targetSum - root.val\n        )\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "222-count-complete-tree-nodes",
      "title": "222.Count Complete Tree Nodes",
      "path": "modules/dsa/binary-tree/222-count-complete-tree-nodes",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 222.Count Complete Tree Nodes\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nCount nodes with a simple DFS.\n\n## Function\n\n```python\nclass Solution:\n    def countNodes(self, root: Optional[TreeNode]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/222-count-complete-tree-nodes/python/problem_222_count_complete_tree_nodes.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def countNodes(self, root: Optional[TreeNode]) -> int:\n        if root is None:\n            return 0\n        return 1 + self.countNodes(root.left) + self.countNodes(root.right)\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "226-invert-binary-tree",
      "title": "226.Invert Binary Tree",
      "path": "modules/dsa/binary-tree/226-invert-binary-tree",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 226.Invert Binary Tree\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nSwap left and right children recursively.\n\n## Function\n\n```python\nclass Solution:\n    def invertTree(self, root: Optional[TreeNode]) -> Optional[TreeNode]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/226-invert-binary-tree/python/problem_226_invert_binary_tree.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def invertTree(self, root: Optional[TreeNode]) -> Optional[TreeNode]:\n        if root is None:\n            return None\n        root.left, root.right = self.invertTree(root.right), self.invertTree(root.left)\n        return root\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "235-lowest-common-ancestor-of-a-bst",
      "title": "235.Lowest Common Ancestor of a BST",
      "path": "modules/dsa/binary-tree/235-lowest-common-ancestor-of-a-bst",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 235.Lowest Common Ancestor of a BST\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nUse BST ordering to walk down to the split point.\n\n## Function\n\n```python\nclass Solution:\n    def lowestCommonAncestor(self, root: Optional[TreeNode], p: TreeNode, q: TreeNode) -> Optional[TreeNode]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/235-lowest-common-ancestor-of-a-bst/python/problem_235_lowest_common_ancestor_of_a_bst.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def lowestCommonAncestor(self, root: Optional[TreeNode], p: TreeNode, q: TreeNode) -> Optional[TreeNode]:\n        current = root\n        while current:\n            if p.val < current.val and q.val < current.val:\n                current = current.left\n            elif p.val > current.val and q.val > current.val:\n                current = current.right\n            else:\n                return current\n        return None\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "236-lowest-common-ancestor-of-a-binary-tree",
      "title": "236.Lowest Common Ancestor of a Binary Tree",
      "path": "modules/dsa/binary-tree/236-lowest-common-ancestor-of-a-binary-tree",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 236.Lowest Common Ancestor of a Binary Tree\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nReturn the node where paths to p and q diverge.\n\n## Function\n\n```python\nclass Solution:\n    def lowestCommonAncestor(self, root: Optional[TreeNode], p: TreeNode, q: TreeNode) -> Optional[TreeNode]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/236-lowest-common-ancestor-of-a-binary-tree/python/problem_236_lowest_common_ancestor_of_a_binary_tree.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def lowestCommonAncestor(self, root: Optional[TreeNode], p: TreeNode, q: TreeNode) -> Optional[TreeNode]:\n        if root is None or root is p or root is q:\n            return root\n        left = self.lowestCommonAncestor(root.left, p, q)\n        right = self.lowestCommonAncestor(root.right, p, q)\n        if left and right:\n            return root\n        return left or right\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "257-binary-tree-paths",
      "title": "257.Binary Tree Paths",
      "path": "modules/dsa/binary-tree/257-binary-tree-paths",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 257.Binary Tree Paths\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nDFS to build all root-to-leaf path strings.\n\n## Function\n\n```python\nclass Solution:\n    def binaryTreePaths(self, root: Optional[TreeNode]) -> List[str]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/257-binary-tree-paths/python/problem_257_binary_tree_paths.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional, List\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def binaryTreePaths(self, root: Optional[TreeNode]) -> List[str]:\n        paths: list[str] = []\n\n        def dfs(node: Optional[TreeNode], path: str) -> None:\n            if node is None:\n                return\n            current = f\"{path}{node.val}\"\n            if node.left is None and node.right is None:\n                paths.append(current)\n                return\n            dfs(node.left, current + \"->\")\n            dfs(node.right, current + \"->\")\n\n        dfs(root, \"\")\n        return paths\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "404-sum-of-left-leaves",
      "title": "404.Sum of Left Leaves",
      "path": "modules/dsa/binary-tree/404-sum-of-left-leaves",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 404.Sum of Left Leaves\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nSum leaf nodes that are left children.\n\n## Function\n\n```python\nclass Solution:\n    def sumOfLeftLeaves(self, root: Optional[TreeNode]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/404-sum-of-left-leaves/python/problem_404_sum_of_left_leaves.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def sumOfLeftLeaves(self, root: Optional[TreeNode]) -> int:\n        if root is None:\n            return 0\n        total = 0\n        if root.left and root.left.left is None and root.left.right is None:\n            total += root.left.val\n        return total + self.sumOfLeftLeaves(root.left) + self.sumOfLeftLeaves(root.right)\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "450-delete-node-in-a-bst",
      "title": "450.Delete Node in a BST",
      "path": "modules/dsa/binary-tree/450-delete-node-in-a-bst",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 450.Delete Node in a BST\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nDelete a node and reconnect using the inorder successor.\n\n## Function\n\n```python\nclass Solution:\n    def deleteNode(self, root: Optional[TreeNode], key: int) -> Optional[TreeNode]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/450-delete-node-in-a-bst/python/problem_450_delete_node_in_a_bst.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def deleteNode(self, root: Optional[TreeNode], key: int) -> Optional[TreeNode]:\n        if root is None:\n            return None\n        if key < root.val:\n            root.left = self.deleteNode(root.left, key)\n        elif key > root.val:\n            root.right = self.deleteNode(root.right, key)\n        else:\n            if root.left is None:\n                return root.right\n            if root.right is None:\n                return root.left\n            successor = root.right\n            while successor.left:\n                successor = successor.left\n            root.val = successor.val\n            root.right = self.deleteNode(root.right, successor.val)\n        return root\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "501-find-mode-in-binary-search-tree",
      "title": "501.Find Mode in Binary Search Tree",
      "path": "modules/dsa/binary-tree/501-find-mode-in-binary-search-tree",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 501.Find Mode in Binary Search Tree\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nInorder traversal to count consecutive values.\n\n## Function\n\n```python\nclass Solution:\n    def findMode(self, root: Optional[TreeNode]) -> List[int]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/501-find-mode-in-binary-search-tree/python/problem_501_find_mode_in_binary_search_tree.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional, List\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def findMode(self, root: Optional[TreeNode]) -> List[int]:\n        self.prev: int | None = None\n        self.count = 0\n        self.max_count = 0\n        modes: list[int] = []\n\n        def inorder(node: Optional[TreeNode]) -> None:\n            if node is None:\n                return\n            inorder(node.left)\n            if self.prev == node.val:\n                self.count += 1\n            else:\n                self.count = 1\n            if self.count > self.max_count:\n                self.max_count = self.count\n                modes.clear()\n                modes.append(node.val)\n            elif self.count == self.max_count:\n                modes.append(node.val)\n            self.prev = node.val\n            inorder(node.right)\n\n        inorder(root)\n        return modes\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "513-find-bottom-left-tree-value",
      "title": "513.Find Bottom Left Tree Value",
      "path": "modules/dsa/binary-tree/513-find-bottom-left-tree-value",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 513.Find Bottom Left Tree Value\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nTrack the first node at each level with BFS.\n\n## Function\n\n```python\nclass Solution:\n    def findBottomLeftValue(self, root: Optional[TreeNode]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/513-find-bottom-left-tree-value/python/problem_513_find_bottom_left_tree_value.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom collections import deque\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def findBottomLeftValue(self, root: Optional[TreeNode]) -> int:\n        queue: deque[TreeNode] = deque([root])\n        leftmost = root.val\n        while queue:\n            leftmost = queue[0].val\n            for _ in range(len(queue)):\n                node = queue.popleft()\n                if node.left:\n                    queue.append(node.left)\n                if node.right:\n                    queue.append(node.right)\n        return leftmost\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "530-minimum-absolute-difference-in-bst",
      "title": "530.Minimum Absolute Difference in BST",
      "path": "modules/dsa/binary-tree/530-minimum-absolute-difference-in-bst",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 530.Minimum Absolute Difference in BST\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nInorder traversal to minimize adjacent differences.\n\n## Function\n\n```python\nclass Solution:\n    def getMinimumDifference(self, root: Optional[TreeNode]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/530-minimum-absolute-difference-in-bst/python/problem_530_minimum_absolute_difference_in_bst.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def getMinimumDifference(self, root: Optional[TreeNode]) -> int:\n        self.prev: int | None = None\n        self.min_diff = float(\"inf\")\n\n        def inorder(node: Optional[TreeNode]) -> None:\n            if node is None:\n                return\n            inorder(node.left)\n            if self.prev is not None:\n                self.min_diff = min(self.min_diff, node.val - self.prev)\n            self.prev = node.val\n            inorder(node.right)\n\n        inorder(root)\n        return int(self.min_diff)\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "538-convert-bst-to-greater-tree",
      "title": "538.Convert BST to Greater Tree",
      "path": "modules/dsa/binary-tree/538-convert-bst-to-greater-tree",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 538.Convert BST to Greater Tree\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nReverse inorder traversal accumulates greater sums.\n\n## Function\n\n```python\nclass Solution:\n    def convertBST(self, root: Optional[TreeNode]) -> Optional[TreeNode]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/538-convert-bst-to-greater-tree/python/problem_538_convert_bst_to_greater_tree.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def convertBST(self, root: Optional[TreeNode]) -> Optional[TreeNode]:\n        self.total = 0\n\n        def traverse(node: Optional[TreeNode]) -> None:\n            if node is None:\n                return\n            traverse(node.right)\n            self.total += node.val\n            node.val = self.total\n            traverse(node.left)\n\n        traverse(root)\n        return root\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "617-merge-two-binary-trees",
      "title": "617.Merge Two Binary Trees",
      "path": "modules/dsa/binary-tree/617-merge-two-binary-trees",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 617.Merge Two Binary Trees\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nMerge by summing overlapping nodes in DFS.\n\n## Function\n\n```python\nclass Solution:\n    def mergeTrees(self, root1: Optional[TreeNode], root2: Optional[TreeNode]) -> Optional[TreeNode]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/617-merge-two-binary-trees/python/problem_617_merge_two_binary_trees.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def mergeTrees(self, root1: Optional[TreeNode], root2: Optional[TreeNode]) -> Optional[TreeNode]:\n        if root1 is None:\n            return root2\n        if root2 is None:\n            return root1\n        root1.val += root2.val\n        root1.left = self.mergeTrees(root1.left, root2.left)\n        root1.right = self.mergeTrees(root1.right, root2.right)\n        return root1\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "654-maximum-binary-tree",
      "title": "654.Maximum Binary Tree",
      "path": "modules/dsa/binary-tree/654-maximum-binary-tree",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 654.Maximum Binary Tree\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nPick the maximum as root and recurse on subarrays.\n\n## Function\n\n```python\nclass Solution:\n    def constructMaximumBinaryTree(self, nums: List[int]) -> Optional[TreeNode]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/654-maximum-binary-tree/python/problem_654_maximum_binary_tree.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional, List\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def constructMaximumBinaryTree(self, nums: List[int]) -> Optional[TreeNode]:\n        if not nums:\n            return None\n        max_val = max(nums)\n        idx = nums.index(max_val)\n        root = TreeNode(max_val)\n        root.left = self.constructMaximumBinaryTree(nums[:idx])\n        root.right = self.constructMaximumBinaryTree(nums[idx + 1 :])\n        return root\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "669-trim-a-binary-search-tree",
      "title": "669.Trim a Binary Search Tree",
      "path": "modules/dsa/binary-tree/669-trim-a-binary-search-tree",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 669.Trim a Binary Search Tree\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nPrune subtrees outside the [low, high] range.\n\n## Function\n\n```python\nclass Solution:\n    def trimBST(self, root: Optional[TreeNode], low: int, high: int) -> Optional[TreeNode]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/669-trim-a-binary-search-tree/python/problem_669_trim_a_binary_search_tree.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def trimBST(self, root: Optional[TreeNode], low: int, high: int) -> Optional[TreeNode]:\n        if root is None:\n            return None\n        if root.val < low:\n            return self.trimBST(root.right, low, high)\n        if root.val > high:\n            return self.trimBST(root.left, low, high)\n        root.left = self.trimBST(root.left, low, high)\n        root.right = self.trimBST(root.right, low, high)\n        return root\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "700-search-in-a-binary-search-tree",
      "title": "700.Search in a Binary Search Tree",
      "path": "modules/dsa/binary-tree/700-search-in-a-binary-search-tree",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 700.Search in a Binary Search Tree\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nWalk down the BST comparing the target value.\n\n## Function\n\n```python\nclass Solution:\n    def searchBST(self, root: Optional[TreeNode], val: int) -> Optional[TreeNode]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/700-search-in-a-binary-search-tree/python/problem_700_search_in_a_binary_search_tree.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def searchBST(self, root: Optional[TreeNode], val: int) -> Optional[TreeNode]:\n        current = root\n        while current and current.val != val:\n            current = current.left if val < current.val else current.right\n        return current\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "701-insert-into-a-binary-search-tree",
      "title": "701.Insert into a Binary Search Tree",
      "path": "modules/dsa/binary-tree/701-insert-into-a-binary-search-tree",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 701.Insert into a Binary Search Tree\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nInsert by descending until a leaf position is found.\n\n## Function\n\n```python\nclass Solution:\n    def insertIntoBST(self, root: Optional[TreeNode], val: int) -> Optional[TreeNode]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/701-insert-into-a-binary-search-tree/python/problem_701_insert_into_a_binary_search_tree.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def insertIntoBST(self, root: Optional[TreeNode], val: int) -> Optional[TreeNode]:\n        if root is None:\n            return TreeNode(val)\n        if val < root.val:\n            root.left = self.insertIntoBST(root.left, val)\n        else:\n            root.right = self.insertIntoBST(root.right, val)\n        return root\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "98-validate-binary-search-tree",
      "title": "98.Validate Binary Search Tree",
      "path": "modules/dsa/binary-tree/98-validate-binary-search-tree",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# 98.Validate Binary Search Tree\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nValidate BST by ensuring each node value stays within bounds.\n\n## Function\n\n```python\nclass Solution:\n    def isValidBST(self, root: Optional[TreeNode]) -> bool:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/98-validate-binary-search-tree/python/problem_98_validate_binary_search_tree.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\nclass Solution:\n    def isValidBST(self, root: Optional[TreeNode]) -> bool:\n        def helper(node: Optional[TreeNode], low: int | None, high: int | None) -> bool:\n            if node is None:\n                return True\n            if low is not None and node.val <= low:\n                return False\n            if high is not None and node.val >= high:\n                return False\n            return helper(node.left, low, node.val) and helper(node.right, node.val, high)\n\n        return helper(root, None, None)\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "binary-tree-summary",
      "title": "Binary Tree Summary",
      "path": "modules/dsa/binary-tree/binary-tree-summary",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# Binary Tree Summary\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nReview traversal patterns, recursion, and BST operations.\n",
      "sources": []
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "introduction-to-binary-trees",
      "title": "Introduction to Binary Trees",
      "path": "modules/dsa/binary-tree/introduction-to-binary-trees",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# Introduction to Binary Trees\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nBinary trees store values with left and right child pointers.\n",
      "sources": []
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "iterative-traversal-of-binary-trees",
      "title": "Iterative Traversal of Binary Trees",
      "path": "modules/dsa/binary-tree/iterative-traversal-of-binary-trees",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# Iterative Traversal of Binary Trees\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nIterative traversals using an explicit stack.\n\n## Function\n\n```python\ndef preorder_traversal(root: Optional[TreeNode]) -> list[int]:\ndef inorder_traversal(root: Optional[TreeNode]) -> list[int]:\ndef postorder_traversal(root: Optional[TreeNode]) -> list[int]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/iterative-traversal-of-binary-trees/python/iterative_traversal_of_binary_trees.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\ndef preorder_traversal(root: Optional[TreeNode]) -> list[int]:\n    if root is None:\n        return []\n    result: list[int] = []\n    stack: list[TreeNode] = [root]\n    while stack:\n        node = stack.pop()\n        result.append(node.val)\n        if node.right:\n            stack.append(node.right)\n        if node.left:\n            stack.append(node.left)\n    return result\n\n\ndef inorder_traversal(root: Optional[TreeNode]) -> list[int]:\n    result: list[int] = []\n    stack: list[TreeNode] = []\n    current = root\n    while current or stack:\n        while current:\n            stack.append(current)\n            current = current.left\n        node = stack.pop()\n        result.append(node.val)\n        current = node.right\n    return result\n\n\ndef postorder_traversal(root: Optional[TreeNode]) -> list[int]:\n    result: list[int] = []\n    stack: list[TreeNode] = []\n    current = root\n    prev: TreeNode | None = None\n    while current or stack:\n        while current:\n            stack.append(current)\n            current = current.left\n        node = stack[-1]\n        if node.right and prev is not node.right:\n            current = node.right\n        else:\n            result.append(node.val)\n            prev = stack.pop()\n    return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "recursive-traversal-of-binary-trees",
      "title": "Recursive Traversal of Binary Trees",
      "path": "modules/dsa/binary-tree/recursive-traversal-of-binary-trees",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# Recursive Traversal of Binary Trees\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nRecursive preorder, inorder, and postorder traversal.\n\n## Function\n\n```python\ndef preorder_traversal(root: Optional[TreeNode]) -> list[int]:\ndef inorder_traversal(root: Optional[TreeNode]) -> list[int]:\ndef postorder_traversal(root: Optional[TreeNode]) -> list[int]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/recursive-traversal-of-binary-trees/python/recursive_traversal_of_binary_trees.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\ndef preorder_traversal(root: Optional[TreeNode]) -> list[int]:\n    result: list[int] = []\n\n    def dfs(node: Optional[TreeNode]) -> None:\n        if node is None:\n            return\n        result.append(node.val)\n        dfs(node.left)\n        dfs(node.right)\n\n    dfs(root)\n    return result\n\n\ndef inorder_traversal(root: Optional[TreeNode]) -> list[int]:\n    result: list[int] = []\n\n    def dfs(node: Optional[TreeNode]) -> None:\n        if node is None:\n            return\n        dfs(node.left)\n        result.append(node.val)\n        dfs(node.right)\n\n    dfs(root)\n    return result\n\n\ndef postorder_traversal(root: Optional[TreeNode]) -> list[int]:\n    result: list[int] = []\n\n    def dfs(node: Optional[TreeNode]) -> None:\n        if node is None:\n            return\n        dfs(node.left)\n        dfs(node.right)\n        result.append(node.val)\n\n    dfs(root)\n    return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "unified-iterative-method-for-binary-trees",
      "title": "Unified Iterative Method for Binary Trees",
      "path": "modules/dsa/binary-tree/unified-iterative-method-for-binary-trees",
      "summary": "> Track: `dsa` | Topic: `binary-tree`",
      "readme": "# Unified Iterative Method for Binary Trees\n\n> Track: `dsa` | Topic: `binary-tree`\n\n## Concept\n\nUnified stack + visited marker traversal for all orders.\n\n## Function\n\n```python\ndef preorder_traversal(root: Optional[TreeNode]) -> list[int]:\ndef inorder_traversal(root: Optional[TreeNode]) -> list[int]:\ndef postorder_traversal(root: Optional[TreeNode]) -> list[int]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/binary-tree/unified-iterative-method-for-binary-trees/python/unified_iterative_method_for_binary_trees.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\n\ndef preorder_traversal(root: Optional[TreeNode]) -> list[int]:\n    result: list[int] = []\n    stack: list[tuple[TreeNode | None, bool]] = [(root, False)]\n    while stack:\n        node, visited = stack.pop()\n        if node is None:\n            continue\n        if visited:\n            result.append(node.val)\n        else:\n            if node.right:\n                stack.append((node.right, False))\n            if node.left:\n                stack.append((node.left, False))\n            stack.append((node, True))\n    return result\n\n\ndef inorder_traversal(root: Optional[TreeNode]) -> list[int]:\n    result: list[int] = []\n    stack: list[tuple[TreeNode | None, bool]] = [(root, False)]\n    while stack:\n        node, visited = stack.pop()\n        if node is None:\n            continue\n        if visited:\n            result.append(node.val)\n        else:\n            if node.right:\n                stack.append((node.right, False))\n            stack.append((node, True))\n            if node.left:\n                stack.append((node.left, False))\n    return result\n\n\ndef postorder_traversal(root: Optional[TreeNode]) -> list[int]:\n    result: list[int] = []\n    stack: list[tuple[TreeNode | None, bool]] = [(root, False)]\n    while stack:\n        node, visited = stack.pop()\n        if node is None:\n            continue\n        if visited:\n            result.append(node.val)\n        else:\n            stack.append((node, True))\n            if node.right:\n                stack.append((node.right, False))\n            if node.left:\n                stack.append((node.left, False))\n    return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "double-pointers",
      "slug": "142-linked-list-cycle-ii",
      "title": "142.Linked List Cycle II",
      "path": "modules/dsa/double-pointers/142-linked-list-cycle-ii",
      "summary": "> Track: `dsa` | Topic: `double-pointers`",
      "readme": "# 142.Linked List Cycle II\n\n> Track: `dsa` | Topic: `double-pointers`\n\n## Concept\n\nUse Floyd's cycle detection to find the meeting point, then walk to the entry.\n\n## Function\n\n```python\nclass Solution:\n    def detectCycle(self, head: Optional[ListNode]) -> Optional[ListNode]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/double-pointers/142-linked-list-cycle-ii/python/problem_142_linked_list_cycle_ii.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\nclass ListNode:\n    def __init__(self, val: int = 0, next: Optional[\"ListNode\"] = None) -> None:\n        self.val = val\n        self.next = next\n\nclass Solution:\n    def detectCycle(self, head: Optional[ListNode]) -> Optional[ListNode]:\n        if head is None:\n            return None\n        slow = head\n        fast = head\n        while fast and fast.next:\n            slow = slow.next\n            fast = fast.next.next\n            if slow is fast:\n                ptr = head\n                while ptr is not slow:\n                    ptr = ptr.next\n                    slow = slow.next\n                return ptr\n        return None\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "double-pointers",
      "slug": "15-3sum",
      "title": "15.3Sum",
      "path": "modules/dsa/double-pointers/15-3sum",
      "summary": "> Track: `dsa` | Topic: `double-pointers`",
      "readme": "# 15.3Sum\n\n> Track: `dsa` | Topic: `double-pointers`\n\n## Concept\n\nSort and fix one number, then move two pointers inward to find pairs.\n\n## Function\n\n```python\nclass Solution:\n    def threeSum(self, nums: List[int]) -> List[List[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/double-pointers/15-3sum/python/problem_15_3sum.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def threeSum(self, nums: List[int]) -> List[List[int]]:\n        nums.sort()\n        result: List[List[int]] = []\n        n = len(nums)\n        for i in range(n):\n            if i > 0 and nums[i] == nums[i - 1]:\n                continue\n            left, right = i + 1, n - 1\n            while left < right:\n                total = nums[i] + nums[left] + nums[right]\n                if total == 0:\n                    result.append([nums[i], nums[left], nums[right]])\n                    left += 1\n                    right -= 1\n                    while left < right and nums[left] == nums[left - 1]:\n                        left += 1\n                    while left < right and nums[right] == nums[right + 1]:\n                        right -= 1\n                elif total < 0:\n                    left += 1\n                else:\n                    right -= 1\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "double-pointers",
      "slug": "151-reverse-words-in-a-string",
      "title": "151.Reverse Words in a String",
      "path": "modules/dsa/double-pointers/151-reverse-words-in-a-string",
      "summary": "> Track: `dsa` | Topic: `double-pointers`",
      "readme": "# 151.Reverse Words in a String\n\n> Track: `dsa` | Topic: `double-pointers`\n\n## Concept\n\nSplit on whitespace, reverse the word order, and join with single spaces.\n\n## Function\n\n```python\nclass Solution:\n    def reverseWords(self, s: str) -> str:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/double-pointers/151-reverse-words-in-a-string/python/problem_151_reverse_words_in_a_string.py",
          "language": "python",
          "content": "class Solution:\n    def reverseWords(self, s: str) -> str:\n        words = s.split()\n        return \" \".join(reversed(words))\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "double-pointers",
      "slug": "160-intersection-of-two-linked-lists",
      "title": "160.Intersection of Two Linked Lists",
      "path": "modules/dsa/double-pointers/160-intersection-of-two-linked-lists",
      "summary": "> Track: `dsa` | Topic: `double-pointers`",
      "readme": "# 160.Intersection of Two Linked Lists\n\n> Track: `dsa` | Topic: `double-pointers`\n\n## Concept\n\nTraverse both lists with pointer switching to align path lengths.\n\n## Function\n\n```python\nclass Solution:\n    def getIntersectionNode(self, headA: Optional[ListNode], headB: Optional[ListNode]) -> Optional[ListNode]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/double-pointers/160-intersection-of-two-linked-lists/python/problem_160_intersection_of_two_linked_lists.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\nclass ListNode:\n    def __init__(self, val: int = 0, next: Optional[\"ListNode\"] = None) -> None:\n        self.val = val\n        self.next = next\n\nclass Solution:\n    def getIntersectionNode(\n        self, headA: Optional[ListNode], headB: Optional[ListNode]\n    ) -> Optional[ListNode]:\n        if headA is None or headB is None:\n            return None\n        pa, pb = headA, headB\n        while pa is not pb:\n            pa = pa.next if pa else headB\n            pb = pb.next if pb else headA\n        return pa\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "double-pointers",
      "slug": "18-4sum",
      "title": "18.4Sum",
      "path": "modules/dsa/double-pointers/18-4sum",
      "summary": "> Track: `dsa` | Topic: `double-pointers`",
      "readme": "# 18.4Sum\n\n> Track: `dsa` | Topic: `double-pointers`\n\n## Concept\n\nSort and fix two numbers, then use two pointers to search for remaining pairs.\n\n## Function\n\n```python\nclass Solution:\n    def fourSum(self, nums: List[int], target: int) -> List[List[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/double-pointers/18-4sum/python/problem_18_4sum.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def fourSum(self, nums: List[int], target: int) -> List[List[int]]:\n        nums.sort()\n        result: List[List[int]] = []\n        n = len(nums)\n        for i in range(n):\n            if i > 0 and nums[i] == nums[i - 1]:\n                continue\n            for j in range(i + 1, n):\n                if j > i + 1 and nums[j] == nums[j - 1]:\n                    continue\n                left, right = j + 1, n - 1\n                while left < right:\n                    total = nums[i] + nums[j] + nums[left] + nums[right]\n                    if total == target:\n                        result.append([nums[i], nums[j], nums[left], nums[right]])\n                        left += 1\n                        right -= 1\n                        while left < right and nums[left] == nums[left - 1]:\n                            left += 1\n                        while left < right and nums[right] == nums[right + 1]:\n                            right -= 1\n                    elif total < target:\n                        left += 1\n                    else:\n                        right -= 1\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "double-pointers",
      "slug": "19-remove-nth-node-from-end-of-list",
      "title": "19.Remove Nth Node From End of List",
      "path": "modules/dsa/double-pointers/19-remove-nth-node-from-end-of-list",
      "summary": "> Track: `dsa` | Topic: `double-pointers`",
      "readme": "# 19.Remove Nth Node From End of List\n\n> Track: `dsa` | Topic: `double-pointers`\n\n## Concept\n\nUse two pointers separated by n nodes so the slow pointer stops before the removal.\n\n## Function\n\n```python\nclass Solution:\n    def removeNthFromEnd(self, head: Optional[ListNode], n: int) -> Optional[ListNode]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/double-pointers/19-remove-nth-node-from-end-of-list/python/problem_19_remove_nth_node_from_end_of_list.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\nclass ListNode:\n    def __init__(self, val: int = 0, next: Optional[\"ListNode\"] = None) -> None:\n        self.val = val\n        self.next = next\n\nclass Solution:\n    def removeNthFromEnd(self, head: Optional[ListNode], n: int) -> Optional[ListNode]:\n        dummy = ListNode(0, head)\n        fast = dummy\n        slow = dummy\n        for _ in range(n + 1):\n            fast = fast.next\n        while fast:\n            fast = fast.next\n            slow = slow.next\n        slow.next = slow.next.next\n        return dummy.next\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "double-pointers",
      "slug": "206-reverse-linked-list",
      "title": "206.Reverse Linked List",
      "path": "modules/dsa/double-pointers/206-reverse-linked-list",
      "summary": "> Track: `dsa` | Topic: `double-pointers`",
      "readme": "# 206.Reverse Linked List\n\n> Track: `dsa` | Topic: `double-pointers`\n\n## Concept\n\nIteratively reverse next pointers with a moving prev/current pair.\n\n## Function\n\n```python\nclass Solution:\n    def reverseList(self, head: Optional[ListNode]) -> Optional[ListNode]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/double-pointers/206-reverse-linked-list/python/problem_206_reverse_linked_list.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\nclass ListNode:\n    def __init__(self, val: int = 0, next: Optional[\"ListNode\"] = None) -> None:\n        self.val = val\n        self.next = next\n\nclass Solution:\n    def reverseList(self, head: Optional[ListNode]) -> Optional[ListNode]:\n        prev = None\n        current = head\n        while current:\n            nxt = current.next\n            current.next = prev\n            prev = current\n            current = nxt\n        return prev\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "double-pointers",
      "slug": "27-remove-element",
      "title": "27.Remove Element",
      "path": "modules/dsa/double-pointers/27-remove-element",
      "summary": "> Track: `dsa` | Topic: `double-pointers`",
      "readme": "# 27.Remove Element\n\n> Track: `dsa` | Topic: `double-pointers`\n\n## Concept\n\nUse two pointers to overwrite values not equal to the target and return the new length.\n\n## Function\n\n```python\nclass Solution:\n    def removeElement(self, nums: List[int], val: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/double-pointers/27-remove-element/python/problem_27_remove_element.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def removeElement(self, nums: List[int], val: int) -> int:\n        slow = 0\n        for fast in range(len(nums)):\n            if nums[fast] != val:\n                nums[slow] = nums[fast]\n                slow += 1\n        return slow\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "double-pointers",
      "slug": "344-reverse-string",
      "title": "344.Reverse String",
      "path": "modules/dsa/double-pointers/344-reverse-string",
      "summary": "> Track: `dsa` | Topic: `double-pointers`",
      "readme": "# 344.Reverse String\n\n> Track: `dsa` | Topic: `double-pointers`\n\n## Concept\n\nSwap characters from both ends until the pointers meet.\n\n## Function\n\n```python\nclass Solution:\n    def reverseString(self, s: List[str]) -> None:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/double-pointers/344-reverse-string/python/problem_344_reverse_string.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def reverseString(self, s: List[str]) -> None:\n        left, right = 0, len(s) - 1\n        while left < right:\n            s[left], s[right] = s[right], s[left]\n            left += 1\n            right -= 1\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "double-pointers",
      "slug": "double-pointers-summary",
      "title": "Double Pointers Summary",
      "path": "modules/dsa/double-pointers/double-pointers-summary",
      "summary": "> Track: `dsa` | Topic: `double-pointers`",
      "readme": "# Double Pointers Summary\n\n> Track: `dsa` | Topic: `double-pointers`\n\n## Concept\n\nReview two-pointer patterns: inward scan, fast/slow, and opposite-direction pointers.\n",
      "sources": []
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "0-1-knapsack-theory-part-1",
      "title": "0-1 Knapsack Theory (Part 1)",
      "path": "modules/dsa/dynamic-programming/0-1-knapsack-theory-part-1",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 0-1 Knapsack Theory (Part 1)\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nClassic 2D DP for 0-1 knapsack.\n\n## Function\n\n```python\nclass Solution:\n    def knapSack(self, weights: List[int], values: List[int], capacity: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/0-1-knapsack-theory-part-1/python/problem_0_1_knapsack_theory_part_1.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def knapSack(self, weights: List[int], values: List[int], capacity: int) -> int:\n        n = len(weights)\n        dp = [[0] * (capacity + 1) for _ in range(n + 1)]\n        for i in range(1, n + 1):\n            w = weights[i - 1]\n            v = values[i - 1]\n            for c in range(capacity + 1):\n                dp[i][c] = dp[i - 1][c]\n                if c >= w:\n                    dp[i][c] = max(dp[i][c], dp[i - 1][c - w] + v)\n        return dp[n][capacity]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "0-1-knapsack-theory-part-2",
      "title": "0-1 Knapsack Theory (Part 2)",
      "path": "modules/dsa/dynamic-programming/0-1-knapsack-theory-part-2",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 0-1 Knapsack Theory (Part 2)\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\n1D optimized DP for 0-1 knapsack.\n\n## Function\n\n```python\nclass Solution:\n    def knapSack(self, weights: List[int], values: List[int], capacity: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/0-1-knapsack-theory-part-2/python/problem_0_1_knapsack_theory_part_2.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def knapSack(self, weights: List[int], values: List[int], capacity: int) -> int:\n        dp = [0] * (capacity + 1)\n        for w, v in zip(weights, values):\n            for c in range(capacity, w - 1, -1):\n                dp[c] = max(dp[c], dp[c - w] + v)\n        return dp[capacity]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "1035-uncrossed-lines",
      "title": "1035.Uncrossed Lines",
      "path": "modules/dsa/dynamic-programming/1035-uncrossed-lines",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 1035.Uncrossed Lines\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nLCS DP on two sequences to maximize non-crossing lines.\n\n## Function\n\n```python\nclass Solution:\n    def maxUncrossedLines(self, nums1: List[int], nums2: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/1035-uncrossed-lines/python/problem_1035_uncrossed_lines.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def maxUncrossedLines(self, nums1: List[int], nums2: List[int]) -> int:\n        m, n = len(nums1), len(nums2)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if nums1[i - 1] == nums2[j - 1]:\n                    dp[i][j] = dp[i - 1][j - 1] + 1\n                else:\n                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n        return dp[m][n]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "1049-last-stone-weight-ii",
      "title": "1049.Last Stone Weight II",
      "path": "modules/dsa/dynamic-programming/1049-last-stone-weight-ii",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 1049.Last Stone Weight II\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nKnapsack DP to minimize difference of two piles.\n\n## Function\n\n```python\nclass Solution:\n    def lastStoneWeightII(self, stones: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/1049-last-stone-weight-ii/python/problem_1049_last_stone_weight_ii.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def lastStoneWeightII(self, stones: List[int]) -> int:\n        total = sum(stones)\n        target = total // 2\n        dp = [0] * (target + 1)\n        for stone in stones:\n            for t in range(target, stone - 1, -1):\n                dp[t] = max(dp[t], dp[t - stone] + stone)\n        return total - 2 * dp[target]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "1143-longest-common-subsequence",
      "title": "1143.Longest Common Subsequence",
      "path": "modules/dsa/dynamic-programming/1143-longest-common-subsequence",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 1143.Longest Common Subsequence\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP for LCS length between two strings.\n\n## Function\n\n```python\nclass Solution:\n    def longestCommonSubsequence(self, text1: str, text2: str) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/1143-longest-common-subsequence/python/problem_1143_longest_common_subsequence.py",
          "language": "python",
          "content": "class Solution:\n    def longestCommonSubsequence(self, text1: str, text2: str) -> int:\n        m, n = len(text1), len(text2)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if text1[i - 1] == text2[j - 1]:\n                    dp[i][j] = dp[i - 1][j - 1] + 1\n                else:\n                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n        return dp[m][n]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "115-distinct-subsequences",
      "title": "115.Distinct Subsequences",
      "path": "modules/dsa/dynamic-programming/115-distinct-subsequences",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 115.Distinct Subsequences\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP counting ways to form t from s.\n\n## Function\n\n```python\nclass Solution:\n    def numDistinct(self, s: str, t: str) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/115-distinct-subsequences/python/problem_115_distinct_subsequences.py",
          "language": "python",
          "content": "class Solution:\n    def numDistinct(self, s: str, t: str) -> int:\n        n = len(t)\n        dp = [0] * (n + 1)\n        dp[0] = 1\n        for ch in s:\n            for j in range(n, 0, -1):\n                if ch == t[j - 1]:\n                    dp[j] += dp[j - 1]\n        return dp[n]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "121-best-time-to-buy-and-sell-stock",
      "title": "121.Best Time to Buy and Sell Stock",
      "path": "modules/dsa/dynamic-programming/121-best-time-to-buy-and-sell-stock",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 121.Best Time to Buy and Sell Stock\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nTrack minimum price to maximize single-transaction profit.\n\n## Function\n\n```python\nclass Solution:\n    def maxProfit(self, prices: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/121-best-time-to-buy-and-sell-stock/python/problem_121_best_time_to_buy_and_sell_stock.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def maxProfit(self, prices: List[int]) -> int:\n        min_price = float(\"inf\")\n        best = 0\n        for price in prices:\n            min_price = min(min_price, price)\n            best = max(best, price - min_price)\n        return best\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "122-best-time-to-buy-and-sell-stock-ii-dp",
      "title": "122.Best Time to Buy and Sell Stock II (DP)",
      "path": "modules/dsa/dynamic-programming/122-best-time-to-buy-and-sell-stock-ii-dp",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 122.Best Time to Buy and Sell Stock II (DP)\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP with hold/cash states for unlimited transactions.\n\n## Function\n\n```python\nclass Solution:\n    def maxProfit(self, prices: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/122-best-time-to-buy-and-sell-stock-ii-dp/python/problem_122_best_time_to_buy_and_sell_stock_ii_dp.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def maxProfit(self, prices: List[int]) -> int:\n        if not prices:\n            return 0\n        cash = 0\n        hold = -prices[0]\n        for price in prices[1:]:\n            prev_cash = cash\n            cash = max(cash, hold + price)\n            hold = max(hold, prev_cash - price)\n        return cash\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "123-best-time-to-buy-and-sell-stock-iii",
      "title": "123.Best Time to Buy and Sell Stock III",
      "path": "modules/dsa/dynamic-programming/123-best-time-to-buy-and-sell-stock-iii",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 123.Best Time to Buy and Sell Stock III\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP with up to two transactions.\n\n## Function\n\n```python\nclass Solution:\n    def maxProfit(self, prices: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/123-best-time-to-buy-and-sell-stock-iii/python/problem_123_best_time_to_buy_and_sell_stock_iii.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def maxProfit(self, prices: List[int]) -> int:\n        buy1 = float(\"-inf\")\n        sell1 = 0\n        buy2 = float(\"-inf\")\n        sell2 = 0\n        for price in prices:\n            buy1 = max(buy1, -price)\n            sell1 = max(sell1, buy1 + price)\n            buy2 = max(buy2, sell1 - price)\n            sell2 = max(sell2, buy2 + price)\n        return sell2\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "139-word-break",
      "title": "139.Word Break",
      "path": "modules/dsa/dynamic-programming/139-word-break",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 139.Word Break\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP over prefixes to test valid word breaks.\n\n## Function\n\n```python\nclass Solution:\n    def wordBreak(self, s: str, wordDict: list[str]) -> bool:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/139-word-break/python/problem_139_word_break.py",
          "language": "python",
          "content": "class Solution:\n    def wordBreak(self, s: str, wordDict: list[str]) -> bool:\n        words = set(wordDict)\n        dp = [False] * (len(s) + 1)\n        dp[0] = True\n        for i in range(1, len(s) + 1):\n            for j in range(i):\n                if dp[j] and s[j:i] in words:\n                    dp[i] = True\n                    break\n        return dp[-1]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "188-best-time-to-buy-and-sell-stock-iv",
      "title": "188.Best Time to Buy and Sell Stock IV",
      "path": "modules/dsa/dynamic-programming/188-best-time-to-buy-and-sell-stock-iv",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 188.Best Time to Buy and Sell Stock IV\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP with k transactions using hold/cash arrays.\n\n## Function\n\n```python\nclass Solution:\n    def maxProfit(self, k: int, prices: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/188-best-time-to-buy-and-sell-stock-iv/python/problem_188_best_time_to_buy_and_sell_stock_iv.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def maxProfit(self, k: int, prices: List[int]) -> int:\n        if not prices or k == 0:\n            return 0\n        n = len(prices)\n        if k >= n // 2:\n            profit = 0\n            for i in range(1, n):\n                if prices[i] > prices[i - 1]:\n                    profit += prices[i] - prices[i - 1]\n            return profit\n        buy = [float(\"-inf\")] * k\n        sell = [0] * k\n        for price in prices:\n            prev_buy = buy[:]\n            prev_sell = sell[:]\n            for i in range(k):\n                buy[i] = max(prev_buy[i], (prev_sell[i - 1] if i > 0 else 0) - price)\n                sell[i] = max(prev_sell[i], buy[i] + price)\n        return sell[-1]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "198-house-robber",
      "title": "198.House Robber",
      "path": "modules/dsa/dynamic-programming/198-house-robber",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 198.House Robber\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP for max sum without adjacent houses.\n\n## Function\n\n```python\nclass Solution:\n    def rob(self, nums: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/198-house-robber/python/problem_198_house_robber.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def rob(self, nums: List[int]) -> int:\n        prev2 = 0\n        prev1 = 0\n        for value in nums:\n            current = max(prev1, prev2 + value)\n            prev2, prev1 = prev1, current\n        return prev1\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "213-house-robber-ii",
      "title": "213.House Robber II",
      "path": "modules/dsa/dynamic-programming/213-house-robber-ii",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 213.House Robber II\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nSplit the circle into two linear DP cases.\n\n## Function\n\n```python\nclass Solution:\n    def rob(self, nums: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/213-house-robber-ii/python/problem_213_house_robber_ii.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def rob(self, nums: List[int]) -> int:\n        if len(nums) == 1:\n            return nums[0]\n\n        def rob_line(values: List[int]) -> int:\n            prev2 = 0\n            prev1 = 0\n            for value in values:\n                current = max(prev1, prev2 + value)\n                prev2, prev1 = prev1, current\n            return prev1\n\n        return max(rob_line(nums[:-1]), rob_line(nums[1:]))\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "279-perfect-squares",
      "title": "279.Perfect Squares",
      "path": "modules/dsa/dynamic-programming/279-perfect-squares",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 279.Perfect Squares\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP for minimum count of squares to sum to n.\n\n## Function\n\n```python\nclass Solution:\n    def numSquares(self, n: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/279-perfect-squares/python/problem_279_perfect_squares.py",
          "language": "python",
          "content": "class Solution:\n    def numSquares(self, n: int) -> int:\n        dp = [0] + [n] * n\n        for i in range(1, n + 1):\n            j = 1\n            while j * j <= i:\n                dp[i] = min(dp[i], dp[i - j * j] + 1)\n                j += 1\n        return dp[n]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "300-longest-increasing-subsequence",
      "title": "300.Longest Increasing Subsequence",
      "path": "modules/dsa/dynamic-programming/300-longest-increasing-subsequence",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 300.Longest Increasing Subsequence\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nO(n^2) DP where dp[i] is LIS ending at i.\n\n## Function\n\n```python\nclass Solution:\n    def lengthOfLIS(self, nums: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/300-longest-increasing-subsequence/python/problem_300_longest_increasing_subsequence.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def lengthOfLIS(self, nums: List[int]) -> int:\n        if not nums:\n            return 0\n        dp = [1] * len(nums)\n        for i in range(len(nums)):\n            for j in range(i):\n                if nums[i] > nums[j]:\n                    dp[i] = max(dp[i], dp[j] + 1)\n        return max(dp)\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "309-best-time-to-buy-and-sell-stock-with-cooldown",
      "title": "309.Best Time to Buy and Sell Stock with Cooldown",
      "path": "modules/dsa/dynamic-programming/309-best-time-to-buy-and-sell-stock-with-cooldown",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 309.Best Time to Buy and Sell Stock with Cooldown\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP with a cooldown state after selling.\n\n## Function\n\n```python\nclass Solution:\n    def maxProfit(self, prices: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/309-best-time-to-buy-and-sell-stock-with-cooldown/python/problem_309_best_time_to_buy_and_sell_stock_with_cooldown.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def maxProfit(self, prices: List[int]) -> int:\n        if not prices:\n            return 0\n        hold = -prices[0]\n        sold = 0\n        rest = 0\n        for price in prices[1:]:\n            prev_hold = hold\n            prev_sold = sold\n            prev_rest = rest\n            hold = max(prev_hold, prev_rest - price)\n            sold = prev_hold + price\n            rest = max(prev_rest, prev_sold)\n        return max(sold, rest)\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "322-coin-change",
      "title": "322.Coin Change",
      "path": "modules/dsa/dynamic-programming/322-coin-change",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 322.Coin Change\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP for minimum coins to reach each amount.\n\n## Function\n\n```python\nclass Solution:\n    def coinChange(self, coins: List[int], amount: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/322-coin-change/python/problem_322_coin_change.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def coinChange(self, coins: List[int], amount: int) -> int:\n        max_value = amount + 1\n        dp = [max_value] * (amount + 1)\n        dp[0] = 0\n        for coin in coins:\n            for total in range(coin, amount + 1):\n                dp[total] = min(dp[total], dp[total - coin] + 1)\n        return dp[amount] if dp[amount] != max_value else -1\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "337-house-robber-iii",
      "title": "337.House Robber III",
      "path": "modules/dsa/dynamic-programming/337-house-robber-iii",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 337.House Robber III\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nTree DP with rob/skip states per node.\n\n## Function\n\n```python\nclass Solution:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/337-house-robber-iii/python/problem_337_house_robber_iii.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\nclass Solution:\n    def rob(self, root: Optional[TreeNode]) -> int:\n        def dfs(node: Optional[TreeNode]) -> tuple[int, int]:\n            if node is None:\n                return (0, 0)\n            left_rob, left_skip = dfs(node.left)\n            right_rob, right_skip = dfs(node.right)\n            rob_this = node.val + left_skip + right_skip\n            skip_this = max(left_rob, left_skip) + max(right_rob, right_skip)\n            return (rob_this, skip_this)\n\n        rob_root, skip_root = dfs(root)\n        return max(rob_root, skip_root)\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "343-integer-break",
      "title": "343.Integer Break",
      "path": "modules/dsa/dynamic-programming/343-integer-break",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 343.Integer Break\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP to maximize product by splitting integer.\n\n## Function\n\n```python\nclass Solution:\n    def integerBreak(self, n: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/343-integer-break/python/problem_343_integer_break.py",
          "language": "python",
          "content": "class Solution:\n    def integerBreak(self, n: int) -> int:\n        dp = [0] * (n + 1)\n        dp[2] = 1\n        for i in range(3, n + 1):\n            for j in range(1, i):\n                dp[i] = max(dp[i], max(j * (i - j), j * dp[i - j]))\n        return dp[n]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "377-combination-sum-iv",
      "title": "377.Combination Sum IV",
      "path": "modules/dsa/dynamic-programming/377-combination-sum-iv",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 377.Combination Sum IV\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP counting ordered combinations for a target sum.\n\n## Function\n\n```python\nclass Solution:\n    def combinationSum4(self, nums: List[int], target: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/377-combination-sum-iv/python/problem_377_combination_sum_iv.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def combinationSum4(self, nums: List[int], target: int) -> int:\n        dp = [0] * (target + 1)\n        dp[0] = 1\n        for total in range(1, target + 1):\n            for num in nums:\n                if total >= num:\n                    dp[total] += dp[total - num]\n        return dp[target]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "392-is-subsequence",
      "title": "392.Is Subsequence",
      "path": "modules/dsa/dynamic-programming/392-is-subsequence",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 392.Is Subsequence\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nGreedy scan to match s in order within t.\n\n## Function\n\n```python\nclass Solution:\n    def isSubsequence(self, s: str, t: str) -> bool:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/392-is-subsequence/python/problem_392_is_subsequence.py",
          "language": "python",
          "content": "class Solution:\n    def isSubsequence(self, s: str, t: str) -> bool:\n        i = 0\n        for ch in t:\n            if i < len(s) and s[i] == ch:\n                i += 1\n        return i == len(s)\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "416-partition-equal-subset-sum",
      "title": "416.Partition Equal Subset Sum",
      "path": "modules/dsa/dynamic-programming/416-partition-equal-subset-sum",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 416.Partition Equal Subset Sum\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nSubset-sum DP to reach half of total.\n\n## Function\n\n```python\nclass Solution:\n    def canPartition(self, nums: List[int]) -> bool:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/416-partition-equal-subset-sum/python/problem_416_partition_equal_subset_sum.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def canPartition(self, nums: List[int]) -> bool:\n        total = sum(nums)\n        if total % 2 == 1:\n            return False\n        target = total // 2\n        dp = [False] * (target + 1)\n        dp[0] = True\n        for num in nums:\n            for t in range(target, num - 1, -1):\n                dp[t] = dp[t] or dp[t - num]\n        return dp[target]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "474-ones-and-zeroes",
      "title": "474.Ones and Zeroes",
      "path": "modules/dsa/dynamic-programming/474-ones-and-zeroes",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 474.Ones and Zeroes\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\n2D 0-1 knapsack DP on zeros and ones.\n\n## Function\n\n```python\nclass Solution:\n    def findMaxForm(self, strs: List[str], m: int, n: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/474-ones-and-zeroes/python/problem_474_ones_and_zeroes.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def findMaxForm(self, strs: List[str], m: int, n: int) -> int:\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n        for s in strs:\n            zeros = s.count(\"0\")\n            ones = s.count(\"1\")\n            for i in range(m, zeros - 1, -1):\n                for j in range(n, ones - 1, -1):\n                    dp[i][j] = max(dp[i][j], dp[i - zeros][j - ones] + 1)\n        return dp[m][n]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "494-target-sum",
      "title": "494.Target Sum",
      "path": "modules/dsa/dynamic-programming/494-target-sum",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 494.Target Sum\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nTransform to subset-sum counting DP.\n\n## Function\n\n```python\nclass Solution:\n    def findTargetSumWays(self, nums: List[int], target: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/494-target-sum/python/problem_494_target_sum.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def findTargetSumWays(self, nums: List[int], target: int) -> int:\n        total = sum(nums)\n        if total < abs(target) or (total + target) % 2 == 1:\n            return 0\n        subset = (total + target) // 2\n        dp = [0] * (subset + 1)\n        dp[0] = 1\n        for num in nums:\n            for s in range(subset, num - 1, -1):\n                dp[s] += dp[s - num]\n        return dp[subset]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "509-fibonacci-number",
      "title": "509.Fibonacci Number",
      "path": "modules/dsa/dynamic-programming/509-fibonacci-number",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 509.Fibonacci Number\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nBottom-up DP for Fibonacci numbers.\n\n## Function\n\n```python\nclass Solution:\n    def fib(self, n: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/509-fibonacci-number/python/problem_509_fibonacci_number.py",
          "language": "python",
          "content": "class Solution:\n    def fib(self, n: int) -> int:\n        if n <= 1:\n            return n\n        a, b = 0, 1\n        for _ in range(2, n + 1):\n            a, b = b, a + b\n        return b\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "516-longest-palindromic-subsequence",
      "title": "516.Longest Palindromic Subsequence",
      "path": "modules/dsa/dynamic-programming/516-longest-palindromic-subsequence",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 516.Longest Palindromic Subsequence\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nInterval DP for longest palindromic subsequence.\n\n## Function\n\n```python\nclass Solution:\n    def longestPalindromeSubseq(self, s: str) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/516-longest-palindromic-subsequence/python/problem_516_longest_palindromic_subsequence.py",
          "language": "python",
          "content": "class Solution:\n    def longestPalindromeSubseq(self, s: str) -> int:\n        n = len(s)\n        dp = [[0] * n for _ in range(n)]\n        for i in range(n - 1, -1, -1):\n            dp[i][i] = 1\n            for j in range(i + 1, n):\n                if s[i] == s[j]:\n                    dp[i][j] = dp[i + 1][j - 1] + 2\n                else:\n                    dp[i][j] = max(dp[i + 1][j], dp[i][j - 1])\n        return dp[0][n - 1] if s else 0\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "518-coin-change-ii",
      "title": "518.Coin Change II",
      "path": "modules/dsa/dynamic-programming/518-coin-change-ii",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 518.Coin Change II\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP counting combinations with unlimited coins.\n\n## Function\n\n```python\nclass Solution:\n    def change(self, amount: int, coins: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/518-coin-change-ii/python/problem_518_coin_change_ii.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def change(self, amount: int, coins: List[int]) -> int:\n        dp = [0] * (amount + 1)\n        dp[0] = 1\n        for coin in coins:\n            for total in range(coin, amount + 1):\n                dp[total] += dp[total - coin]\n        return dp[amount]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "53-maximum-subarray-dp",
      "title": "53.Maximum Subarray (DP)",
      "path": "modules/dsa/dynamic-programming/53-maximum-subarray-dp",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 53.Maximum Subarray (DP)\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nKadane-style DP for best subarray ending at each index.\n\n## Function\n\n```python\nclass Solution:\n    def maxSubArray(self, nums: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/53-maximum-subarray-dp/python/problem_53_maximum_subarray_dp.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def maxSubArray(self, nums: List[int]) -> int:\n        current = best = nums[0]\n        for value in nums[1:]:\n            current = max(value, current + value)\n            best = max(best, current)\n        return best\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "583-delete-operation-for-two-strings",
      "title": "583.Delete Operation for Two Strings",
      "path": "modules/dsa/dynamic-programming/583-delete-operation-for-two-strings",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 583.Delete Operation for Two Strings\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP based on LCS to minimize deletions.\n\n## Function\n\n```python\nclass Solution:\n    def minDistance(self, word1: str, word2: str) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/583-delete-operation-for-two-strings/python/problem_583_delete_operation_for_two_strings.py",
          "language": "python",
          "content": "class Solution:\n    def minDistance(self, word1: str, word2: str) -> int:\n        m, n = len(word1), len(word2)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if word1[i - 1] == word2[j - 1]:\n                    dp[i][j] = dp[i - 1][j - 1] + 1\n                else:\n                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n        lcs = dp[m][n]\n        return m + n - 2 * lcs\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "62-unique-paths",
      "title": "62.Unique Paths",
      "path": "modules/dsa/dynamic-programming/62-unique-paths",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 62.Unique Paths\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nGrid DP counting paths from top-left to bottom-right.\n\n## Function\n\n```python\nclass Solution:\n    def uniquePaths(self, m: int, n: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/62-unique-paths/python/problem_62_unique_paths.py",
          "language": "python",
          "content": "class Solution:\n    def uniquePaths(self, m: int, n: int) -> int:\n        dp = [1] * n\n        for _ in range(1, m):\n            for j in range(1, n):\n                dp[j] += dp[j - 1]\n        return dp[-1]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "63-unique-paths-ii",
      "title": "63.Unique Paths II",
      "path": "modules/dsa/dynamic-programming/63-unique-paths-ii",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 63.Unique Paths II\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nGrid DP counting paths while skipping obstacles.\n\n## Function\n\n```python\nclass Solution:\n    def uniquePathsWithObstacles(self, obstacleGrid: List[List[int]]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/63-unique-paths-ii/python/problem_63_unique_paths_ii.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def uniquePathsWithObstacles(self, obstacleGrid: List[List[int]]) -> int:\n        if not obstacleGrid or obstacleGrid[0][0] == 1:\n            return 0\n        m, n = len(obstacleGrid), len(obstacleGrid[0])\n        dp = [0] * n\n        dp[0] = 1\n        for i in range(m):\n            for j in range(n):\n                if obstacleGrid[i][j] == 1:\n                    dp[j] = 0\n                elif j > 0:\n                    dp[j] += dp[j - 1]\n        return dp[-1]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "647-palindromic-substrings",
      "title": "647.Palindromic Substrings",
      "path": "modules/dsa/dynamic-programming/647-palindromic-substrings",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 647.Palindromic Substrings\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP on substrings to count palindromes.\n\n## Function\n\n```python\nclass Solution:\n    def countSubstrings(self, s: str) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/647-palindromic-substrings/python/problem_647_palindromic_substrings.py",
          "language": "python",
          "content": "class Solution:\n    def countSubstrings(self, s: str) -> int:\n        def expand(left: int, right: int) -> int:\n            count = 0\n            while left >= 0 and right < len(s) and s[left] == s[right]:\n                count += 1\n                left -= 1\n                right += 1\n            return count\n\n        total = 0\n        for i in range(len(s)):\n            total += expand(i, i)\n            total += expand(i, i + 1)\n        return total\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "674-longest-continuous-increasing-subsequence",
      "title": "674.Longest Continuous Increasing Subsequence",
      "path": "modules/dsa/dynamic-programming/674-longest-continuous-increasing-subsequence",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 674.Longest Continuous Increasing Subsequence\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP for longest increasing run ending at each index.\n\n## Function\n\n```python\nclass Solution:\n    def findLengthOfLCIS(self, nums: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/674-longest-continuous-increasing-subsequence/python/problem_674_longest_continuous_increasing_subsequence.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def findLengthOfLCIS(self, nums: List[int]) -> int:\n        if not nums:\n            return 0\n        best = current = 1\n        for i in range(1, len(nums)):\n            if nums[i] > nums[i - 1]:\n                current += 1\n            else:\n                current = 1\n            best = max(best, current)\n        return best\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "70-climbing-stairs",
      "title": "70.Climbing Stairs",
      "path": "modules/dsa/dynamic-programming/70-climbing-stairs",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 70.Climbing Stairs\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP on steps: f(n) = f(n-1) + f(n-2).\n\n## Function\n\n```python\nclass Solution:\n    def climbStairs(self, n: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/70-climbing-stairs/python/problem_70_climbing_stairs.py",
          "language": "python",
          "content": "class Solution:\n    def climbStairs(self, n: int) -> int:\n        if n <= 2:\n            return n\n        a, b = 1, 2\n        for _ in range(3, n + 1):\n            a, b = b, a + b\n        return b\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "714-best-time-to-buy-and-sell-stock-with-transaction-fee",
      "title": "714.Best Time to Buy and Sell Stock with Transaction Fee",
      "path": "modules/dsa/dynamic-programming/714-best-time-to-buy-and-sell-stock-with-transaction-fee",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 714.Best Time to Buy and Sell Stock with Transaction Fee\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP with a fee applied on each sell.\n\n## Function\n\n```python\nclass Solution:\n    def maxProfit(self, prices: List[int], fee: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/714-best-time-to-buy-and-sell-stock-with-transaction-fee/python/problem_714_best_time_to_buy_and_sell_stock_with_transaction_fee.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def maxProfit(self, prices: List[int], fee: int) -> int:\n        cash = 0\n        hold = -prices[0]\n        for price in prices[1:]:\n            prev_cash = cash\n            cash = max(cash, hold + price - fee)\n            hold = max(hold, prev_cash - price)\n        return cash\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "718-maximum-length-of-repeated-subarray",
      "title": "718.Maximum Length of Repeated Subarray",
      "path": "modules/dsa/dynamic-programming/718-maximum-length-of-repeated-subarray",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 718.Maximum Length of Repeated Subarray\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP on ending positions to find longest common subarray.\n\n## Function\n\n```python\nclass Solution:\n    def findLength(self, nums1: List[int], nums2: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/718-maximum-length-of-repeated-subarray/python/problem_718_maximum_length_of_repeated_subarray.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def findLength(self, nums1: List[int], nums2: List[int]) -> int:\n        m, n = len(nums1), len(nums2)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n        best = 0\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if nums1[i - 1] == nums2[j - 1]:\n                    dp[i][j] = dp[i - 1][j - 1] + 1\n                    best = max(best, dp[i][j])\n        return best\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "72-edit-distance",
      "title": "72.Edit Distance",
      "path": "modules/dsa/dynamic-programming/72-edit-distance",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 72.Edit Distance\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP with insert/delete/replace to compute edit distance.\n\n## Function\n\n```python\nclass Solution:\n    def minDistance(self, word1: str, word2: str) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/72-edit-distance/python/problem_72_edit_distance.py",
          "language": "python",
          "content": "class Solution:\n    def minDistance(self, word1: str, word2: str) -> int:\n        m, n = len(word1), len(word2)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n        for i in range(m + 1):\n            dp[i][0] = i\n        for j in range(n + 1):\n            dp[0][j] = j\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if word1[i - 1] == word2[j - 1]:\n                    dp[i][j] = dp[i - 1][j - 1]\n                else:\n                    dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n        return dp[m][n]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "746-min-cost-climbing-stairs",
      "title": "746.Min Cost Climbing Stairs",
      "path": "modules/dsa/dynamic-programming/746-min-cost-climbing-stairs",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 746.Min Cost Climbing Stairs\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP for minimum cost to reach each step.\n\n## Function\n\n```python\nclass Solution:\n    def minCostClimbingStairs(self, cost: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/746-min-cost-climbing-stairs/python/problem_746_min_cost_climbing_stairs.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def minCostClimbingStairs(self, cost: List[int]) -> int:\n        prev2 = 0\n        prev1 = 0\n        for i in range(2, len(cost) + 1):\n            current = min(prev1 + cost[i - 1], prev2 + cost[i - 2])\n            prev2, prev1 = prev1, current\n        return prev1\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "96-unique-binary-search-trees",
      "title": "96.Unique Binary Search Trees",
      "path": "modules/dsa/dynamic-programming/96-unique-binary-search-trees",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# 96.Unique Binary Search Trees\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nCatalan DP counting BST structures.\n\n## Function\n\n```python\nclass Solution:\n    def numTrees(self, n: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/96-unique-binary-search-trees/python/problem_96_unique_binary_search_trees.py",
          "language": "python",
          "content": "class Solution:\n    def numTrees(self, n: int) -> int:\n        dp = [0] * (n + 1)\n        dp[0] = 1\n        dp[1] = 1\n        for i in range(2, n + 1):\n            total = 0\n            for root in range(1, i + 1):\n                total += dp[root - 1] * dp[i - root]\n            dp[i] = total\n        return dp[n]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "climbing-stairs-advanced",
      "title": "Climbing Stairs (Advanced)",
      "path": "modules/dsa/dynamic-programming/climbing-stairs-advanced",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# Climbing Stairs (Advanced)\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDP counting ways with variable step sizes.\n\n## Function\n\n```python\nclass Solution:\n    def climbStairsAdvanced(self, n: int, steps: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/climbing-stairs-advanced/python/climbing_stairs_advanced.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def climbStairsAdvanced(self, n: int, steps: List[int]) -> int:\n        dp = [0] * (n + 1)\n        dp[0] = 1\n        for i in range(1, n + 1):\n            total = 0\n            for step in steps:\n                if i >= step:\n                    total += dp[i - step]\n            dp[i] = total\n        return dp[n]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "complete-knapsack-theory",
      "title": "Complete Knapsack Theory",
      "path": "modules/dsa/dynamic-programming/complete-knapsack-theory",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# Complete Knapsack Theory\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\n1D DP with forward iteration for unlimited items.\n\n## Function\n\n```python\nclass Solution:\n    def completeKnapsack(self, weights: List[int], values: List[int], capacity: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/complete-knapsack-theory/python/complete_knapsack_theory.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def completeKnapsack(self, weights: List[int], values: List[int], capacity: int) -> int:\n        dp = [0] * (capacity + 1)\n        for w, v in zip(weights, values):\n            for c in range(w, capacity + 1):\n                dp[c] = max(dp[c], dp[c - w] + v)\n        return dp[capacity]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "dynamic-programming-summary",
      "title": "Dynamic Programming Summary",
      "path": "modules/dsa/dynamic-programming/dynamic-programming-summary",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# Dynamic Programming Summary\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nReview DP patterns: state, transition, initialization, and optimization.\n",
      "sources": []
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "edit-distance-summary",
      "title": "Edit Distance Summary",
      "path": "modules/dsa/dynamic-programming/edit-distance-summary",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# Edit Distance Summary\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nSummarize edit-distance style DP for string alignment problems.\n",
      "sources": []
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "introduction-to-dynamic-programming",
      "title": "Introduction to Dynamic Programming",
      "path": "modules/dsa/dynamic-programming/introduction-to-dynamic-programming",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# Introduction to Dynamic Programming\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nDefine DP with overlapping subproblems, optimal substructure, and state transitions.\n",
      "sources": []
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "knapsack-summary",
      "title": "Knapsack Summary",
      "path": "modules/dsa/dynamic-programming/knapsack-summary",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# Knapsack Summary\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nSummarize knapsack variants and how transition direction changes.\n",
      "sources": []
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "multiple-knapsack-theory",
      "title": "Multiple Knapsack Theory",
      "path": "modules/dsa/dynamic-programming/multiple-knapsack-theory",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# Multiple Knapsack Theory\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nBounded knapsack by expanding counts then 0-1 DP.\n\n## Function\n\n```python\nclass Solution:\n    def multipleKnapsack(self, weights: List[int], values: List[int], counts: List[int], capacity: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/dynamic-programming/multiple-knapsack-theory/python/multiple_knapsack_theory.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def multipleKnapsack(self, weights: List[int], values: List[int], counts: List[int], capacity: int) -> int:\n        items: list[tuple[int, int]] = []\n        for w, v, c in zip(weights, values, counts):\n            for _ in range(c):\n                items.append((w, v))\n        dp = [0] * (capacity + 1)\n        for w, v in items:\n            for c in range(capacity, w - 1, -1):\n                dp[c] = max(dp[c], dp[c - w] + v)\n        return dp[capacity]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "stock-problems-summary",
      "title": "Stock Problems Summary",
      "path": "modules/dsa/dynamic-programming/stock-problems-summary",
      "summary": "> Track: `dsa` | Topic: `dynamic-programming`",
      "readme": "# Stock Problems Summary\n\n> Track: `dsa` | Topic: `dynamic-programming`\n\n## Concept\n\nSummarize stock DP states across transaction constraints.\n",
      "sources": []
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "1005-maximize-sum-after-k-negations",
      "title": "1005.Maximize Sum After K Negations",
      "path": "modules/dsa/greedy-algorithm/1005-maximize-sum-after-k-negations",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# 1005.Maximize Sum After K Negations\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nFlip negative values with the largest absolute value first.\n\n## Function\n\n```python\nclass Solution:\n    def largestSumAfterKNegations(self, nums: List[int], k: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/1005-maximize-sum-after-k-negations/python/problem_1005_maximize_sum_after_k_negations.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def largestSumAfterKNegations(self, nums: List[int], k: int) -> int:\n        nums.sort(key=abs, reverse=True)\n        for i in range(len(nums)):\n            if nums[i] < 0 and k > 0:\n                nums[i] = -nums[i]\n                k -= 1\n        if k % 2 == 1:\n            nums[-1] = -nums[-1]\n        return sum(nums)\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "122-best-time-to-buy-and-sell-stock-ii",
      "title": "122.Best Time to Buy and Sell Stock II",
      "path": "modules/dsa/greedy-algorithm/122-best-time-to-buy-and-sell-stock-ii",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# 122.Best Time to Buy and Sell Stock II\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nSum all ascending price differences to capture every profit.\n\n## Function\n\n```python\nclass Solution:\n    def maxProfit(self, prices: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/122-best-time-to-buy-and-sell-stock-ii/python/problem_122_best_time_to_buy_and_sell_stock_ii.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def maxProfit(self, prices: List[int]) -> int:\n        profit = 0\n        for i in range(1, len(prices)):\n            if prices[i] > prices[i - 1]:\n                profit += prices[i] - prices[i - 1]\n        return profit\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "134-gas-station",
      "title": "134.Gas Station",
      "path": "modules/dsa/greedy-algorithm/134-gas-station",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# 134.Gas Station\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nReset the start when the running fuel tank drops below zero.\n\n## Function\n\n```python\nclass Solution:\n    def canCompleteCircuit(self, gas: List[int], cost: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/134-gas-station/python/problem_134_gas_station.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def canCompleteCircuit(self, gas: List[int], cost: List[int]) -> int:\n        total = 0\n        tank = 0\n        start = 0\n        for i in range(len(gas)):\n            diff = gas[i] - cost[i]\n            total += diff\n            tank += diff\n            if tank < 0:\n                start = i + 1\n                tank = 0\n        return start if total >= 0 else -1\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "135-candy",
      "title": "135.Candy",
      "path": "modules/dsa/greedy-algorithm/135-candy",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# 135.Candy\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nAssign candies with two passes to satisfy both neighbors.\n\n## Function\n\n```python\nclass Solution:\n    def candy(self, ratings: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/135-candy/python/problem_135_candy.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def candy(self, ratings: List[int]) -> int:\n        n = len(ratings)\n        candies = [1] * n\n        for i in range(1, n):\n            if ratings[i] > ratings[i - 1]:\n                candies[i] = candies[i - 1] + 1\n        for i in range(n - 2, -1, -1):\n            if ratings[i] > ratings[i + 1]:\n                candies[i] = max(candies[i], candies[i + 1] + 1)\n        return sum(candies)\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "376-wiggle-subsequence",
      "title": "376.Wiggle Subsequence",
      "path": "modules/dsa/greedy-algorithm/376-wiggle-subsequence",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# 376.Wiggle Subsequence\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nCount sign changes between consecutive differences.\n\n## Function\n\n```python\nclass Solution:\n    def wiggleMaxLength(self, nums: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/376-wiggle-subsequence/python/problem_376_wiggle_subsequence.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def wiggleMaxLength(self, nums: List[int]) -> int:\n        if len(nums) < 2:\n            return len(nums)\n        prev_diff = 0\n        count = 1\n        for i in range(1, len(nums)):\n            diff = nums[i] - nums[i - 1]\n            if (diff > 0 and prev_diff <= 0) or (diff < 0 and prev_diff >= 0):\n                count += 1\n                prev_diff = diff\n        return count\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "406-queue-reconstruction-by-height",
      "title": "406.Queue Reconstruction by Height",
      "path": "modules/dsa/greedy-algorithm/406-queue-reconstruction-by-height",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# 406.Queue Reconstruction by Height\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nSort by height descending, then insert by k to rebuild the queue.\n\n## Function\n\n```python\nclass Solution:\n    def reconstructQueue(self, people: List[List[int]]) -> List[List[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/406-queue-reconstruction-by-height/python/problem_406_queue_reconstruction_by_height.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def reconstructQueue(self, people: List[List[int]]) -> List[List[int]]:\n        people.sort(key=lambda x: (-x[0], x[1]))\n        queue: list[list[int]] = []\n        for person in people:\n            queue.insert(person[1], person)\n        return queue\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "435-non-overlapping-intervals",
      "title": "435.Non-overlapping Intervals",
      "path": "modules/dsa/greedy-algorithm/435-non-overlapping-intervals",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# 435.Non-overlapping Intervals\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nKeep intervals with earliest end to maximize non-overlapping count.\n\n## Function\n\n```python\nclass Solution:\n    def eraseOverlapIntervals(self, intervals: List[List[int]]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/435-non-overlapping-intervals/python/problem_435_non_overlapping_intervals.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def eraseOverlapIntervals(self, intervals: List[List[int]]) -> int:\n        if not intervals:\n            return 0\n        intervals.sort(key=lambda x: x[1])\n        count = 1\n        end = intervals[0][1]\n        for start, finish in intervals[1:]:\n            if start >= end:\n                count += 1\n                end = finish\n        return len(intervals) - count\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "45-jump-game-ii",
      "title": "45.Jump Game II",
      "path": "modules/dsa/greedy-algorithm/45-jump-game-ii",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# 45.Jump Game II\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nUse a greedy window to count the minimum jumps.\n\n## Function\n\n```python\nclass Solution:\n    def jump(self, nums: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/45-jump-game-ii/python/problem_45_jump_game_ii.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def jump(self, nums: List[int]) -> int:\n        jumps = 0\n        current_end = 0\n        farthest = 0\n        for i in range(len(nums) - 1):\n            farthest = max(farthest, i + nums[i])\n            if i == current_end:\n                jumps += 1\n                current_end = farthest\n        return jumps\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "452-minimum-number-of-arrows-to-burst-balloons",
      "title": "452.Minimum Number of Arrows to Burst Balloons",
      "path": "modules/dsa/greedy-algorithm/452-minimum-number-of-arrows-to-burst-balloons",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# 452.Minimum Number of Arrows to Burst Balloons\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nSort by end points and shoot a new arrow when ranges no longer overlap.\n\n## Function\n\n```python\nclass Solution:\n    def findMinArrowShots(self, points: List[List[int]]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/452-minimum-number-of-arrows-to-burst-balloons/python/problem_452_minimum_number_of_arrows_to_burst_balloons.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def findMinArrowShots(self, points: List[List[int]]) -> int:\n        if not points:\n            return 0\n        points.sort(key=lambda x: x[1])\n        arrows = 1\n        end = points[0][1]\n        for start, finish in points[1:]:\n            if start > end:\n                arrows += 1\n                end = finish\n        return arrows\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "455-assign-cookies",
      "title": "455.Assign Cookies",
      "path": "modules/dsa/greedy-algorithm/455-assign-cookies",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# 455.Assign Cookies\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nSort greed factors and cookie sizes, then assign smallest feasible cookies.\n\n## Function\n\n```python\nclass Solution:\n    def findContentChildren(self, g: List[int], s: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/455-assign-cookies/python/problem_455_assign_cookies.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def findContentChildren(self, g: List[int], s: List[int]) -> int:\n        g.sort()\n        s.sort()\n        child = 0\n        for size in s:\n            if child < len(g) and size >= g[child]:\n                child += 1\n        return child\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "53-maximum-subarray",
      "title": "53.Maximum Subarray",
      "path": "modules/dsa/greedy-algorithm/53-maximum-subarray",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# 53.Maximum Subarray\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nUse Kadane's algorithm to track the best running sum.\n\n## Function\n\n```python\nclass Solution:\n    def maxSubArray(self, nums: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/53-maximum-subarray/python/problem_53_maximum_subarray.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def maxSubArray(self, nums: List[int]) -> int:\n        current = best = nums[0]\n        for value in nums[1:]:\n            current = max(value, current + value)\n            best = max(best, current)\n        return best\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "55-jump-game",
      "title": "55.Jump Game",
      "path": "modules/dsa/greedy-algorithm/55-jump-game",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# 55.Jump Game\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nTrack the farthest reachable index while scanning.\n\n## Function\n\n```python\nclass Solution:\n    def canJump(self, nums: List[int]) -> bool:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/55-jump-game/python/problem_55_jump_game.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def canJump(self, nums: List[int]) -> bool:\n        farthest = 0\n        for i, value in enumerate(nums):\n            if i > farthest:\n                return False\n            farthest = max(farthest, i + value)\n        return True\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "56-merge-intervals",
      "title": "56.Merge Intervals",
      "path": "modules/dsa/greedy-algorithm/56-merge-intervals",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# 56.Merge Intervals\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nSort by start and merge overlapping intervals greedily.\n\n## Function\n\n```python\nclass Solution:\n    def merge(self, intervals: List[List[int]]) -> List[List[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/56-merge-intervals/python/problem_56_merge_intervals.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def merge(self, intervals: List[List[int]]) -> List[List[int]]:\n        if not intervals:\n            return []\n        intervals.sort(key=lambda x: x[0])\n        merged = [intervals[0]]\n        for start, end in intervals[1:]:\n            if start <= merged[-1][1]:\n                merged[-1][1] = max(merged[-1][1], end)\n            else:\n                merged.append([start, end])\n        return merged\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "738-monotone-increasing-digits",
      "title": "738.Monotone Increasing Digits",
      "path": "modules/dsa/greedy-algorithm/738-monotone-increasing-digits",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# 738.Monotone Increasing Digits\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nScan right-to-left to fix decreasing digits, then fill with 9s.\n\n## Function\n\n```python\nclass Solution:\n    def monotoneIncreasingDigits(self, n: int) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/738-monotone-increasing-digits/python/problem_738_monotone_increasing_digits.py",
          "language": "python",
          "content": "class Solution:\n    def monotoneIncreasingDigits(self, n: int) -> int:\n        digits = list(str(n))\n        marker = len(digits)\n        for i in range(len(digits) - 1, 0, -1):\n            if digits[i] < digits[i - 1]:\n                marker = i\n                digits[i - 1] = str(int(digits[i - 1]) - 1)\n        for i in range(marker, len(digits)):\n            digits[i] = \"9\"\n        return int(\"\".join(digits))\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "763-partition-labels",
      "title": "763.Partition Labels",
      "path": "modules/dsa/greedy-algorithm/763-partition-labels",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# 763.Partition Labels\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nExpand the current partition to the farthest last occurrence.\n\n## Function\n\n```python\nclass Solution:\n    def partitionLabels(self, s: str) -> list[int]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/763-partition-labels/python/problem_763_partition_labels.py",
          "language": "python",
          "content": "class Solution:\n    def partitionLabels(self, s: str) -> list[int]:\n        last = {ch: i for i, ch in enumerate(s)}\n        result: list[int] = []\n        start = 0\n        end = 0\n        for i, ch in enumerate(s):\n            end = max(end, last[ch])\n            if i == end:\n                result.append(end - start + 1)\n                start = i + 1\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "860-lemonade-change",
      "title": "860.Lemonade Change",
      "path": "modules/dsa/greedy-algorithm/860-lemonade-change",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# 860.Lemonade Change\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nTrack $5 and $10 bills to make change greedily.\n\n## Function\n\n```python\nclass Solution:\n    def lemonadeChange(self, bills: list[int]) -> bool:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/860-lemonade-change/python/problem_860_lemonade_change.py",
          "language": "python",
          "content": "class Solution:\n    def lemonadeChange(self, bills: list[int]) -> bool:\n        five = 0\n        ten = 0\n        for bill in bills:\n            if bill == 5:\n                five += 1\n            elif bill == 10:\n                if five == 0:\n                    return False\n                five -= 1\n                ten += 1\n            else:\n                if ten > 0 and five > 0:\n                    ten -= 1\n                    five -= 1\n                elif five >= 3:\n                    five -= 3\n                else:\n                    return False\n        return True\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "968-binary-tree-cameras",
      "title": "968.Binary Tree Cameras",
      "path": "modules/dsa/greedy-algorithm/968-binary-tree-cameras",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# 968.Binary Tree Cameras\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nPlace cameras based on child coverage states to minimize total.\n\n## Function\n\n```python\nclass Solution:\n    def minCameraCover(self, root: Optional[TreeNode]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/968-binary-tree-cameras/python/problem_968_binary_tree_cameras.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\nclass TreeNode:\n    def __init__(self, val: int = 0, left: Optional[\"TreeNode\"] = None, right: Optional[\"TreeNode\"] = None) -> None:\n        self.val = val\n        self.left = left\n        self.right = right\n\nclass Solution:\n    def minCameraCover(self, root: Optional[TreeNode]) -> int:\n        # 0: not covered, 1: has camera, 2: covered\n        self.cameras = 0\n\n        def dfs(node: Optional[TreeNode]) -> int:\n            if node is None:\n                return 2\n            left = dfs(node.left)\n            right = dfs(node.right)\n            if left == 0 or right == 0:\n                self.cameras += 1\n                return 1\n            if left == 1 or right == 1:\n                return 2\n            return 0\n\n        if dfs(root) == 0:\n            self.cameras += 1\n        return self.cameras\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "greedy-algorithms-summary",
      "title": "Greedy Algorithms Summary",
      "path": "modules/dsa/greedy-algorithm/greedy-algorithms-summary",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# Greedy Algorithms Summary\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nReview common greedy patterns: sorting, interval selection, and local optimality proofs.\n",
      "sources": []
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "introduction-to-greedy-algorithm",
      "title": "Introduction to Greedy Algorithm",
      "path": "modules/dsa/greedy-algorithm/introduction-to-greedy-algorithm",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# Introduction to Greedy Algorithm\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nGreedy algorithms build solutions by choosing the best local option at each step.\n",
      "sources": []
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "queue-reconstruction-by-height-vector-explanation",
      "title": "Queue Reconstruction by Height (Vector Explanation)",
      "path": "modules/dsa/greedy-algorithm/queue-reconstruction-by-height-vector-explanation",
      "summary": "> Track: `dsa` | Topic: `greedy-algorithm`",
      "readme": "# Queue Reconstruction by Height (Vector Explanation)\n\n> Track: `dsa` | Topic: `greedy-algorithm`\n\n## Concept\n\nAlternate explanation of queue reconstruction using vector insertion.\n\n## Function\n\n```python\nclass Solution:\n    def reconstructQueue(self, people: List[List[int]]) -> List[List[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/greedy-algorithm/queue-reconstruction-by-height-vector-explanation/python/queue_reconstruction_by_height_vector_explanation.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def reconstructQueue(self, people: List[List[int]]) -> List[List[int]]:\n        people.sort(key=lambda x: (-x[0], x[1]))\n        queue: list[list[int]] = []\n        for person in people:\n            queue.insert(person[1], person)\n        return queue\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "hash-tables",
      "slug": "1-two-sum",
      "title": "1.Two Sum",
      "path": "modules/dsa/hash-tables/1-two-sum",
      "summary": "> Track: `dsa` | Topic: `hash-tables`",
      "readme": "# 1.Two Sum\n\n> Track: `dsa` | Topic: `hash-tables`\n\n## Concept\n\nUse a hash map to remember values and their indices.\n\n## Function\n\n```python\ndef two_sum(nums: list[int], target: int) -> list[int]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/hash-tables/1-two-sum/python/problem_1_two_sum.py",
          "language": "python",
          "content": "def two_sum(nums: list[int], target: int) -> list[int]:\n    seen: dict[int, int] = {}\n    for idx, value in enumerate(nums):\n        complement = target - value\n        if complement in seen:\n            return [seen[complement], idx]\n        seen[value] = idx\n    raise ValueError(\"No solution\")\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "hash-tables",
      "slug": "15-3sum",
      "title": "15.3Sum",
      "path": "modules/dsa/hash-tables/15-3sum",
      "summary": "> Track: `dsa` | Topic: `hash-tables`",
      "readme": "# 15.3Sum\n\n> Track: `dsa` | Topic: `hash-tables`\n\n## Concept\n\nSort the list and use a two-pointer scan to find zero-sum triplets.\n\n## Function\n\n```python\ndef three_sum(nums: list[int]) -> list[list[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/hash-tables/15-3sum/python/problem_15_3sum.py",
          "language": "python",
          "content": "def three_sum(nums: list[int]) -> list[list[int]]:\n    nums.sort()\n    result: list[list[int]] = []\n    n = len(nums)\n    for i in range(n):\n        if i > 0 and nums[i] == nums[i - 1]:\n            continue\n        if nums[i] > 0:\n            break\n        left, right = i + 1, n - 1\n        while left < right:\n            total = nums[i] + nums[left] + nums[right]\n            if total == 0:\n                result.append([nums[i], nums[left], nums[right]])\n                left += 1\n                right -= 1\n                while left < right and nums[left] == nums[left - 1]:\n                    left += 1\n                while left < right and nums[right] == nums[right + 1]:\n                    right -= 1\n            elif total < 0:\n                left += 1\n            else:\n                right -= 1\n    return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "hash-tables",
      "slug": "18-4sum",
      "title": "18.4Sum",
      "path": "modules/dsa/hash-tables/18-4sum",
      "summary": "> Track: `dsa` | Topic: `hash-tables`",
      "readme": "# 18.4Sum\n\n> Track: `dsa` | Topic: `hash-tables`\n\n## Concept\n\nSort and fix two numbers, then use two pointers for the remaining pair.\n\n## Function\n\n```python\ndef four_sum(nums: list[int], target: int) -> list[list[int]]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/hash-tables/18-4sum/python/problem_18_4sum.py",
          "language": "python",
          "content": "def four_sum(nums: list[int], target: int) -> list[list[int]]:\n    nums.sort()\n    result: list[list[int]] = []\n    n = len(nums)\n    for i in range(n):\n        if i > 0 and nums[i] == nums[i - 1]:\n            continue\n        for j in range(i + 1, n):\n            if j > i + 1 and nums[j] == nums[j - 1]:\n                continue\n            left, right = j + 1, n - 1\n            while left < right:\n                total = nums[i] + nums[j] + nums[left] + nums[right]\n                if total == target:\n                    result.append([nums[i], nums[j], nums[left], nums[right]])\n                    left += 1\n                    right -= 1\n                    while left < right and nums[left] == nums[left - 1]:\n                        left += 1\n                    while left < right and nums[right] == nums[right + 1]:\n                        right -= 1\n                elif total < target:\n                    left += 1\n                else:\n                    right -= 1\n    return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "hash-tables",
      "slug": "202-happy-number",
      "title": "202.Happy Number",
      "path": "modules/dsa/hash-tables/202-happy-number",
      "summary": "> Track: `dsa` | Topic: `hash-tables`",
      "readme": "# 202.Happy Number\n\n> Track: `dsa` | Topic: `hash-tables`\n\n## Concept\n\nRepeatedly replace the number with the sum of squares of digits and detect cycles.\n\n## Function\n\n```python\ndef is_happy(n: int) -> bool:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/hash-tables/202-happy-number/python/problem_202_happy_number.py",
          "language": "python",
          "content": "def is_happy(n: int) -> bool:\n    seen = set()\n    while n != 1 and n not in seen:\n        seen.add(n)\n        total = 0\n        while n > 0:\n            n, digit = divmod(n, 10)\n            total += digit * digit\n        n = total\n    return n == 1\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "hash-tables",
      "slug": "242-valid-anagram",
      "title": "242.Valid Anagram",
      "path": "modules/dsa/hash-tables/242-valid-anagram",
      "summary": "> Track: `dsa` | Topic: `hash-tables`",
      "readme": "# 242.Valid Anagram\n\n> Track: `dsa` | Topic: `hash-tables`\n\n## Concept\n\nTwo strings are anagrams when their character counts match.\n\n## Function\n\n```python\ndef is_anagram(s: str, t: str) -> bool:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/hash-tables/242-valid-anagram/python/problem_242_valid_anagram.py",
          "language": "python",
          "content": "def is_anagram(s: str, t: str) -> bool:\n    if len(s) != len(t):\n        return False\n    counts: dict[str, int] = {}\n    for ch in s:\n        counts[ch] = counts.get(ch, 0) + 1\n    for ch in t:\n        if ch not in counts:\n            return False\n        counts[ch] -= 1\n        if counts[ch] == 0:\n            del counts[ch]\n    return not counts\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "hash-tables",
      "slug": "349-intersection-of-two-arrays",
      "title": "349.Intersection of Two Arrays",
      "path": "modules/dsa/hash-tables/349-intersection-of-two-arrays",
      "summary": "> Track: `dsa` | Topic: `hash-tables`",
      "readme": "# 349.Intersection of Two Arrays\n\n> Track: `dsa` | Topic: `hash-tables`\n\n## Concept\n\nUse sets to keep only unique elements in the intersection.\n\n## Function\n\n```python\ndef intersection(nums1: list[int], nums2: list[int]) -> list[int]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/hash-tables/349-intersection-of-two-arrays/python/problem_349_intersection_of_two_arrays.py",
          "language": "python",
          "content": "def intersection(nums1: list[int], nums2: list[int]) -> list[int]:\n    return list(set(nums1) & set(nums2))\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "hash-tables",
      "slug": "383-ransom-note",
      "title": "383.Ransom Note",
      "path": "modules/dsa/hash-tables/383-ransom-note",
      "summary": "> Track: `dsa` | Topic: `hash-tables`",
      "readme": "# 383.Ransom Note\n\n> Track: `dsa` | Topic: `hash-tables`\n\n## Concept\n\nCount magazine letters and subtract for each ransom character.\n\n## Function\n\n```python\ndef can_construct(ransom_note: str, magazine: str) -> bool:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/hash-tables/383-ransom-note/python/problem_383_ransom_note.py",
          "language": "python",
          "content": "def can_construct(ransom_note: str, magazine: str) -> bool:\n    counts: dict[str, int] = {}\n    for ch in magazine:\n        counts[ch] = counts.get(ch, 0) + 1\n    for ch in ransom_note:\n        if counts.get(ch, 0) == 0:\n            return False\n        counts[ch] -= 1\n    return True\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "hash-tables",
      "slug": "454-4sum-ii",
      "title": "454.4Sum II",
      "path": "modules/dsa/hash-tables/454-4sum-ii",
      "summary": "> Track: `dsa` | Topic: `hash-tables`",
      "readme": "# 454.4Sum II\n\n> Track: `dsa` | Topic: `hash-tables`\n\n## Concept\n\nCount pair sums from two arrays and look for complements from the other two.\n\n## Function\n\n```python\ndef four_sum_count(nums1: list[int], nums2: list[int], nums3: list[int], nums4: list[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/hash-tables/454-4sum-ii/python/problem_454_4sum_ii.py",
          "language": "python",
          "content": "def four_sum_count(\n    nums1: list[int],\n    nums2: list[int],\n    nums3: list[int],\n    nums4: list[int],\n) -> int:\n    counts: dict[int, int] = {}\n    for a in nums1:\n        for b in nums2:\n            counts[a + b] = counts.get(a + b, 0) + 1\n    total = 0\n    for c in nums3:\n        for d in nums4:\n            total += counts.get(-(c + d), 0)\n    return total\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "hash-tables",
      "slug": "hash-table-summary",
      "title": "Hash Table Summary",
      "path": "modules/dsa/hash-tables/hash-table-summary",
      "summary": "> Track: `dsa` | Topic: `hash-tables`",
      "readme": "# Hash Table Summary\n\n> Track: `dsa` | Topic: `hash-tables`\n\n## Concept\n\nSummarize a list by counting uniques and the most frequent value.\n\n## Function\n\n```python\ndef hash_table_summary(values: list[str]) -> dict[str, int | str | None]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/hash-tables/hash-table-summary/python/hash_table_summary.py",
          "language": "python",
          "content": "def hash_table_summary(values: list[str]) -> dict[str, int | str | None]:\n    if not values:\n        return {\"unique\": 0, \"most_common\": None, \"max_count\": 0}\n\n    counts: dict[str, int] = {}\n    for value in values:\n        counts[value] = counts.get(value, 0) + 1\n\n    most_common = None\n    max_count = 0\n    for key in sorted(counts):\n        count = counts[key]\n        if count > max_count:\n            max_count = count\n            most_common = key\n    return {\"unique\": len(counts), \"most_common\": most_common, \"max_count\": max_count}\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "hash-tables",
      "slug": "introduction-to-hash-table",
      "title": "Introduction to Hash Table",
      "path": "modules/dsa/hash-tables/introduction-to-hash-table",
      "summary": "> Track: `dsa` | Topic: `hash-tables`",
      "readme": "# Introduction to Hash Table\n\n> Track: `dsa` | Topic: `hash-tables`\n\n## Concept\n\nCount frequencies by storing values as keys and incrementing their counts.\n\n## Function\n\n```python\ndef frequency_map(values: list[int]) -> dict[int, int]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/hash-tables/introduction-to-hash-table/python/introduction_to_hash_table.py",
          "language": "python",
          "content": "def frequency_map(values: list[int]) -> dict[int, int]:\n    counts: dict[int, int] = {}\n    for value in values:\n        counts[value] = counts.get(value, 0) + 1\n    return counts\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "linked-list",
      "slug": "142-linked-list-cycle-ii",
      "title": "142.Linked List Cycle II",
      "path": "modules/dsa/linked-list/142-linked-list-cycle-ii",
      "summary": "> Track: `dsa` | Topic: `linked-list`",
      "readme": "# 142.Linked List Cycle II\n\n> Track: `dsa` | Topic: `linked-list`\n\n## Concept\n\nUse Floyd's cycle detection to find the entry node of the loop.\n\n## Function\n\n```python\ndef detect_cycle(head: ListNode | None) -> ListNode | None:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/linked-list/142-linked-list-cycle-ii/python/problem_142_linked_list_cycle_ii.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass ListNode:\n    def __init__(self, val: int, next: Optional[\"ListNode\"] = None):\n        self.val = val\n        self.next = next\n\n\ndef detect_cycle(head: Optional[ListNode]) -> Optional[ListNode]:\n    slow = head\n    fast = head\n    while fast and fast.next:\n        slow = slow.next\n        fast = fast.next.next\n        if slow is fast:\n            entry = head\n            while entry is not slow:\n                entry = entry.next\n                slow = slow.next\n            return entry\n    return None\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "linked-list",
      "slug": "160-intersection-of-two-linked-lists",
      "title": "160.Intersection Of Two Linked Lists",
      "path": "modules/dsa/linked-list/160-intersection-of-two-linked-lists",
      "summary": "> Track: `dsa` | Topic: `linked-list`",
      "readme": "# 160.Intersection Of Two Linked Lists\n\n> Track: `dsa` | Topic: `linked-list`\n\n## Concept\n\nWalk two pointers that switch heads to align the remaining distance.\n\n## Function\n\n```python\ndef get_intersection_node(head_a: ListNode | None, head_b: ListNode | None) -> ListNode | None:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/linked-list/160-intersection-of-two-linked-lists/python/problem_160_intersection_of_two_linked_lists.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass ListNode:\n    def __init__(self, val: int, next: Optional[\"ListNode\"] = None):\n        self.val = val\n        self.next = next\n\n\ndef get_intersection_node(\n    head_a: Optional[ListNode],\n    head_b: Optional[ListNode],\n) -> Optional[ListNode]:\n    pointer_a = head_a\n    pointer_b = head_b\n    while pointer_a is not pointer_b:\n        pointer_a = pointer_a.next if pointer_a else head_b\n        pointer_b = pointer_b.next if pointer_b else head_a\n    return pointer_a\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "linked-list",
      "slug": "19-remove-nth-node-from-end-of-list",
      "title": "19.Remove Nth Node From End Of List",
      "path": "modules/dsa/linked-list/19-remove-nth-node-from-end-of-list",
      "summary": "> Track: `dsa` | Topic: `linked-list`",
      "readme": "# 19.Remove Nth Node From End Of List\n\n> Track: `dsa` | Topic: `linked-list`\n\n## Concept\n\nUse two pointers separated by n nodes to delete the target.\n\n## Function\n\n```python\ndef remove_nth_node_from_end(head: ListNode | None, n: int) -> ListNode | None:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/linked-list/19-remove-nth-node-from-end-of-list/python/problem_19_remove_nth_node_from_end_of_list.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass ListNode:\n    def __init__(self, val: int, next: Optional[\"ListNode\"] = None):\n        self.val = val\n        self.next = next\n\n\ndef remove_nth_node_from_end(head: Optional[ListNode], n: int) -> Optional[ListNode]:\n    dummy = ListNode(0, head)\n    fast = dummy\n    for _ in range(n):\n        if fast.next is None:\n            return head\n        fast = fast.next\n    slow = dummy\n    while fast.next:\n        fast = fast.next\n        slow = slow.next\n    slow.next = slow.next.next\n    return dummy.next\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "linked-list",
      "slug": "203-remove-linked-list-elements",
      "title": "203.Remove Linked List Elements",
      "path": "modules/dsa/linked-list/203-remove-linked-list-elements",
      "summary": "> Track: `dsa` | Topic: `linked-list`",
      "readme": "# 203.Remove Linked List Elements\n\n> Track: `dsa` | Topic: `linked-list`\n\n## Concept\n\nRemove nodes with a target value by skipping them during traversal.\n\n## Function\n\n```python\ndef remove_linked_list_elements(head: ListNode | None, val: int) -> ListNode | None:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/linked-list/203-remove-linked-list-elements/python/problem_203_remove_linked_list_elements.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass ListNode:\n    def __init__(self, val: int, next: Optional[\"ListNode\"] = None):\n        self.val = val\n        self.next = next\n\n\ndef remove_linked_list_elements(head: Optional[ListNode], val: int) -> Optional[ListNode]:\n    dummy = ListNode(0, head)\n    prev = dummy\n    current = head\n    while current:\n        if current.val == val:\n            prev.next = current.next\n        else:\n            prev = current\n        current = current.next\n    return dummy.next\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "linked-list",
      "slug": "206-reverse-linked-list",
      "title": "206.Reverse Linked List",
      "path": "modules/dsa/linked-list/206-reverse-linked-list",
      "summary": "> Track: `dsa` | Topic: `linked-list`",
      "readme": "# 206.Reverse Linked List\n\n> Track: `dsa` | Topic: `linked-list`\n\n## Concept\n\nIteratively reverse pointers while walking the list.\n\n## Function\n\n```python\ndef reverse_linked_list(head: ListNode | None) -> ListNode | None:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/linked-list/206-reverse-linked-list/python/problem_206_reverse_linked_list.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass ListNode:\n    def __init__(self, val: int, next: Optional[\"ListNode\"] = None):\n        self.val = val\n        self.next = next\n\n\ndef reverse_linked_list(head: Optional[ListNode]) -> Optional[ListNode]:\n    prev = None\n    current = head\n    while current:\n        next_node = current.next\n        current.next = prev\n        prev = current\n        current = next_node\n    return prev\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "linked-list",
      "slug": "24-swap-nodes-in-pairs",
      "title": "24.Swap Nodes In Pairs",
      "path": "modules/dsa/linked-list/24-swap-nodes-in-pairs",
      "summary": "> Track: `dsa` | Topic: `linked-list`",
      "readme": "# 24.Swap Nodes In Pairs\n\n> Track: `dsa` | Topic: `linked-list`\n\n## Concept\n\nSwap adjacent pairs by rewiring pointers with a dummy head.\n\n## Function\n\n```python\ndef swap_nodes_in_pairs(head: ListNode | None) -> ListNode | None:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/linked-list/24-swap-nodes-in-pairs/python/problem_24_swap_nodes_in_pairs.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass ListNode:\n    def __init__(self, val: int, next: Optional[\"ListNode\"] = None):\n        self.val = val\n        self.next = next\n\n\ndef swap_nodes_in_pairs(head: Optional[ListNode]) -> Optional[ListNode]:\n    dummy = ListNode(0, head)\n    prev = dummy\n    while prev.next and prev.next.next:\n        first = prev.next\n        second = first.next\n        first.next = second.next\n        second.next = first\n        prev.next = second\n        prev = first\n    return dummy.next\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "linked-list",
      "slug": "707-design-linked-list",
      "title": "707.Design Linked List",
      "path": "modules/dsa/linked-list/707-design-linked-list",
      "summary": "> Track: `dsa` | Topic: `linked-list`",
      "readme": "# 707.Design Linked List\n\n> Track: `dsa` | Topic: `linked-list`\n\n## Concept\n\nImplement a singly linked list with indexed operations and a size counter.\n\n## Function\n\n```python\nclass MyLinkedList:\n    def get(self, index: int) -> int:\n    def addAtHead(self, val: int) -> None:\n    def addAtTail(self, val: int) -> None:\n    def addAtIndex(self, index: int, val: int) -> None:\n    def deleteAtIndex(self, index: int) -> None:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/linked-list/707-design-linked-list/python/problem_707_design_linked_list.py",
          "language": "python",
          "content": "class ListNode:\n    def __init__(self, val: int = 0, next: \"ListNode | None\" = None):\n        self.val = val\n        self.next = next\n\n\nclass MyLinkedList:\n    def __init__(self) -> None:\n        self.size = 0\n        self.head = ListNode(0)\n\n    def get(self, index: int) -> int:\n        if index < 0 or index >= self.size:\n            return -1\n        current = self.head.next\n        for _ in range(index):\n            current = current.next\n        return current.val\n\n    def addAtHead(self, val: int) -> None:\n        self.addAtIndex(0, val)\n\n    def addAtTail(self, val: int) -> None:\n        self.addAtIndex(self.size, val)\n\n    def addAtIndex(self, index: int, val: int) -> None:\n        if index < 0:\n            index = 0\n        if index > self.size:\n            return\n        prev = self.head\n        for _ in range(index):\n            prev = prev.next\n        prev.next = ListNode(val, prev.next)\n        self.size += 1\n\n    def deleteAtIndex(self, index: int) -> None:\n        if index < 0 or index >= self.size:\n            return\n        prev = self.head\n        for _ in range(index):\n            prev = prev.next\n        prev.next = prev.next.next\n        self.size -= 1\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "linked-list",
      "slug": "linked-list-basics",
      "title": "Linked List Basics",
      "path": "modules/dsa/linked-list/linked-list-basics",
      "summary": "> Track: `dsa` | Topic: `linked-list`",
      "readme": "# Linked List Basics\n\n> Track: `dsa` | Topic: `linked-list`\n\n## Concept\n\nBuild a singly linked list and report basic properties like length, head, and tail.\n\n## Function\n\n```python\ndef linked_list_basics(values: list[int]) -> dict[str, int | None]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/linked-list/linked-list-basics/python/linked_list_basics.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass ListNode:\n    def __init__(self, val: int, next: Optional[\"ListNode\"] = None):\n        self.val = val\n        self.next = next\n\n\ndef linked_list_basics(values: list[int]) -> dict[str, int | None]:\n    head: Optional[ListNode] = None\n    tail: Optional[ListNode] = None\n    length = 0\n    for value in values:\n        node = ListNode(value)\n        if head is None:\n            head = node\n            tail = node\n        else:\n            tail.next = node\n            tail = node\n        length += 1\n    return {\n        \"length\": length,\n        \"head\": head.val if head else None,\n        \"tail\": tail.val if tail else None,\n    }\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "linked-list",
      "slug": "linked-list-summary",
      "title": "Linked List Summary",
      "path": "modules/dsa/linked-list/linked-list-summary",
      "summary": "> Track: `dsa` | Topic: `linked-list`",
      "readme": "# Linked List Summary\n\n> Track: `dsa` | Topic: `linked-list`\n\n## Concept\n\nCompute quick stats like length, head, tail, and detect cycles.\n\n## Function\n\n```python\ndef linked_list_summary(head: ListNode | None) -> dict[str, int | bool | None]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/linked-list/linked-list-summary/python/linked_list_summary.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nfrom typing import Optional\n\n\nclass ListNode:\n    def __init__(self, val: int, next: Optional[\"ListNode\"] = None):\n        self.val = val\n        self.next = next\n\n\ndef linked_list_summary(head: Optional[ListNode]) -> dict[str, int | bool | None]:\n    if head is None:\n        return {\n            \"length\": 0,\n            \"head\": None,\n            \"tail\": None,\n            \"has_cycle\": False,\n        }\n\n    if _has_cycle(head):\n        return {\n            \"length\": None,\n            \"head\": head.val,\n            \"tail\": None,\n            \"has_cycle\": True,\n        }\n\n    length = 0\n    tail = head\n    current = head\n    while current:\n        length += 1\n        tail = current\n        current = current.next\n\n    return {\n        \"length\": length,\n        \"head\": head.val,\n        \"tail\": tail.val,\n        \"has_cycle\": False,\n    }\n\n\ndef _has_cycle(head: ListNode) -> bool:\n    slow = head\n    fast = head\n    while fast and fast.next:\n        slow = slow.next\n        fast = fast.next.next\n        if slow is fast:\n            return True\n    return False\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "monotonic-stack",
      "slug": "42-trapping-rain-water",
      "title": "42.Trapping Rain Water",
      "path": "modules/dsa/monotonic-stack/42-trapping-rain-water",
      "summary": "> Track: `dsa` | Topic: `monotonic-stack`",
      "readme": "# 42.Trapping Rain Water\n\n> Track: `dsa` | Topic: `monotonic-stack`\n\n## Concept\n\nUse a monotonic stack to compute trapped water when a higher bar closes a valley.\n\n## Function\n\n```python\nclass Solution:\n    def trap(self, height: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/monotonic-stack/42-trapping-rain-water/python/problem_42_trapping_rain_water.py",
          "language": "python",
          "content": "from typing import List\n\n\nclass Solution:\n    def trap(self, height: List[int]) -> int:\n        stack: List[int] = []\n        trapped = 0\n        for i, h in enumerate(height):\n            while stack and h > height[stack[-1]]:\n                bottom = stack.pop()\n                if not stack:\n                    break\n                left = stack[-1]\n                width = i - left - 1\n                bounded_height = min(h, height[left]) - height[bottom]\n                if bounded_height > 0:\n                    trapped += width * bounded_height\n            stack.append(i)\n        return trapped\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "monotonic-stack",
      "slug": "496-next-greater-element-i",
      "title": "496.Next Greater Element I",
      "path": "modules/dsa/monotonic-stack/496-next-greater-element-i",
      "summary": "> Track: `dsa` | Topic: `monotonic-stack`",
      "readme": "# 496.Next Greater Element I\n\n> Track: `dsa` | Topic: `monotonic-stack`\n\n## Concept\n\nUse a decreasing monotonic stack to precompute next greater elements in nums2.\n\n## Function\n\n```python\nclass Solution:\n    def nextGreaterElement(self, nums1: List[int], nums2: List[int]) -> List[int]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/monotonic-stack/496-next-greater-element-i/python/problem_496_next_greater_element_i.py",
          "language": "python",
          "content": "from typing import Dict, List\n\n\nclass Solution:\n    def nextGreaterElement(self, nums1: List[int], nums2: List[int]) -> List[int]:\n        next_greater: Dict[int, int] = {}\n        stack: List[int] = []\n        for num in nums2:\n            while stack and num > stack[-1]:\n                next_greater[stack.pop()] = num\n            stack.append(num)\n        for num in stack:\n            next_greater[num] = -1\n        return [next_greater[num] for num in nums1]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "monotonic-stack",
      "slug": "503-next-greater-element-ii",
      "title": "503.Next Greater Element II",
      "path": "modules/dsa/monotonic-stack/503-next-greater-element-ii",
      "summary": "> Track: `dsa` | Topic: `monotonic-stack`",
      "readme": "# 503.Next Greater Element II\n\n> Track: `dsa` | Topic: `monotonic-stack`\n\n## Concept\n\nScan the array twice with a monotonic stack to handle circular next greater elements.\n\n## Function\n\n```python\nclass Solution:\n    def nextGreaterElements(self, nums: List[int]) -> List[int]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/monotonic-stack/503-next-greater-element-ii/python/problem_503_next_greater_element_ii.py",
          "language": "python",
          "content": "from typing import List\n\n\nclass Solution:\n    def nextGreaterElements(self, nums: List[int]) -> List[int]:\n        n = len(nums)\n        result = [-1] * n\n        stack: List[int] = []\n        for i in range(2 * n):\n            idx = i % n\n            while stack and nums[idx] > nums[stack[-1]]:\n                result[stack.pop()] = nums[idx]\n            if i < n:\n                stack.append(idx)\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "monotonic-stack",
      "slug": "739-daily-temperatures",
      "title": "739.Daily Temperatures",
      "path": "modules/dsa/monotonic-stack/739-daily-temperatures",
      "summary": "> Track: `dsa` | Topic: `monotonic-stack`",
      "readme": "# 739.Daily Temperatures\n\n> Track: `dsa` | Topic: `monotonic-stack`\n\n## Concept\n\nUse a decreasing monotonic stack of indices to find the next warmer day.\n\n## Function\n\n```python\nclass Solution:\n    def dailyTemperatures(self, temperatures: List[int]) -> List[int]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/monotonic-stack/739-daily-temperatures/python/problem_739_daily_temperatures.py",
          "language": "python",
          "content": "from typing import List\n\n\nclass Solution:\n    def dailyTemperatures(self, temperatures: List[int]) -> List[int]:\n        n = len(temperatures)\n        answer = [0] * n\n        stack = []\n        for i, temp in enumerate(temperatures):\n            while stack and temp > temperatures[stack[-1]]:\n                idx = stack.pop()\n                answer[idx] = i - idx\n            stack.append(i)\n        return answer\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "monotonic-stack",
      "slug": "84-largest-rectangle-in-histogram",
      "title": "84.Largest Rectangle in Histogram",
      "path": "modules/dsa/monotonic-stack/84-largest-rectangle-in-histogram",
      "summary": "> Track: `dsa` | Topic: `monotonic-stack`",
      "readme": "# 84.Largest Rectangle in Histogram\n\n> Track: `dsa` | Topic: `monotonic-stack`\n\n## Concept\n\nUse a monotonic increasing stack to expand each bar to its maximal rectangle.\n\n## Function\n\n```python\nclass Solution:\n    def largestRectangleArea(self, heights: List[int]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/monotonic-stack/84-largest-rectangle-in-histogram/python/problem_84_largest_rectangle_in_histogram.py",
          "language": "python",
          "content": "from typing import List\n\n\nclass Solution:\n    def largestRectangleArea(self, heights: List[int]) -> int:\n        stack: List[int] = []\n        max_area = 0\n        for i, h in enumerate(heights + [0]):\n            while stack and h < heights[stack[-1]]:\n                height = heights[stack.pop()]\n                left = stack[-1] if stack else -1\n                width = i - left - 1\n                area = height * width\n                if area > max_area:\n                    max_area = area\n            stack.append(i)\n        return max_area\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "stack-and-queue",
      "slug": "1047-remove-all-adjacent-duplicates-in-string",
      "title": "1047.Remove All Adjacent Duplicates in String",
      "path": "modules/dsa/stack-and-queue/1047-remove-all-adjacent-duplicates-in-string",
      "summary": "> Track: `dsa` | Topic: `stack-and-queue`",
      "readme": "# 1047.Remove All Adjacent Duplicates in String\n\n> Track: `dsa` | Topic: `stack-and-queue`\n\n## Concept\n\nUse a stack: remove the top when it matches the current character.\n\n## Function\n\n```python\nclass Solution:\n    def removeDuplicates(self, s: str) -> str:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/stack-and-queue/1047-remove-all-adjacent-duplicates-in-string/python/problem_1047_remove_all_adjacent_duplicates_in_string.py",
          "language": "python",
          "content": "class Solution:\n    def removeDuplicates(self, s: str) -> str:\n        stack: list[str] = []\n        for ch in s:\n            if stack and stack[-1] == ch:\n                stack.pop()\n            else:\n                stack.append(ch)\n        return \"\".join(stack)\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "stack-and-queue",
      "slug": "150-evaluate-reverse-polish-notation",
      "title": "150.Evaluate Reverse Polish Notation",
      "path": "modules/dsa/stack-and-queue/150-evaluate-reverse-polish-notation",
      "summary": "> Track: `dsa` | Topic: `stack-and-queue`",
      "readme": "# 150.Evaluate Reverse Polish Notation\n\n> Track: `dsa` | Topic: `stack-and-queue`\n\n## Concept\n\nUse a stack to evaluate tokens, applying operators to the last two values.\n\n## Function\n\n```python\nclass Solution:\n    def evalRPN(self, tokens: List[str]) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/stack-and-queue/150-evaluate-reverse-polish-notation/python/problem_150_evaluate_reverse_polish_notation.py",
          "language": "python",
          "content": "class Solution:\n    def evalRPN(self, tokens: list[str]) -> int:\n        stack: list[int] = []\n        for token in tokens:\n            if token in {\"+\", \"-\", \"*\", \"/\"}:\n                b = stack.pop()\n                a = stack.pop()\n                if token == \"+\":\n                    stack.append(a + b)\n                elif token == \"-\":\n                    stack.append(a - b)\n                elif token == \"*\":\n                    stack.append(a * b)\n                else:\n                    stack.append(int(a / b))\n            else:\n                stack.append(int(token))\n        return stack[-1]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "stack-and-queue",
      "slug": "20-valid-parentheses",
      "title": "20.Valid Parentheses",
      "path": "modules/dsa/stack-and-queue/20-valid-parentheses",
      "summary": "> Track: `dsa` | Topic: `stack-and-queue`",
      "readme": "# 20.Valid Parentheses\n\n> Track: `dsa` | Topic: `stack-and-queue`\n\n## Concept\n\nUse a stack to match each closing bracket with the most recent opening bracket.\n\n## Function\n\n```python\nclass Solution:\n    def isValid(self, s: str) -> bool:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/stack-and-queue/20-valid-parentheses/python/problem_20_valid_parentheses.py",
          "language": "python",
          "content": "class Solution:\n    def isValid(self, s: str) -> bool:\n        stack = []\n        pairs = {')': '(', ']': '[', '}': '{'}\n        for ch in s:\n            if ch in pairs:\n                if not stack or stack[-1] != pairs[ch]:\n                    return False\n                stack.pop()\n            else:\n                stack.append(ch)\n        return not stack\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "stack-and-queue",
      "slug": "225-implement-stack-using-queues",
      "title": "225.Implement Stack using Queues",
      "path": "modules/dsa/stack-and-queue/225-implement-stack-using-queues",
      "summary": "> Track: `dsa` | Topic: `stack-and-queue`",
      "readme": "# 225.Implement Stack using Queues\n\n> Track: `dsa` | Topic: `stack-and-queue`\n\n## Concept\n\nRotate the queue after each push so the front is the stack top.\n\n## Function\n\n```python\nclass MyStack:\n    def push(self, x: int) -> None:\n    def pop(self) -> int:\n    def top(self) -> int:\n    def empty(self) -> bool:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/stack-and-queue/225-implement-stack-using-queues/python/problem_225_implement_stack_using_queues.py",
          "language": "python",
          "content": "from collections import deque\n\nclass MyStack:\n    def __init__(self) -> None:\n        self.queue = deque()\n\n    def push(self, x: int) -> None:\n        self.queue.append(x)\n        for _ in range(len(self.queue) - 1):\n            self.queue.append(self.queue.popleft())\n\n    def pop(self) -> int:\n        return self.queue.popleft()\n\n    def top(self) -> int:\n        return self.queue[0]\n\n    def empty(self) -> bool:\n        return not self.queue\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "stack-and-queue",
      "slug": "232-implement-queue-using-stacks",
      "title": "232.Implement Queue using Stacks",
      "path": "modules/dsa/stack-and-queue/232-implement-queue-using-stacks",
      "summary": "> Track: `dsa` | Topic: `stack-and-queue`",
      "readme": "# 232.Implement Queue using Stacks\n\n> Track: `dsa` | Topic: `stack-and-queue`\n\n## Concept\n\nUse two stacks: push to in-stack, pop/peek from out-stack after transferring.\n\n## Function\n\n```python\nclass MyQueue:\n    def push(self, x: int) -> None:\n    def pop(self) -> int:\n    def peek(self) -> int:\n    def empty(self) -> bool:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/stack-and-queue/232-implement-queue-using-stacks/python/problem_232_implement_queue_using_stacks.py",
          "language": "python",
          "content": "class MyQueue:\n    def __init__(self) -> None:\n        self.in_stack: list[int] = []\n        self.out_stack: list[int] = []\n\n    def push(self, x: int) -> None:\n        self.in_stack.append(x)\n\n    def _move(self) -> None:\n        if not self.out_stack:\n            while self.in_stack:\n                self.out_stack.append(self.in_stack.pop())\n\n    def pop(self) -> int:\n        self._move()\n        return self.out_stack.pop()\n\n    def peek(self) -> int:\n        self._move()\n        return self.out_stack[-1]\n\n    def empty(self) -> bool:\n        return not self.in_stack and not self.out_stack\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "stack-and-queue",
      "slug": "239-sliding-window-maximum",
      "title": "239.Sliding Window Maximum",
      "path": "modules/dsa/stack-and-queue/239-sliding-window-maximum",
      "summary": "> Track: `dsa` | Topic: `stack-and-queue`",
      "readme": "# 239.Sliding Window Maximum\n\n> Track: `dsa` | Topic: `stack-and-queue`\n\n## Concept\n\nMaintain a decreasing deque of indices to track the window maximum.\n\n## Function\n\n```python\nclass Solution:\n    def maxSlidingWindow(self, nums: List[int], k: int) -> List[int]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/stack-and-queue/239-sliding-window-maximum/python/problem_239_sliding_window_maximum.py",
          "language": "python",
          "content": "from collections import deque\nfrom typing import List\n\nclass Solution:\n    def maxSlidingWindow(self, nums: List[int], k: int) -> List[int]:\n        if not nums or k <= 0:\n            return []\n        deq: deque[int] = deque()\n        result: list[int] = []\n        for i, num in enumerate(nums):\n            while deq and deq[0] <= i - k:\n                deq.popleft()\n            while deq and nums[deq[-1]] <= num:\n                deq.pop()\n            deq.append(i)\n            if i >= k - 1:\n                result.append(nums[deq[0]])\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "stack-and-queue",
      "slug": "347-top-k-frequent-elements",
      "title": "347.Top K Frequent Elements",
      "path": "modules/dsa/stack-and-queue/347-top-k-frequent-elements",
      "summary": "> Track: `dsa` | Topic: `stack-and-queue`",
      "readme": "# 347.Top K Frequent Elements\n\n> Track: `dsa` | Topic: `stack-and-queue`\n\n## Concept\n\nCount frequencies and use buckets to collect the k most frequent elements.\n\n## Function\n\n```python\nclass Solution:\n    def topKFrequent(self, nums: List[int], k: int) -> List[int]:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/stack-and-queue/347-top-k-frequent-elements/python/problem_347_top_k_frequent_elements.py",
          "language": "python",
          "content": "from collections import Counter\nfrom typing import List\n\nclass Solution:\n    def topKFrequent(self, nums: List[int], k: int) -> List[int]:\n        counts = Counter(nums)\n        buckets: list[list[int]] = [[] for _ in range(len(nums) + 1)]\n        for num, freq in counts.items():\n            buckets[freq].append(num)\n        result: list[int] = []\n        for freq in range(len(buckets) - 1, 0, -1):\n            for num in buckets[freq]:\n                result.append(num)\n                if len(result) == k:\n                    return result\n        return result\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "stack-and-queue",
      "slug": "introduction-to-stack-and-queue",
      "title": "Introduction to Stack and Queue",
      "path": "modules/dsa/stack-and-queue/introduction-to-stack-and-queue",
      "summary": "> Track: `dsa` | Topic: `stack-and-queue`",
      "readme": "# Introduction to Stack and Queue\n\n> Track: `dsa` | Topic: `stack-and-queue`\n\n## Concept\n\nStacks are LIFO and queues are FIFO structures with push/pop or enqueue/dequeue operations.\n",
      "sources": []
    },
    {
      "track": "dsa",
      "topic": "stack-and-queue",
      "slug": "stack-and-queue-summary",
      "title": "Stack and Queue Summary",
      "path": "modules/dsa/stack-and-queue/stack-and-queue-summary",
      "summary": "> Track: `dsa` | Topic: `stack-and-queue`",
      "readme": "# Stack and Queue Summary\n\n> Track: `dsa` | Topic: `stack-and-queue`\n\n## Concept\n\nReview stacks, queues, monotonic deques, and typical stack/queue problem patterns.\n",
      "sources": []
    },
    {
      "track": "dsa",
      "topic": "string",
      "slug": "151-reverse-words-in-a-string",
      "title": "151.Reverse Words in a String",
      "path": "modules/dsa/string/151-reverse-words-in-a-string",
      "summary": "> Track: `dsa` | Topic: `string`",
      "readme": "# 151.Reverse Words in a String\n\n> Track: `dsa` | Topic: `string`\n\n## Concept\n\nSplit on whitespace, reverse the word order, and join with single spaces.\n\n## Function\n\n```python\nclass Solution:\n    def reverseWords(self, s: str) -> str:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/string/151-reverse-words-in-a-string/python/reverse_words_in_a_string.py",
          "language": "python",
          "content": "class Solution:\n    def reverseWords(self, s: str) -> str:\n        words = s.split()\n        return \" \".join(reversed(words))\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "string",
      "slug": "28-implement-strstr",
      "title": "28.Implement strStr()",
      "path": "modules/dsa/string/28-implement-strstr",
      "summary": "> Track: `dsa` | Topic: `string`",
      "readme": "# 28.Implement strStr()\n\n> Track: `dsa` | Topic: `string`\n\n## Concept\n\nReturn the index of the first match, or -1 if not found.\n\n## Function\n\n```python\nclass Solution:\n    def strStr(self, haystack: str, needle: str) -> int:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/string/28-implement-strstr/python/implement_strstr.py",
          "language": "python",
          "content": "class Solution:\n    def strStr(self, haystack: str, needle: str) -> int:\n        if needle == \"\":\n            return 0\n        n, m = len(haystack), len(needle)\n        if m > n:\n            return -1\n        for i in range(n - m + 1):\n            if haystack[i : i + m] == needle:\n                return i\n        return -1\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "string",
      "slug": "344-reverse-string",
      "title": "344.Reverse String",
      "path": "modules/dsa/string/344-reverse-string",
      "summary": "> Track: `dsa` | Topic: `string`",
      "readme": "# 344.Reverse String\n\n> Track: `dsa` | Topic: `string`\n\n## Concept\n\nReverse characters with slicing (or two pointers).\n\n## Function\n\n```python\nclass Solution:\n    def reverseString(self, s: List[str]) -> None:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/string/344-reverse-string/python/reverse_string.py",
          "language": "python",
          "content": "from typing import List\n\nclass Solution:\n    def reverseString(self, s: List[str]) -> None:\n        left, right = 0, len(s) - 1\n        while left < right:\n            s[left], s[right] = s[right], s[left]\n            left += 1\n            right -= 1\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "string",
      "slug": "459-repeated-substring-pattern",
      "title": "459.Repeated Substring Pattern",
      "path": "modules/dsa/string/459-repeated-substring-pattern",
      "summary": "> Track: `dsa` | Topic: `string`",
      "readme": "# 459.Repeated Substring Pattern\n\n> Track: `dsa` | Topic: `string`\n\n## Concept\n\nA string is repeating if it appears inside (s + s)[1:-1].\n\n## Function\n\n```python\nclass Solution:\n    def repeatedSubstringPattern(self, s: str) -> bool:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/string/459-repeated-substring-pattern/python/repeated_substring_pattern.py",
          "language": "python",
          "content": "class Solution:\n    def repeatedSubstringPattern(self, s: str) -> bool:\n        if not s:\n            return False\n        doubled = s + s\n        return s in doubled[1:-1]\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "string",
      "slug": "541-reverse-string-ii",
      "title": "541.Reverse String II",
      "path": "modules/dsa/string/541-reverse-string-ii",
      "summary": "> Track: `dsa` | Topic: `string`",
      "readme": "# 541.Reverse String II\n\n> Track: `dsa` | Topic: `string`\n\n## Concept\n\nReverse the first k characters in each 2k block.\n\n## Function\n\n```python\nclass Solution:\n    def reverseStr(self, s: str, k: int) -> str:\n```\n",
      "sources": [
        {
          "path": "modules/dsa/string/541-reverse-string-ii/python/reverse_string_ii.py",
          "language": "python",
          "content": "class Solution:\n    def reverseStr(self, s: str, k: int) -> str:\n        chars = list(s)\n        step = 2 * k\n        for start in range(0, len(chars), step):\n            i = start\n            j = min(start + k - 1, len(chars) - 1)\n            while i < j:\n                chars[i], chars[j] = chars[j], chars[i]\n                i += 1\n                j -= 1\n        return \"\".join(chars)\n"
        }
      ]
    },
    {
      "track": "dsa",
      "topic": "string",
      "slug": "string-summary",
      "title": "String Summary",
      "path": "modules/dsa/string/string-summary",
      "summary": "> Track: `dsa` | Topic: `string`",
      "readme": "# String Summary\n\n> Track: `dsa` | Topic: `string`\n\n## Concept\n\nReview string patterns: reversal, two pointers, split/join, and substring search.\n",
      "sources": []
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "alexnet",
      "title": "AlexNet",
      "path": "modules/ml/computer-vision/alexnet",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# AlexNet\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nAlexNet is a classic CNN architecture with characteristic layer design.\n\n## Math\n\n$$\\text{Architecture defined by ordered layers.}$$\n\n## Function\n\n```python\ndef layers() -> list[str]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/alexnet/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/alexnet/python/alexnet.py",
          "language": "python",
          "content": "def layers() -> list[str]:\n    return [\"conv\", \"relu\", \"pool\", \"fc\"]\n"
        },
        {
          "path": "modules/ml/computer-vision/alexnet/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn layers() -> Vec<&'static str> {\n    vec![\"conv\", \"relu\", \"pool\", \"fc\"]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "bilinear-resizing",
      "title": "Bilinear Resizing",
      "path": "modules/ml/computer-vision/bilinear-resizing",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Bilinear Resizing\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nBilinear interpolation blends four neighbors.\n\n## Math\n\n$$f(x,y) = \\sum_{i,j} w_{ij} v_{ij}$$\n\n## Function\n\n```python\ndef bilinear_sample(v00: float, v01: float, v10: float, v11: float, tx: float, ty: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/bilinear-resizing/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/bilinear-resizing/python/bilinear_resizing.py",
          "language": "python",
          "content": "def bilinear_sample(v00: float, v01: float, v10: float, v11: float, tx: float, ty: float) -> float:\n    a = v00 * (1 - tx) + v10 * tx\n    b = v01 * (1 - tx) + v11 * tx\n    return a * (1 - ty) + b * ty\n"
        },
        {
          "path": "modules/ml/computer-vision/bilinear-resizing/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn bilinear_sample(v00: f64, v01: f64, v10: f64, v11: f64, tx: f64, ty: f64) -> f64 {\n    let a = v00 * (1.0 - tx) + v10 * tx;\n    let b = v01 * (1.0 - tx) + v11 * tx;\n    a * (1.0 - ty) + b * ty\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "cnn-2d-vs-3d",
      "title": "2D vs 3D CNN",
      "path": "modules/ml/computer-vision/cnn-2d-vs-3d",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# 2D vs 3D CNN\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\n3D CNNs convolve over time/depth in addition to height/width.\n\n## Math\n\n$$output_depth = input_depth - kernel_depth + 1$$\n\n## Function\n\n```python\ndef output_depth(input_depth: int, kernel_depth: int) -> int:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/cnn-2d-vs-3d/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/cnn-2d-vs-3d/python/cnn_2d_vs_3d.py",
          "language": "python",
          "content": "def output_depth(input_depth: int, kernel_depth: int) -> int:\n    return input_depth - kernel_depth + 1\n"
        },
        {
          "path": "modules/ml/computer-vision/cnn-2d-vs-3d/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn output_depth(input_depth: i32, kernel_depth: i32) -> i32 {\n    input_depth - kernel_depth + 1\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "cnn-basics",
      "title": "CNNs",
      "path": "modules/ml/computer-vision/cnn-basics",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# CNNs\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nCNNs use convolutional filters to capture local patterns.\n\n## Math\n\n$$feature_map = conv(input, kernel)$$\n\n## Function\n\n```python\ndef conv1d(signal: list[float], kernel: list[float]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/cnn-basics/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/cnn-basics/python/cnn_basics.py",
          "language": "python",
          "content": "def conv1d(signal: list[float], kernel: list[float]) -> list[float]:\n    out = []\n    k = len(kernel)\n    for i in range(len(signal) - k + 1):\n        out.append(sum(signal[i + j] * kernel[j] for j in range(k)))\n    return out\n"
        },
        {
          "path": "modules/ml/computer-vision/cnn-basics/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn conv1d(signal: &[f64], kernel: &[f64]) -> Vec<f64> {\n    let k = kernel.len();\n    let mut out = Vec::new();\n    for i in 0..=signal.len() - k {\n        let mut acc = 0.0;\n        for j in 0..k {\n            acc += signal[i + j] * kernel[j];\n        }\n        out.push(acc);\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "contrast-brightness",
      "title": "Contrast and Brightness",
      "path": "modules/ml/computer-vision/contrast-brightness",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Contrast and Brightness\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nAdjust brightness and contrast via linear transform.\n\n## Math\n\n$$x' = a x + b$$\n\n## Function\n\n```python\ndef adjust(x: float, a: float, b: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/contrast-brightness/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/contrast-brightness/python/contrast_brightness.py",
          "language": "python",
          "content": "def adjust(x: float, a: float, b: float) -> float:\n    return a * x + b\n"
        },
        {
          "path": "modules/ml/computer-vision/contrast-brightness/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn adjust(x: f64, a: f64, b: f64) -> f64 {\n    a * x + b\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "convolution-layer",
      "title": "Convolution Layer",
      "path": "modules/ml/computer-vision/convolution-layer",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Convolution Layer\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nA convolution layer slides a kernel over the input.\n\n## Math\n\n$$(I * K)_{i,j} = \\sum_{u,v} K_{u,v} I_{i+u, j+v}$$\n\n## Function\n\n```python\ndef conv2d(image: list[list[float]], kernel: list[list[float]]) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/convolution-layer/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/convolution-layer/python/convolution_layer.py",
          "language": "python",
          "content": "def conv2d(image: list[list[float]], kernel: list[list[float]]) -> list[list[float]]:\n    h = len(image)\n    w = len(image[0])\n    kh = len(kernel)\n    kw = len(kernel[0])\n    out = []\n    for i in range(h - kh + 1):\n        row = []\n        for j in range(w - kw + 1):\n            val = 0.0\n            for u in range(kh):\n                for v in range(kw):\n                    val += image[i + u][j + v] * kernel[u][v]\n            row.append(val)\n        out.append(row)\n    return out\n"
        },
        {
          "path": "modules/ml/computer-vision/convolution-layer/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn conv2d(image: &[Vec<f64>], kernel: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let h = image.len();\n    let w = image[0].len();\n    let kh = kernel.len();\n    let kw = kernel[0].len();\n    let mut out = Vec::new();\n    for i in 0..=h - kh {\n        let mut row = Vec::new();\n        for j in 0..=w - kw {\n            let mut val = 0.0;\n            for u in 0..kh {\n                for v in 0..kw {\n                    val += image[i + u][j + v] * kernel[u][v];\n                }\n            }\n            row.push(val);\n        }\n        out.push(row);\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "data-augmentation",
      "title": "Data Augmentation",
      "path": "modules/ml/computer-vision/data-augmentation",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Data Augmentation\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nAugmentation applies random transforms like flips.\n\n## Math\n\n$$x' = flip(x)$$\n\n## Function\n\n```python\ndef horizontal_flip(image: list[list[int]]) -> list[list[int]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/data-augmentation/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/data-augmentation/python/data_augmentation.py",
          "language": "python",
          "content": "def horizontal_flip(image: list[list[int]]) -> list[list[int]]:\n    return [list(reversed(row)) for row in image]\n"
        },
        {
          "path": "modules/ml/computer-vision/data-augmentation/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn horizontal_flip(image: &[Vec<i32>]) -> Vec<Vec<i32>> {\n    let mut out = Vec::new();\n    for row in image {\n        let mut r = row.clone();\n        r.reverse();\n        out.push(r);\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "image-preprocessing",
      "title": "Image Preprocessing",
      "path": "modules/ml/computer-vision/image-preprocessing",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Image Preprocessing\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nPreprocessing normalizes pixel ranges for stable training.\n\n## Math\n\n$$x' = x / 255$$\n\n## Function\n\n```python\ndef normalize_pixels(pixels: list[int]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/image-preprocessing/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/image-preprocessing/python/image_preprocessing.py",
          "language": "python",
          "content": "def normalize_pixels(pixels: list[int]) -> list[float]:\n    return [p / 255.0 for p in pixels]\n"
        },
        {
          "path": "modules/ml/computer-vision/image-preprocessing/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn normalize_pixels(pixels: &[i32]) -> Vec<f64> {\n    pixels.iter().map(|p| *p as f64 / 255.0).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "lenet-5",
      "title": "LeNet-5",
      "path": "modules/ml/computer-vision/lenet-5",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# LeNet-5\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nLeNet-5 is a classic CNN architecture with characteristic layer design.\n\n## Math\n\n$$\\text{Architecture defined by ordered layers.}$$\n\n## Function\n\n```python\ndef layers() -> list[str]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/lenet-5/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/lenet-5/python/lenet_5.py",
          "language": "python",
          "content": "def layers() -> list[str]:\n    return [\"conv\", \"relu\", \"pool\", \"fc\"]\n"
        },
        {
          "path": "modules/ml/computer-vision/lenet-5/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn layers() -> Vec<&'static str> {\n    vec![\"conv\", \"relu\", \"pool\", \"fc\"]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "non-maximum-suppression",
      "title": "Non-Maximum Suppression",
      "path": "modules/ml/computer-vision/non-maximum-suppression",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Non-Maximum Suppression\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nNMS removes overlapping boxes with lower scores.\n\n## Math\n\n$$Keep highest score; suppress IoU > threshold.$$\n\n## Function\n\n```python\ndef iou(box_a: tuple[float, float, float, float], box_b: tuple[float, float, float, float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/non-maximum-suppression/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/non-maximum-suppression/python/non_maximum_suppression.py",
          "language": "python",
          "content": "def iou(box_a: tuple[float, float, float, float], box_b: tuple[float, float, float, float]) -> float:\n    ax1, ay1, ax2, ay2 = box_a\n    bx1, by1, bx2, by2 = box_b\n    inter_x1 = max(ax1, bx1)\n    inter_y1 = max(ay1, by1)\n    inter_x2 = min(ax2, bx2)\n    inter_y2 = min(ay2, by2)\n    if inter_x2 <= inter_x1 or inter_y2 <= inter_y1:\n        return 0.0\n    inter = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)\n    area_a = (ax2 - ax1) * (ay2 - ay1)\n    area_b = (bx2 - bx1) * (by2 - by1)\n    return inter / (area_a + area_b - inter)\n"
        },
        {
          "path": "modules/ml/computer-vision/non-maximum-suppression/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn iou(a: (f64, f64, f64, f64), b: (f64, f64, f64, f64)) -> f64 {\n    let (ax1, ay1, ax2, ay2) = a;\n    let (bx1, by1, bx2, by2) = b;\n    let inter_x1 = ax1.max(bx1);\n    let inter_y1 = ay1.max(by1);\n    let inter_x2 = ax2.min(bx2);\n    let inter_y2 = ay2.min(by2);\n    if inter_x2 <= inter_x1 || inter_y2 <= inter_y1 {\n        return 0.0;\n    }\n    let inter = (inter_x2 - inter_x1) * (inter_y2 - inter_y1);\n    let area_a = (ax2 - ax1) * (ay2 - ay1);\n    let area_b = (bx2 - bx1) * (by2 - by1);\n    inter / (area_a + area_b - inter)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "optical-flow-epe",
      "title": "Optical Flow (EPE)",
      "path": "modules/ml/computer-vision/optical-flow-epe",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Optical Flow (EPE)\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nEndpoint error measures flow prediction error.\n\n## Math\n\n$$\\mathrm{EPE} = \\sqrt{(u-u^*)^2 + (v-v^*)^2}$$\n\n## Function\n\n```python\ndef epe(pred: tuple[float, float], target: tuple[float, float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/optical-flow-epe/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/optical-flow-epe/python/optical_flow_epe.py",
          "language": "python",
          "content": "def epe(pred: tuple[float, float], target: tuple[float, float]) -> float:\n    return ((pred[0] - target[0]) ** 2 + (pred[1] - target[1]) ** 2) ** 0.5\n"
        },
        {
          "path": "modules/ml/computer-vision/optical-flow-epe/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn epe(pred: (f64, f64), target: (f64, f64)) -> f64 {\n    let dx = pred.0 - target.0;\n    let dy = pred.1 - target.1;\n    (dx * dx + dy * dy).sqrt()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "pooling",
      "title": "Pooling (Max/Average)",
      "path": "modules/ml/computer-vision/pooling",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Pooling (Max/Average)\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nPooling downsamples by taking max or average over windows.\n\n## Math\n\n$$\n\\begin{aligned}\ny_{\\max} &= \\max_{i \\in \\mathcal{W}} x_i \\\\\ny_{\\text{avg}} &= \\frac{1}{|\\mathcal{W}|}\\sum_{i \\in \\mathcal{W}} x_i\n\\end{aligned}\n$$\n\n## Function\n\n```python\ndef max_pool(window: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/pooling/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/pooling/python/pooling.py",
          "language": "python",
          "content": "def max_pool(window: list[float]) -> float:\n    return max(window)\n\n\ndef avg_pool(window: list[float]) -> float:\n    return sum(window) / len(window)\n"
        },
        {
          "path": "modules/ml/computer-vision/pooling/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn max_pool(window: &[f64]) -> f64 {\n    window.iter().copied().fold(f64::NEG_INFINITY, f64::max)\n}\n\npub fn avg_pool(window: &[f64]) -> f64 {\n    window.iter().sum::<f64>() / window.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "resnet",
      "title": "ResNet",
      "path": "modules/ml/computer-vision/resnet",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# ResNet\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nResNet is a classic CNN architecture with characteristic layer design.\n\n## Math\n\n$$\\text{Architecture defined by ordered layers.}$$\n\n## Function\n\n```python\ndef layers() -> list[str]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/resnet/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/resnet/python/resnet.py",
          "language": "python",
          "content": "def layers() -> list[str]:\n    return [\"conv\", \"relu\", \"pool\", \"fc\"]\n"
        },
        {
          "path": "modules/ml/computer-vision/resnet/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn layers() -> Vec<&'static str> {\n    vec![\"conv\", \"relu\", \"pool\", \"fc\"]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "rgb-to-grayscale",
      "title": "RGB to Grayscale",
      "path": "modules/ml/computer-vision/rgb-to-grayscale",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# RGB to Grayscale\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nConvert RGB to grayscale using luminance weights.\n\n## Math\n\n$$Y = 0.299R + 0.587G + 0.114B$$\n\n## Function\n\n```python\ndef rgb_to_gray(r: int, g: int, b: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/rgb-to-grayscale/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/rgb-to-grayscale/python/rgb_to_grayscale.py",
          "language": "python",
          "content": "def rgb_to_gray(r: int, g: int, b: int) -> float:\n    return 0.299 * r + 0.587 * g + 0.114 * b\n"
        },
        {
          "path": "modules/ml/computer-vision/rgb-to-grayscale/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn rgb_to_gray(r: i32, g: i32, b: i32) -> f64 {\n    0.299 * r as f64 + 0.587 * g as f64 + 0.114 * b as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "sobel-edge-detection",
      "title": "Sobel Edge Detection",
      "path": "modules/ml/computer-vision/sobel-edge-detection",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# Sobel Edge Detection\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nSobel filters estimate image gradients.\n\n## Math\n\n$$Gx = Kx * I, Gy = Ky * I$$\n\n## Function\n\n```python\ndef sobel_center(patch: list[list[float]]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/sobel-edge-detection/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/sobel-edge-detection/python/sobel_edge_detection.py",
          "language": "python",
          "content": "def sobel_center(patch: list[list[float]]) -> float:\n    gx = (\n        -1 * patch[0][0] + 0 * patch[0][1] + 1 * patch[0][2]\n        -2 * patch[1][0] + 0 * patch[1][1] + 2 * patch[1][2]\n        -1 * patch[2][0] + 0 * patch[2][1] + 1 * patch[2][2]\n    )\n    gy = (\n        -1 * patch[0][0] -2 * patch[0][1] -1 * patch[0][2]\n         0 * patch[1][0] +0 * patch[1][1] +0 * patch[1][2]\n         1 * patch[2][0] +2 * patch[2][1] +1 * patch[2][2]\n    )\n    return (gx ** 2 + gy ** 2) ** 0.5\n"
        },
        {
          "path": "modules/ml/computer-vision/sobel-edge-detection/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn sobel_center(patch: &[Vec<f64>]) -> f64 {\n    let gx = -1.0 * patch[0][0] + 1.0 * patch[0][2]\n        -2.0 * patch[1][0] + 2.0 * patch[1][2]\n        -1.0 * patch[2][0] + 1.0 * patch[2][2];\n    let gy = -1.0 * patch[0][0] - 2.0 * patch[0][1] - 1.0 * patch[0][2]\n        + 1.0 * patch[2][0] + 2.0 * patch[2][1] + 1.0 * patch[2][2];\n    (gx * gx + gy * gy).sqrt()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "vggnet",
      "title": "VGGNet",
      "path": "modules/ml/computer-vision/vggnet",
      "summary": "> Track: `ml` | Topic: `computer-vision`",
      "readme": "# VGGNet\n\n> Track: `ml` | Topic: `computer-vision`\n\n## Concept\n\nVGGNet is a classic CNN architecture with characteristic layer design.\n\n## Math\n\n$$\\text{Architecture defined by ordered layers.}$$\n\n## Function\n\n```python\ndef layers() -> list[str]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/computer-vision/vggnet/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/computer-vision/vggnet/python/vggnet.py",
          "language": "python",
          "content": "def layers() -> list[str]:\n    return [\"conv\", \"relu\", \"pool\", \"fc\"]\n"
        },
        {
          "path": "modules/ml/computer-vision/vggnet/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn layers() -> Vec<&'static str> {\n    vec![\"conv\", \"relu\", \"pool\", \"fc\"]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "data",
      "slug": "batch-iterator",
      "title": "Batch Iterator",
      "path": "modules/ml/data/batch-iterator",
      "summary": "> Track: `ml` | Topic: `data`",
      "readme": "# Batch Iterator\n\n> Track: `ml` | Topic: `data`\n\n## Concept\n\nMini-batch iteration is the process of shuffling a dataset and yielding fixed-size chunks of indices so that each chunk can be used as one gradient-update step. Stochastic gradient descent (SGD) relies on random mini-batches rather than the full dataset because smaller, randomized subsets reduce per-step computation while still providing an unbiased estimate of the true gradient.\n\nShuffling the data before each epoch ensures the model does not see examples in the same order repeatedly, which could introduce ordering bias and cause the optimizer to cycle through a fixed trajectory. After shuffling, the iterator slices the index sequence into consecutive blocks of size $B$, where the last block may contain fewer than $B$ elements if $N$ is not evenly divisible by $B$.\n\n## Math\n\n$$\\text{number of batches} = \\left\\lceil \\frac{N}{B} \\right\\rceil$$\n\n$$b_i = \\left[ iB, \\; \\min\\!\\left((i+1)B, \\; N\\right) \\right)$$\n\n- $N$ -- total number of samples in the dataset\n- $B$ -- batch size (number of samples per mini-batch)\n- $b_i$ -- index range for the $i$-th batch\n\n## Key Points\n\n- The last batch may be smaller than $B$ when $N$ is not divisible by $B$.\n- Shuffling before each epoch prevents ordering bias and improves convergence.\n- Batch size directly affects gradient variance: smaller batches give noisier but more frequent updates, larger batches give smoother but fewer updates.\n- Setting $B = 1$ recovers pure SGD; setting $B = N$ recovers full-batch gradient descent.\n\n## Function\n\n```python\ndef batch_indices(n: int, batch_size: int) -> list[list[int]]:\n```\n\n- `n` -- total number of samples in the dataset\n- `batch_size` -- number of samples in each mini-batch\n\n## Run tests\n\n```bash\npytest modules/ml/data/batch-iterator/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/data/batch-iterator/python/batch_iterator.py",
          "language": "python",
          "content": "def batch_indices(n: int, batch_size: int) -> list[list[int]]:\n    batches = []\n    for start in range(0, n, batch_size):\n        batches.append(list(range(start, min(n, start + batch_size))))\n    return batches\n"
        },
        {
          "path": "modules/ml/data/batch-iterator/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn batch_indices(n: usize, batch_size: usize) -> Vec<Vec<usize>> {\n    let mut batches = Vec::new();\n    let mut start = 0;\n    while start < n {\n        let end = usize::min(n, start + batch_size);\n        batches.push((start..end).collect());\n        start += batch_size;\n    }\n    batches\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "data",
      "slug": "class-imbalance",
      "title": "Handling Class Imbalance",
      "path": "modules/ml/data/class-imbalance",
      "summary": "> Track: `ml` | Topic: `data`",
      "readme": "# Handling Class Imbalance\n\n> Track: `ml` | Topic: `data`\n\n## Concept\n\nClass imbalance occurs when some classes appear far more frequently than others in a dataset. In such settings a model can achieve high overall accuracy by simply predicting the majority class for every input, which makes raw accuracy a misleading metric. The minority class, often the class of greatest interest (e.g., fraud detection, rare disease diagnosis), is effectively ignored by the model.\n\nReweighting the loss function is one of the most straightforward remedies. By assigning each class a weight inversely proportional to its frequency, the contribution of rare-class samples to the total loss is amplified, encouraging the model to learn meaningful decision boundaries for every class. Alternative strategies include oversampling the minority class, undersampling the majority class, or generating synthetic samples (e.g., SMOTE), each with different trade-offs in variance and computational cost.\n\n## Math\n\n$$w_c = \\frac{N}{C \\cdot N_c}$$\n\n- $w_c$ -- weight assigned to class $c$\n- $N$ -- total number of samples\n- $C$ -- total number of classes\n- $N_c$ -- number of samples belonging to class $c$\n\n## Key Points\n\n- Accuracy is misleading under class imbalance; prefer metrics like precision, recall, F1, or AUC.\n- Reweighting the loss is simple and effective but does not add new information; oversampling can help the model see more minority examples at the cost of potential overfitting.\n- Undersampling discards majority-class data, which may hurt performance when the dataset is already small.\n- A perfectly balanced dataset yields $w_c = 1$ for every class.\n\n## Function\n\n```python\ndef class_weights(labels: list[int]) -> dict[int, float]:\n```\n\n- `labels` -- list of integer class labels for every sample in the dataset\n\n## Run tests\n\n```bash\npytest modules/ml/data/class-imbalance/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/data/class-imbalance/python/class_imbalance.py",
          "language": "python",
          "content": "from collections import Counter\n\n\ndef class_weights(labels: list[int]) -> dict[int, float]:\n    counts = Counter(labels)\n    n = len(labels)\n    k = len(counts)\n    return {cls: n / (k * cnt) for cls, cnt in counts.items()}\n"
        },
        {
          "path": "modules/ml/data/class-imbalance/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashMap;\n\npub fn class_weights(labels: &[i32]) -> HashMap<i32, f64> {\n    let mut counts: HashMap<i32, usize> = HashMap::new();\n    for label in labels {\n        *counts.entry(*label).or_insert(0) += 1;\n    }\n    let n = labels.len() as f64;\n    let k = counts.len() as f64;\n    let mut weights = HashMap::new();\n    for (cls, cnt) in counts {\n        weights.insert(cls, n / (k * cnt as f64));\n    }\n    weights\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "data",
      "slug": "data-leakage",
      "title": "Data Leakage",
      "path": "modules/ml/data/data-leakage",
      "summary": "> Track: `ml` | Topic: `data`",
      "readme": "# Data Leakage\n\n> Track: `ml` | Topic: `data`\n\n## Concept\n\nData leakage occurs when information from outside the training distribution contaminates the model during training, leading to overly optimistic performance estimates that do not generalize to production. The most common forms are train/test overlap (the same sample appears in both sets), temporal leakage (using future data to predict the past), and preprocessing leakage (fitting a scaler or encoder on the full dataset before splitting).\n\nLeakage is dangerous precisely because it is invisible at training time -- the model appears to perform well on the test set, but fails once deployed on truly unseen data. The fundamental safeguard is to ensure that train and test sets are strictly disjoint and that every preprocessing step is fitted only on the training partition. Feature leakage, where a feature is derived from or strongly correlated with the target, is another subtle variant that can inflate metrics without providing real predictive power.\n\n## Math\n\n$$\\text{Train} \\cap \\text{Test} = \\emptyset$$\n\n$$|\\text{Train}| + |\\text{Test}| \\leq N$$\n\n- $\\text{Train}$ -- set of sample identifiers used for training\n- $\\text{Test}$ -- set of sample identifiers reserved for evaluation\n- $N$ -- total number of samples in the dataset\n\n## Key Points\n\n- Always split the data before any preprocessing (scaling, encoding, imputation) to avoid information leaking from test into train.\n- In time-series problems, use temporal splits rather than random splits to prevent future data from influencing past predictions.\n- Feature leakage from target encoding or proxy variables can be as harmful as direct sample overlap.\n- A simple overlap check ($|\\text{Train} \\cap \\text{Test}| > 0$) catches the most obvious form of leakage.\n\n## Function\n\n```python\ndef has_leakage(train_ids: list[int], test_ids: list[int]) -> bool:\n```\n\n- `train_ids` -- list of sample identifiers in the training set\n- `test_ids` -- list of sample identifiers in the test set\n\n## Run tests\n\n```bash\npytest modules/ml/data/data-leakage/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/data/data-leakage/python/data_leakage.py",
          "language": "python",
          "content": "def has_leakage(train_ids: list[int], test_ids: list[int]) -> bool:\n    return bool(set(train_ids) & set(test_ids))\n"
        },
        {
          "path": "modules/ml/data/data-leakage/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashSet;\n\npub fn has_leakage(train_ids: &[i32], test_ids: &[i32]) -> bool {\n    let train: HashSet<i32> = train_ids.iter().copied().collect();\n    test_ids.iter().any(|id| train.contains(id))\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "data",
      "slug": "dataset-batch-epoch",
      "title": "Dataset vs Batch vs Epoch",
      "path": "modules/ml/data/dataset-batch-epoch",
      "summary": "> Track: `ml` | Topic: `data`",
      "readme": "# Dataset vs Batch vs Epoch\n\n> Track: `ml` | Topic: `data`\n\n## Concept\n\nA dataset is the full collection of samples available for training. A batch (or mini-batch) is a subset of the dataset used in a single parameter update step. An epoch is one complete pass through the entire dataset. These three concepts define the rhythm of training: within each epoch the data is divided into batches, and the model parameters are updated once per batch.\n\nThe choice of batch size $B$ is a key hyperparameter that affects both optimization dynamics and hardware utilization. Smaller batches inject noise into gradient estimates, which can act as implicit regularization and help escape sharp minima. Larger batches give more accurate gradients and better GPU throughput, but training for too many epochs risks overfitting as the model begins to memorize the training data rather than learning general patterns.\n\n## Math\n\n$$\\text{updates per epoch} = \\left\\lceil \\frac{N}{B} \\right\\rceil$$\n\n$$\\text{total updates} = E \\cdot \\left\\lceil \\frac{N}{B} \\right\\rceil$$\n\n- $N$ -- number of samples in the dataset\n- $B$ -- batch size\n- $E$ -- number of epochs\n\n## Key Points\n\n- More epochs allow the model to see every sample multiple times, but too many epochs risk overfitting.\n- Batch size is a critical hyperparameter: it controls the trade-off between gradient noise and computational efficiency.\n- One training step (weight update) corresponds to one batch, not one epoch.\n- Common batch sizes are powers of two (32, 64, 128, 256) to align with GPU memory architecture.\n\n## Function\n\n```python\ndef num_batches(dataset_size: int, batch_size: int) -> int:\n```\n\n- `dataset_size` -- total number of samples in the dataset ($N$)\n- `batch_size` -- number of samples per batch ($B$)\n\n## Run tests\n\n```bash\npytest modules/ml/data/dataset-batch-epoch/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/data/dataset-batch-epoch/python/dataset_batch_epoch.py",
          "language": "python",
          "content": "import math\n\n\ndef num_batches(dataset_size: int, batch_size: int) -> int:\n    return math.ceil(dataset_size / batch_size)\n"
        },
        {
          "path": "modules/ml/data/dataset-batch-epoch/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn num_batches(dataset_size: usize, batch_size: usize) -> usize {\n    (dataset_size + batch_size - 1) / batch_size\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "data",
      "slug": "polynomial-features",
      "title": "Polynomial Feature Expansion",
      "path": "modules/ml/data/polynomial-features",
      "summary": "> Track: `ml` | Topic: `data`",
      "readme": "# Polynomial Feature Expansion\n\n> Track: `ml` | Topic: `data`\n\n## Concept\n\nPolynomial feature expansion creates higher-order terms (powers and interactions) from the original input features, enabling linear models to capture nonlinear relationships. For a single feature $x$ and degree $d$, the expansion produces the vector $[x, x^2, \\ldots, x^d]$. For multiple features, it also generates all cross-product interaction terms up to the specified degree.\n\nThis technique is valuable because it allows the well-understood and computationally efficient linear regression framework to fit curves, surfaces, and complex decision boundaries. However, the number of generated features grows combinatorially with the number of input features and the degree, which can lead to overfitting and high computational cost. Pairing polynomial expansion with regularization (e.g., Ridge or Lasso) is essential for controlling model complexity.\n\n## Math\n\nFor a single feature $x$ with degree $d$:\n\n$$\\phi(x) = \\bigl[x, \\; x^2, \\; \\ldots, \\; x^d\\bigr]$$\n\nFor two features $x_1, x_2$ with degree $d = 2$:\n\n$$\\phi(x_1, x_2) = \\bigl[1, \\; x_1, \\; x_2, \\; x_1^2, \\; x_1 x_2, \\; x_2^2\\bigr]$$\n\nThe total number of features for $p$ original features and degree $d$ (including the bias term) is:\n\n$$\\binom{p + d}{d}$$\n\n- $x$ -- original input feature\n- $d$ -- maximum polynomial degree\n- $p$ -- number of original input features\n- $\\phi$ -- feature mapping from the original space to the expanded space\n\n## Key Points\n\n- Feature count grows combinatorially: with $p = 10$ features and $d = 3$, the expansion produces $\\binom{13}{3} = 286$ features.\n- Always pair polynomial expansion with regularization to prevent overfitting in the high-dimensional feature space.\n- For a single feature, polynomial expansion is equivalent to fitting a degree-$d$ polynomial curve.\n- Interaction terms (e.g., $x_1 x_2$) capture relationships between features that pure power terms miss.\n\n## Function\n\n```python\ndef poly_features(x: float, degree: int) -> list[float]:\n```\n\n- `x` -- scalar input value to expand\n- `degree` -- maximum polynomial degree $d$\n\n## Run tests\n\n```bash\npytest modules/ml/data/polynomial-features/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/data/polynomial-features/python/polynomial_features.py",
          "language": "python",
          "content": "def poly_features(x: float, degree: int) -> list[float]:\n    return [x ** d for d in range(1, degree + 1)]\n"
        },
        {
          "path": "modules/ml/data/polynomial-features/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn poly_features(x: f64, degree: usize) -> Vec<f64> {\n    (1..=degree).map(|d| x.powi(d as i32)).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "data",
      "slug": "stratified-split",
      "title": "Stratified Split",
      "path": "modules/ml/data/stratified-split",
      "summary": "> Track: `ml` | Topic: `data`",
      "readme": "# Stratified Split\n\n> Track: `ml` | Topic: `data`\n\n## Concept\n\nA stratified split divides a dataset into subsets (e.g., train and test) while preserving the original class proportions in each subset. In a standard random split, rare classes may end up severely under-represented or even entirely absent from one of the partitions, leading to biased training or unreliable evaluation. Stratification guarantees that every class is represented in roughly the same ratio as in the full dataset.\n\nThis technique is especially critical for imbalanced datasets where some classes contain only a handful of samples. Without stratification, a small test set might contain zero examples of a minority class, making it impossible to evaluate performance on that class. Scikit-learn's `train_test_split` supports stratification directly through the `stratify` parameter.\n\n## Math\n\nFor each class $c$ with a train fraction $f$:\n\n$$\\frac{|S_c^{\\text{train}}|}{|S_c|} \\approx f$$\n\n$$\\frac{|S_c^{\\text{test}}|}{|S_c|} \\approx 1 - f$$\n\n- $S_c$ -- set of all indices belonging to class $c$\n- $S_c^{\\text{train}}$ -- subset of class-$c$ indices assigned to the training set\n- $f$ -- desired fraction of data allocated to training\n\n## Key Points\n\n- Stratification is most important when classes are rare; with balanced classes, random splitting is often sufficient.\n- Each class is split independently, so the overall train fraction is maintained per class, not just globally.\n- In scikit-learn, pass `stratify=y` to `train_test_split` to enable stratified splitting.\n- For very small classes, even stratification may leave too few samples for reliable evaluation; consider cross-validation in such cases.\n\n## Function\n\n```python\ndef stratified_split(labels: list[int], train_frac: float) -> tuple[list[int], list[int]]:\n```\n\n- `labels` -- list of integer class labels for each sample\n- `train_frac` -- fraction of samples to assign to the training set ($f$)\n\n## Run tests\n\n```bash\npytest modules/ml/data/stratified-split/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/data/stratified-split/python/stratified_split.py",
          "language": "python",
          "content": "from collections import defaultdict\n\n\ndef stratified_split(labels: list[int], train_frac: float) -> tuple[list[int], list[int]]:\n    buckets = defaultdict(list)\n    for idx, label in enumerate(labels):\n        buckets[label].append(idx)\n    train, test = [], []\n    for idxs in buckets.values():\n        cut = int(len(idxs) * train_frac)\n        train.extend(idxs[:cut])\n        test.extend(idxs[cut:])\n    return train, test\n"
        },
        {
          "path": "modules/ml/data/stratified-split/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashMap;\n\npub fn stratified_split(labels: &[i32], train_frac: f64) -> (Vec<usize>, Vec<usize>) {\n    let mut buckets: HashMap<i32, Vec<usize>> = HashMap::new();\n    for (idx, label) in labels.iter().enumerate() {\n        buckets.entry(*label).or_default().push(idx);\n    }\n    let mut train = Vec::new();\n    let mut test = Vec::new();\n    for idxs in buckets.values() {\n        let cut = (idxs.len() as f64 * train_frac) as usize;\n        train.extend_from_slice(&idxs[..cut]);\n        test.extend_from_slice(&idxs[cut..]);\n    }\n    (train, test)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "data",
      "slug": "train-validation-test-split",
      "title": "Train/Validation/Test Split",
      "path": "modules/ml/data/train-validation-test-split",
      "summary": "> Track: `ml` | Topic: `data`",
      "readme": "# Train/Validation/Test Split\n\n> Track: `ml` | Topic: `data`\n\n## Concept\n\nA three-way split partitions data into training, validation, and test sets, each serving a distinct purpose. The training set is used to fit model parameters, the validation set guides hyperparameter tuning and model selection, and the test set provides a final, unbiased estimate of generalization performance. This separation ensures that decisions made during development do not contaminate the evaluation.\n\nUsing the test set for tuning leaks information about the test distribution into the model selection process, producing optimistic performance estimates that fail to generalize. The validation set acts as a proxy for unseen data during development: you train multiple model configurations, evaluate each on the validation set, select the best, and only then report its performance on the held-out test set. The test set should ideally be touched only once.\n\n## Math\n\n$$n_{\\text{train}} = \\lfloor N \\cdot f_{\\text{train}} \\rfloor$$\n\n$$n_{\\text{val}} = \\lfloor N \\cdot f_{\\text{val}} \\rfloor$$\n\n$$n_{\\text{test}} = N - n_{\\text{train}} - n_{\\text{val}}$$\n\n- $N$ -- total number of samples\n- $f_{\\text{train}}$ -- fraction allocated to training (commonly $0.6$ -- $0.8$)\n- $f_{\\text{val}}$ -- fraction allocated to validation (commonly $0.1$ -- $0.2$)\n- $n_{\\text{train}}, n_{\\text{val}}, n_{\\text{test}}$ -- resulting partition sizes\n\n## Key Points\n\n- The validation set guides model selection and hyperparameter tuning without touching the test set.\n- The test set should be evaluated only once to provide an unbiased generalization estimate.\n- A common ratio is 60/20/20 or 80/10/10 for train/validation/test.\n- When data is scarce, $k$-fold cross-validation can replace a fixed validation set to make better use of limited samples.\n- All three partitions must be mutually disjoint: $\\text{Train} \\cap \\text{Val} = \\text{Val} \\cap \\text{Test} = \\text{Train} \\cap \\text{Test} = \\emptyset$.\n\n## Function\n\n```python\ndef split_indices(n: int, train_frac: float, val_frac: float) -> tuple[list[int], list[int], list[int]]:\n```\n\n- `n` -- total number of samples ($N$)\n- `train_frac` -- fraction of samples allocated to the training set ($f_{\\text{train}}$)\n- `val_frac` -- fraction of samples allocated to the validation set ($f_{\\text{val}}$)\n\n## Run tests\n\n```bash\npytest modules/ml/data/train-validation-test-split/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/data/train-validation-test-split/python/train_validation_test_split.py",
          "language": "python",
          "content": "def split_indices(n: int, train_frac: float, val_frac: float) -> tuple[list[int], list[int], list[int]]:\n    n_train = int(n * train_frac)\n    n_val = int(n * val_frac)\n    train = list(range(0, n_train))\n    val = list(range(n_train, n_train + n_val))\n    test = list(range(n_train + n_val, n))\n    return train, val, test\n"
        },
        {
          "path": "modules/ml/data/train-validation-test-split/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn split_indices(n: usize, train_frac: f64, val_frac: f64) -> (Vec<usize>, Vec<usize>, Vec<usize>) {\n    let n_train = (n as f64 * train_frac).floor() as usize;\n    let n_val = (n as f64 * val_frac).floor() as usize;\n    let train: Vec<usize> = (0..n_train).collect();\n    let val: Vec<usize> = (n_train..(n_train + n_val)).collect();\n    let test: Vec<usize> = (n_train + n_val..n).collect();\n    (train, val, test)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "activation-failure-modes",
      "title": "Activation Failure Modes",
      "path": "modules/ml/deep-learning/activation-failure-modes",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Activation Failure Modes\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nDead ReLU and saturation reduce gradient flow.\n\n## Math\n\n$$|\\sigma'(x)| \\approx 0 \\quad \\text{for large } |x|$$\n\n## Function\n\n```python\ndef dead_relu_fraction(values: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/activation-failure-modes/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/activation-failure-modes/python/activation_failure_modes.py",
          "language": "python",
          "content": "def dead_relu_fraction(values: list[float]) -> float:\n    if not values:\n        return 0.0\n    dead = sum(1 for v in values if v <= 0.0)\n    return dead / len(values)\n"
        },
        {
          "path": "modules/ml/deep-learning/activation-failure-modes/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn dead_relu_fraction(values: &[f64]) -> f64 {\n    if values.is_empty() {\n        return 0.0;\n    }\n    let dead = values.iter().filter(|v| **v <= 0.0).count();\n    dead as f64 / values.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "activations-modern",
      "title": "Modern Activations",
      "path": "modules/ml/deep-learning/activations-modern",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Modern Activations\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nModern activations like GeLU, Swish, SwiGLU, and Mish improve expressiveness.\n\n## Math\n\n$$\n\\begin{aligned}\n\\mathrm{Swish}(x) &= x\\,\\sigma(x) \\\\\n\\mathrm{GeLU}(x) &\\approx 0.5x\\left(1+\\tanh\\left(\\sqrt{\\frac{2}{\\pi}}\\left(x+0.044715x^3\\right)\\right)\\right)\n\\end{aligned}\n$$\n\n## Function\n\n```python\ndef modern_activations(x: float) -> dict[str, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/activations-modern/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/activations-modern/python/activations_modern.py",
          "language": "python",
          "content": "import math\n\n\ndef modern_activations(x: float) -> dict[str, float]:\n    sigmoid = 1 / (1 + math.exp(-x))\n    swish = x * sigmoid\n    gelu = 0.5 * x * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3)))\n    mish = x * math.tanh(math.log1p(math.exp(x)))\n    swiglu = (x * sigmoid) * x\n    return {\"swish\": swish, \"gelu\": gelu, \"mish\": mish, \"swiglu\": swiglu}\n"
        },
        {
          "path": "modules/ml/deep-learning/activations-modern/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn modern_activations(x: f64) -> (f64, f64, f64, f64) {\n    let sigmoid = 1.0 / (1.0 + (-x).exp());\n    let swish = x * sigmoid;\n    let gelu = 0.5 * x * (1.0 + ( (2.0 / std::f64::consts::PI).sqrt() * (x + 0.044715 * x.powi(3)) ).tanh());\n    let mish = x * (1.0 + x.exp()).ln().tanh();\n    let swiglu = (x * sigmoid) * x;\n    (swish, gelu, mish, swiglu)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "activations-relu-family",
      "title": "ReLU Family",
      "path": "modules/ml/deep-learning/activations-relu-family",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# ReLU Family\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nReLU variants improve gradient flow by keeping nonzero slopes.\n\n## Math\n\n$$\\mathrm{ReLU}(x)=\\max(0,x),\\ \\mathrm{LeakyReLU}(x)=\\max(\\alpha x,x)$$\n\n## Function\n\n```python\ndef relu_family(x: float, alpha: float = 0.01) -> dict[str, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/activations-relu-family/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/activations-relu-family/python/activations_relu_family.py",
          "language": "python",
          "content": "def relu_family(x: float, alpha: float = 0.01) -> dict[str, float]:\n    relu = max(0.0, x)\n    leaky = x if x > 0 else alpha * x\n    elu = x if x > 0 else alpha * (pow(2.718281828, x) - 1)\n    prelu = x if x > 0 else alpha * x\n    return {\"relu\": relu, \"leaky_relu\": leaky, \"elu\": elu, \"prelu\": prelu}\n"
        },
        {
          "path": "modules/ml/deep-learning/activations-relu-family/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn relu_family(x: f64, alpha: f64) -> (f64, f64, f64, f64) {\n    let relu = x.max(0.0);\n    let leaky = if x > 0.0 { x } else { alpha * x };\n    let elu = if x > 0.0 { x } else { alpha * (x.exp() - 1.0) };\n    let prelu = if x > 0.0 { x } else { alpha * x };\n    (relu, leaky, elu, prelu)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "activations-sigmoid-tanh",
      "title": "Sigmoid/Tanh and Hard Variants",
      "path": "modules/ml/deep-learning/activations-sigmoid-tanh",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Sigmoid/Tanh and Hard Variants\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nSigmoid and tanh are smooth activations; hard variants approximate them for efficiency.\nDynamic Tanh (DyT) adds learnable scale/bias parameters to tanh.\n\n## Math\n\n- $\\sigma(x)=\\frac{1}{1+e^{-x}}$\n- $\\tanh(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$\n- $\\mathrm{hard\\_sigmoid}(x)=\\operatorname{clip}(0.2x+0.5, 0, 1)$\n- $\\mathrm{hardtanh}(x)=\\operatorname{clip}(x, -1, 1)$\n- $\\mathrm{dyt}(x)=\\gamma\\,\\tanh(\\alpha x)+\\beta$\n\n## Function\n\n```python\ndef activations(x: float) -> dict[str, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/activations-sigmoid-tanh/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/activations-sigmoid-tanh/python/activations_sigmoid_tanh.py",
          "language": "python",
          "content": "import math\n\n\ndef dynamic_tanh(x: float, alpha: float = 1.0, gamma: float = 1.0, beta: float = 0.0) -> float:\n    \"\"\"Dynamic Tanh (DyT): gamma * tanh(alpha * x) + beta.\"\"\"\n    return gamma * math.tanh(alpha * x) + beta\n\n\ndef activations(x: float) -> dict[str, float]:\n    sigmoid = 1 / (1 + math.exp(-x))\n    tanh = math.tanh(x)\n    hard_sigmoid = max(0.0, min(1.0, 0.2 * x + 0.5))\n    hardtanh = max(-1.0, min(1.0, x))\n    dynamic = dynamic_tanh(x, alpha=1.2, gamma=1.0, beta=0.0)\n    return {\n        \"sigmoid\": sigmoid,\n        \"tanh\": tanh,\n        \"hard_sigmoid\": hard_sigmoid,\n        \"hardtanh\": hardtanh,\n        \"dynamic_tanh\": dynamic,\n    }\n"
        },
        {
          "path": "modules/ml/deep-learning/activations-sigmoid-tanh/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn dynamic_tanh(x: f64, alpha: f64, gamma: f64, beta: f64) -> f64 {\n    gamma * (alpha * x).tanh() + beta\n}\n\npub fn activations(x: f64) -> (f64, f64, f64, f64, f64) {\n    let sigmoid = 1.0 / (1.0 + (-x).exp());\n    let tanh = x.tanh();\n    let hard_sigmoid = (0.2 * x + 0.5).clamp(0.0, 1.0);\n    let hardtanh = x.clamp(-1.0, 1.0);\n    let dynamic = dynamic_tanh(x, 1.2, 1.0, 0.0);\n    (sigmoid, tanh, hard_sigmoid, hardtanh, dynamic)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "activations-softmax-softplus-softsign",
      "title": "Softmax, Softplus, Softsign",
      "path": "modules/ml/deep-learning/activations-softmax-softplus-softsign",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Softmax, Softplus, Softsign\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nSoftmax normalizes logits, softplus smooths ReLU, softsign saturates gently.\n\n## Math\n\n$$softplus(x)=log(1+e^x), softsign(x)=x/(1+|x|)$$\n\n## Function\n\n```python\ndef softmax(row: list[float]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/activations-softmax-softplus-softsign/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/activations-softmax-softplus-softsign/python/activations_softmax_softplus_softsign.py",
          "language": "python",
          "content": "import math\n\n\ndef softmax(row: list[float]) -> list[float]:\n    m = max(row)\n    exps = [math.exp(x - m) for x in row]\n    s = sum(exps)\n    return [e / s for e in exps]\n\n\ndef softplus(x: float) -> float:\n    return math.log1p(math.exp(x))\n\n\ndef softsign(x: float) -> float:\n    return x / (1 + abs(x))\n"
        },
        {
          "path": "modules/ml/deep-learning/activations-softmax-softplus-softsign/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn softmax(row: &[f64]) -> Vec<f64> {\n    let m = row.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let exps: Vec<f64> = row.iter().map(|x| (x - m).exp()).collect();\n    let sum: f64 = exps.iter().sum();\n    exps.iter().map(|e| e / sum).collect()\n}\n\npub fn softplus(x: f64) -> f64 {\n    (1.0 + x.exp()).ln()\n}\n\npub fn softsign(x: f64) -> f64 {\n    x / (1.0 + x.abs())\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "automatic-differentiation",
      "title": "Automatic Differentiation",
      "path": "modules/ml/deep-learning/automatic-differentiation",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Automatic Differentiation\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nAutodiff computes derivatives by composing local gradients.\n\n## Math\n\nIf $y = x^2$ and $z = y + 3$, then by the chain rule:\n\n$$\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx} = 1 \\cdot 2x = 2x$$\n\n## Function\n\n```python\ndef forward_grad(x: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/automatic-differentiation/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/automatic-differentiation/python/automatic_differentiation.py",
          "language": "python",
          "content": "def forward_grad(x: float) -> float:\n    return 2.0 * x\n"
        },
        {
          "path": "modules/ml/deep-learning/automatic-differentiation/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn forward_grad(x: f64) -> f64 {\n    2.0 * x\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "backpropagation",
      "title": "Backpropagation",
      "path": "modules/ml/deep-learning/backpropagation",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Backpropagation\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nBackprop computes gradients using the chain rule from output to inputs.\n\n## Math\n\n$$For z = w x, dL/dw = dL/dz * x.$$\n\n## Function\n\n```python\ndef linear_backprop(x: float, w: float, grad_out: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/backpropagation/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/backpropagation/python/backpropagation.py",
          "language": "python",
          "content": "def linear_backprop(x: float, w: float, grad_out: float) -> float:\n    return grad_out * x\n"
        },
        {
          "path": "modules/ml/deep-learning/backpropagation/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn linear_backprop(x: f64, grad_out: f64) -> f64 {\n    grad_out * x\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "batchnorm-transformers",
      "title": "Why BatchNorm is Bad for Transformers",
      "path": "modules/ml/deep-learning/batchnorm-transformers",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Why BatchNorm is Bad for Transformers\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nBatchNorm mixes statistics across batch/time, which conflicts with sequence modeling.\n\n## Math\n\n$$\\mu_B = \\frac{1}{BT}\\sum_{b=1}^{B}\\sum_{t=1}^{T} x_{b,t}$$\n\n## Function\n\n```python\ndef batch_stats(matrix: list[list[float]]) -> tuple[float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/batchnorm-transformers/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/batchnorm-transformers/python/batchnorm_transformers.py",
          "language": "python",
          "content": "def batch_stats(matrix: list[list[float]]) -> tuple[float, float]:\n    values = [v for row in matrix for v in row]\n    mean = sum(values) / len(values)\n    var = sum((v - mean) ** 2 for v in values) / len(values)\n    return mean, var\n"
        },
        {
          "path": "modules/ml/deep-learning/batchnorm-transformers/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn batch_stats(matrix: &[Vec<f64>]) -> (f64, f64) {\n    let mut values = Vec::new();\n    for row in matrix {\n        for v in row {\n            values.push(*v);\n        }\n    }\n    let mean: f64 = values.iter().sum::<f64>() / values.len() as f64;\n    let var: f64 = values.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / values.len() as f64;\n    (mean, var)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "batchnorm",
      "title": "BatchNorm",
      "path": "modules/ml/deep-learning/batchnorm",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# BatchNorm\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nBatchNorm normalizes activations across the batch.\n\n## Math\n\n$$y = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n\n## Function\n\n```python\ndef batchnorm(x: list[float], eps: float = 1e-5) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/batchnorm/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/batchnorm/python/batchnorm.py",
          "language": "python",
          "content": "import math\n\n\ndef batchnorm(x: list[float], eps: float = 1e-5) -> list[float]:\n    mean = sum(x) / len(x)\n    var = sum((v - mean) ** 2 for v in x) / len(x)\n    return [(v - mean) / math.sqrt(var + eps) for v in x]\n"
        },
        {
          "path": "modules/ml/deep-learning/batchnorm/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn batchnorm(x: &[f64], eps: f64) -> Vec<f64> {\n    let mean: f64 = x.iter().sum::<f64>() / x.len() as f64;\n    let var: f64 = x.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / x.len() as f64;\n    x.iter().map(|v| (v - mean) / (var + eps).sqrt()).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "cross-entropy",
      "title": "Cross-Entropy Loss",
      "path": "modules/ml/deep-learning/cross-entropy",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Cross-Entropy Loss\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nCross-entropy compares a predicted distribution to a target class.\n\n## Math\n\n$$L = -\\log\\left(\\mathrm{softmax}(\\text{logits})_{\\text{target}}\\right)$$\n\n## Function\n\n```python\ndef cross_entropy(logits: list[float], target: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/cross-entropy/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/cross-entropy/python/cross_entropy.py",
          "language": "python",
          "content": "import math\n\n\ndef cross_entropy(logits: list[float], target: int) -> float:\n    m = max(logits)\n    exps = [math.exp(x - m) for x in logits]\n    s = sum(exps)\n    prob = exps[target] / s\n    return -math.log(prob)\n"
        },
        {
          "path": "modules/ml/deep-learning/cross-entropy/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn cross_entropy(logits: &[f64], target: usize) -> f64 {\n    let m = logits.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let exps: Vec<f64> = logits.iter().map(|x| (x - m).exp()).collect();\n    let sum: f64 = exps.iter().sum();\n    let prob = exps[target] / sum;\n    -prob.ln()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "dropout",
      "title": "Dropout",
      "path": "modules/ml/deep-learning/dropout",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Dropout\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nDropout randomly zeroes activations during training to reduce co-adaptation.\n\n## Math\n\n$$x' = mask * x / (1-p)$$\n\n## Function\n\n```python\ndef dropout(x: list[float], p: float, seed: int = 0) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/dropout/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/dropout/python/dropout.py",
          "language": "python",
          "content": "def dropout(x: list[float], p: float, seed: int = 0) -> list[float]:\n    out = []\n    state = seed\n    for v in x:\n        state = (1103515245 * state + 12345) % (2**31)\n        keep = (state / (2**31 - 1)) > p\n        out.append(v / (1 - p) if keep else 0.0)\n    return out\n"
        },
        {
          "path": "modules/ml/deep-learning/dropout/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn dropout(x: &[f64], p: f64, seed: u64) -> Vec<f64> {\n    let mut out = Vec::with_capacity(x.len());\n    let mut state = seed;\n    for &v in x {\n        state = (1103515245u64.wrapping_mul(state).wrapping_add(12345)) % (1u64 << 31);\n        let keep = (state as f64 / ((1u64 << 31) - 1) as f64) > p;\n        if keep {\n            out.push(v / (1.0 - p));\n        } else {\n            out.push(0.0);\n        }\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "early-stopping",
      "title": "Early Stopping",
      "path": "modules/ml/deep-learning/early-stopping",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Early Stopping\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nStop training when validation loss stops improving.\n\n## Math\n\n$$t - t_{\\text{best}} \\ge P$$\n\n## Function\n\n```python\ndef should_stop(losses: list[float], patience: int) -> bool:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/early-stopping/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/early-stopping/python/early_stopping.py",
          "language": "python",
          "content": "def should_stop(losses: list[float], patience: int) -> bool:\n    best = float(\"inf\")\n    bad = 0\n    for loss in losses:\n        if loss < best:\n            best = loss\n            bad = 0\n        else:\n            bad += 1\n            if bad >= patience:\n                return True\n    return False\n"
        },
        {
          "path": "modules/ml/deep-learning/early-stopping/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn should_stop(losses: &[f64], patience: usize) -> bool {\n    let mut best = f64::INFINITY;\n    let mut bad = 0;\n    for &loss in losses {\n        if loss < best {\n            best = loss;\n            bad = 0;\n        } else {\n            bad += 1;\n            if bad >= patience {\n                return true;\n            }\n        }\n    }\n    false\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "feedforward-neural-network",
      "title": "Feedforward Neural Network",
      "path": "modules/ml/deep-learning/feedforward-neural-network",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Feedforward Neural Network\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nA feedforward network applies a sequence of linear layers and activations.\n\n## Math\n\n$$For layer i: h_i = sigma(W_i h_{i-1} + b_i).$$\n\n## Function\n\n```python\ndef feedforward(x: list[float], weights: list[list[list[float]]], biases: list[list[float]]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/feedforward-neural-network/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/feedforward-neural-network/python/feedforward_neural_network.py",
          "language": "python",
          "content": "from __future__ import annotations\n\n\ndef _matvec(w: list[list[float]], x: list[float]) -> list[float]:\n    return [sum(w_row[j] * x[j] for j in range(len(x))) for w_row in w]\n\n\ndef _relu(x: list[float]) -> list[float]:\n    return [max(0.0, v) for v in x]\n\n\ndef feedforward(\n    x: list[float],\n    weights: list[list[list[float]]],\n    biases: list[list[float]],\n) -> list[float]:\n    h = x\n    for w, b in zip(weights, biases):\n        h = [v + b[i] for i, v in enumerate(_matvec(w, h))]\n        h = _relu(h)\n    return h\n"
        },
        {
          "path": "modules/ml/deep-learning/feedforward-neural-network/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn feedforward(\n    x: &[f64],\n    weights: &[Vec<Vec<f64>>],\n    biases: &[Vec<f64>],\n) -> Vec<f64> {\n    let mut h: Vec<f64> = x.to_vec();\n    for (w, b) in weights.iter().zip(biases.iter()) {\n        let mut next = vec![0.0; w.len()];\n        for i in 0..w.len() {\n            let mut acc = 0.0;\n            for j in 0..h.len() {\n                acc += w[i][j] * h[j];\n            }\n            next[i] = (acc + b[i]).max(0.0);\n        }\n        h = next;\n    }\n    h\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "focal-loss",
      "title": "Focal Loss",
      "path": "modules/ml/deep-learning/focal-loss",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Focal Loss\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nFocal loss down-weights easy examples for imbalanced classification.\n\n## Math\n\n$$L = -(1-p)^{\\gamma} \\log(p)$$\n\n## Function\n\n```python\ndef focal_loss(p: float, gamma: float = 2.0) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/focal-loss/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/focal-loss/python/focal_loss.py",
          "language": "python",
          "content": "import math\n\n\ndef focal_loss(p: float, gamma: float = 2.0) -> float:\n    return -((1 - p) ** gamma) * math.log(p)\n"
        },
        {
          "path": "modules/ml/deep-learning/focal-loss/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn focal_loss(p: f64, gamma: f64) -> f64 {\n    -((1.0 - p).powf(gamma)) * p.ln()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "gradient-checking",
      "title": "Gradient Checking",
      "path": "modules/ml/deep-learning/gradient-checking",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Gradient Checking\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nFinite differences approximate gradients to validate backprop.\n\n## Math\n\n$$\\frac{df}{dx} \\approx \\frac{f(x+\\epsilon) - f(x-\\epsilon)}{2\\epsilon}$$\n\n## Function\n\n```python\ndef grad_check(f, x: float, eps: float = 1e-5) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/gradient-checking/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/gradient-checking/python/gradient_checking.py",
          "language": "python",
          "content": "def grad_check(f, x: float, eps: float = 1e-5) -> float:\n    return (f(x + eps) - f(x - eps)) / (2 * eps)\n"
        },
        {
          "path": "modules/ml/deep-learning/gradient-checking/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn grad_check(f: fn(f64) -> f64, x: f64, eps: f64) -> f64 {\n    (f(x + eps) - f(x - eps)) / (2.0 * eps)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "gradient-flow",
      "title": "Effect on Gradient Flow",
      "path": "modules/ml/deep-learning/gradient-flow",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Effect on Gradient Flow\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nInitialization affects gradient variance as it propagates through layers.\n\n## Math\n\n$$Var(g_{l}) \u2248 Var(g_{l+1}) * Var(W_l)$$\n\n## Function\n\n```python\ndef propagate_variance(var: float, layer_vars: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/gradient-flow/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/gradient-flow/python/gradient_flow.py",
          "language": "python",
          "content": "def propagate_variance(var: float, layer_vars: list[float]) -> float:\n    out = var\n    for v in layer_vars:\n        out *= v\n    return out\n"
        },
        {
          "path": "modules/ml/deep-learning/gradient-flow/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn propagate_variance(var: f64, layer_vars: &[f64]) -> f64 {\n    let mut out = var;\n    for v in layer_vars {\n        out *= v;\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "groupnorm",
      "title": "GroupNorm",
      "path": "modules/ml/deep-learning/groupnorm",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# GroupNorm\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nGroupNorm splits channels into groups and normalizes within each group.\n\n## Math\n\n$$y = \\frac{x - \\mu_g}{\\sqrt{\\sigma_g^2 + \\epsilon}}$$\n\n## Function\n\n```python\ndef groupnorm(x: list[float], groups: int, eps: float = 1e-5) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/groupnorm/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/groupnorm/python/groupnorm.py",
          "language": "python",
          "content": "import math\n\n\ndef groupnorm(x: list[float], groups: int, eps: float = 1e-5) -> list[float]:\n    size = len(x)\n    group_size = size // groups\n    out = []\n    for g in range(groups):\n        chunk = x[g * group_size : (g + 1) * group_size]\n        mean = sum(chunk) / len(chunk)\n        var = sum((v - mean) ** 2 for v in chunk) / len(chunk)\n        out.extend([(v - mean) / math.sqrt(var + eps) for v in chunk])\n    return out\n"
        },
        {
          "path": "modules/ml/deep-learning/groupnorm/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn groupnorm(x: &[f64], groups: usize, eps: f64) -> Vec<f64> {\n    let size = x.len();\n    let group_size = size / groups;\n    let mut out = Vec::with_capacity(size);\n    for g in 0..groups {\n        let start = g * group_size;\n        let end = start + group_size;\n        let chunk = &x[start..end];\n        let mean: f64 = chunk.iter().sum::<f64>() / chunk.len() as f64;\n        let var: f64 = chunk.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / chunk.len() as f64;\n        out.extend(chunk.iter().map(|v| (v - mean) / (var + eps).sqrt()));\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "he-initialization",
      "title": "He Initialization",
      "path": "modules/ml/deep-learning/he-initialization",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# He Initialization\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nHe initialization keeps variance stable for ReLU activations. Demo uses deterministic\npseudo-random.\n\n## Math\n\n$$W \\sim \\mathcal{N}\\left(0, \\frac{2}{fan_{in}}\\right)$$\n\n## Function\n\n```python\ndef he_normal(fan_in: int, fan_out: int, seed: int = 0) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/he-initialization/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/he-initialization/python/he_initialization.py",
          "language": "python",
          "content": "import math\n\n\ndef he_normal(fan_in: int, fan_out: int, seed: int = 0) -> list[float]:\n    std = math.sqrt(2 / fan_in)\n    # Box-Muller with deterministic LCG\n    values = []\n    state = seed\n    for _ in range(fan_in * fan_out // 2 + 1):\n        state = (1103515245 * state + 12345) % (2**31)\n        u1 = (state / (2**31 - 1)) or 1e-6\n        state = (1103515245 * state + 12345) % (2**31)\n        u2 = state / (2**31 - 1)\n        z0 = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n        z1 = math.sqrt(-2 * math.log(u1)) * math.sin(2 * math.pi * u2)\n        values.extend([z0 * std, z1 * std])\n    return values[: fan_in * fan_out]\n"
        },
        {
          "path": "modules/ml/deep-learning/he-initialization/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn he_normal(fan_in: usize, fan_out: usize, seed: u64) -> Vec<f64> {\n    let std = (2.0 / fan_in as f64).sqrt();\n    let mut values = Vec::new();\n    let mut state = seed;\n    while values.len() < fan_in * fan_out {\n        state = (1103515245u64.wrapping_mul(state).wrapping_add(12345)) % (1u64 << 31);\n        let u1 = (state as f64 / ((1u64 << 31) - 1) as f64).max(1e-6);\n        state = (1103515245u64.wrapping_mul(state).wrapping_add(12345)) % (1u64 << 31);\n        let u2 = state as f64 / ((1u64 << 31) - 1) as f64;\n        let z0 = (-2.0 * u1.ln()).sqrt() * (2.0 * std::f64::consts::PI * u2).cos();\n        let z1 = (-2.0 * u1.ln()).sqrt() * (2.0 * std::f64::consts::PI * u2).sin();\n        values.push(z0 * std);\n        if values.len() < fan_in * fan_out {\n            values.push(z1 * std);\n        }\n    }\n    values\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "hinge-loss",
      "title": "Hinge Loss",
      "path": "modules/ml/deep-learning/hinge-loss",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Hinge Loss\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nHinge loss encourages a margin between correct and incorrect classes.\n\n## Math\n\n$$L = \\max(0, 1 - y \\cdot \\text{score})$$\n\n## Function\n\n```python\ndef hinge_loss(score: float, label: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/hinge-loss/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/hinge-loss/python/hinge_loss.py",
          "language": "python",
          "content": "def hinge_loss(score: float, label: int) -> float:\n    return max(0.0, 1 - label * score)\n"
        },
        {
          "path": "modules/ml/deep-learning/hinge-loss/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn hinge_loss(score: f64, label: i32) -> f64 {\n    let val = 1.0 - (label as f64) * score;\n    if val > 0.0 { val } else { 0.0 }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "huber-loss",
      "title": "Huber Loss",
      "path": "modules/ml/deep-learning/huber-loss",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Huber Loss\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nHuber loss is quadratic near zero and linear for large errors.\n\n## Math\n\n$$\nL =\n\\begin{cases}\n\\frac{1}{2} e^2, & |e| \\le \\delta \\\\\n\\delta\\left(|e| - \\frac{1}{2}\\delta\\right), & \\text{otherwise}\n\\end{cases}\n$$\n\n## Function\n\n```python\ndef huber(y: float, y_hat: float, delta: float = 1.0) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/huber-loss/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/huber-loss/python/huber_loss.py",
          "language": "python",
          "content": "def huber(y: float, y_hat: float, delta: float = 1.0) -> float:\n    e = y - y_hat\n    if abs(e) <= delta:\n        return 0.5 * e * e\n    return delta * (abs(e) - 0.5 * delta)\n"
        },
        {
          "path": "modules/ml/deep-learning/huber-loss/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn huber(y: f64, y_hat: f64, delta: f64) -> f64 {\n    let e = y - y_hat;\n    if e.abs() <= delta {\n        0.5 * e * e\n    } else {\n        delta * (e.abs() - 0.5 * delta)\n    }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "instancenorm",
      "title": "InstanceNorm",
      "path": "modules/ml/deep-learning/instancenorm",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# InstanceNorm\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nInstanceNorm normalizes per-sample per-channel, often for vision.\n\n## Math\n\n$$y = \\frac{x - \\mu_I}{\\sqrt{\\sigma_I^2 + \\epsilon}}$$\n\n## Function\n\n```python\ndef instancenorm(x: list[float], eps: float = 1e-5) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/instancenorm/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/instancenorm/python/instancenorm.py",
          "language": "python",
          "content": "import math\n\n\ndef instancenorm(x: list[float], eps: float = 1e-5) -> list[float]:\n    mean = sum(x) / len(x)\n    var = sum((v - mean) ** 2 for v in x) / len(x)\n    return [(v - mean) / math.sqrt(var + eps) for v in x]\n"
        },
        {
          "path": "modules/ml/deep-learning/instancenorm/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn instancenorm(x: &[f64], eps: f64) -> Vec<f64> {\n    let mean: f64 = x.iter().sum::<f64>() / x.len() as f64;\n    let var: f64 = x.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / x.len() as f64;\n    x.iter().map(|v| (v - mean) / (var + eps).sqrt()).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "knowledge-distillation-loss",
      "title": "Knowledge Distillation Loss",
      "path": "modules/ml/deep-learning/knowledge-distillation-loss",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Knowledge Distillation Loss\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nDistillation matches student logits to teacher logits.\n\n## Math\n\n$$L = \\mathrm{KL}\\left(\\mathrm{softmax}\\left(\\frac{z_s}{T}\\right)\\,\\middle\\|\\,\\mathrm{softmax}\\left(\\frac{z_t}{T}\\right)\\right)$$\n\n## Function\n\n```python\ndef distill_loss(student: list[float], teacher: list[float], temp: float = 1.0) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/knowledge-distillation-loss/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/knowledge-distillation-loss/python/knowledge_distillation_loss.py",
          "language": "python",
          "content": "import math\n\n\ndef _softmax(logits: list[float], temp: float) -> list[float]:\n    m = max(logits)\n    exps = [math.exp((x - m) / temp) for x in logits]\n    s = sum(exps)\n    return [e / s for e in exps]\n\n\ndef distill_loss(student: list[float], teacher: list[float], temp: float = 1.0) -> float:\n    ps = _softmax(student, temp)\n    pt = _softmax(teacher, temp)\n    loss = 0.0\n    for p_s, p_t in zip(ps, pt):\n        if p_t > 0 and p_s > 0:\n            loss += p_t * math.log(p_t / p_s)\n    return loss\n"
        },
        {
          "path": "modules/ml/deep-learning/knowledge-distillation-loss/rust/src/lib.rs",
          "language": "rust",
          "content": "fn softmax(logits: &[f64], temp: f64) -> Vec<f64> {\n    let m = logits.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let exps: Vec<f64> = logits.iter().map(|x| ((x - m) / temp).exp()).collect();\n    let sum: f64 = exps.iter().sum();\n    exps.iter().map(|e| e / sum).collect()\n}\n\npub fn distill_loss(student: &[f64], teacher: &[f64], temp: f64) -> f64 {\n    let ps = softmax(student, temp);\n    let pt = softmax(teacher, temp);\n    let mut loss = 0.0;\n    for (p_s, p_t) in ps.iter().zip(pt.iter()) {\n        if *p_t > 0.0 && *p_s > 0.0 {\n            loss += p_t * (p_t / p_s).ln();\n        }\n    }\n    loss\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "l1-regularization",
      "title": "L1 Regularization",
      "path": "modules/ml/deep-learning/l1-regularization",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# L1 Regularization\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nL1 adds an absolute weight penalty to encourage sparsity. This is also known as **Lasso** regularization.\n\n## Math\n\n$$L = L_0 + \\lambda \\sum_i |w_i|$$\n\n## Function\n\n```python\ndef l1_penalty(weights: list[float], lam: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/l1-regularization/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/l1-regularization/python/l1_regularization.py",
          "language": "python",
          "content": "def l1_penalty(weights: list[float], lam: float) -> float:\n    return lam * sum(abs(w) for w in weights)\n"
        },
        {
          "path": "modules/ml/deep-learning/l1-regularization/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn l1_penalty(weights: &[f64], lam: f64) -> f64 {\n    lam * weights.iter().map(|w| w.abs()).sum::<f64>()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "l2-regularization",
      "title": "L2 Regularization",
      "path": "modules/ml/deep-learning/l2-regularization",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# L2 Regularization\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nL2 adds a squared weight penalty to discourage large weights. This is also known as **Ridge** regularization.\n\n## Math\n\n$$L = L_0 + \\lambda \\sum_i w_i^2$$\n\n## Function\n\n```python\ndef l2_penalty(weights: list[float], lam: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/l2-regularization/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/l2-regularization/python/l2_regularization.py",
          "language": "python",
          "content": "def l2_penalty(weights: list[float], lam: float) -> float:\n    return lam * sum(w * w for w in weights)\n"
        },
        {
          "path": "modules/ml/deep-learning/l2-regularization/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn l2_penalty(weights: &[f64], lam: f64) -> f64 {\n    lam * weights.iter().map(|w| w * w).sum::<f64>()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "layernorm",
      "title": "LayerNorm",
      "path": "modules/ml/deep-learning/layernorm",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# LayerNorm\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nLayerNorm normalizes across features within a single sample.\n\n## Math\n\n$$y = \\frac{x - \\mu_f}{\\sqrt{\\sigma_f^2 + \\epsilon}}$$\n\n## Function\n\n```python\ndef layernorm(x: list[float], eps: float = 1e-5) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/layernorm/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/layernorm/python/layernorm.py",
          "language": "python",
          "content": "import math\n\n\ndef layernorm(x: list[float], eps: float = 1e-5) -> list[float]:\n    mean = sum(x) / len(x)\n    var = sum((v - mean) ** 2 for v in x) / len(x)\n    return [(v - mean) / math.sqrt(var + eps) for v in x]\n"
        },
        {
          "path": "modules/ml/deep-learning/layernorm/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn layernorm(x: &[f64], eps: f64) -> Vec<f64> {\n    let mean: f64 = x.iter().sum::<f64>() / x.len() as f64;\n    let var: f64 = x.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / x.len() as f64;\n    x.iter().map(|v| (v - mean) / (var + eps).sqrt()).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "mae-loss",
      "title": "Mean Absolute Error",
      "path": "modules/ml/deep-learning/mae-loss",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Mean Absolute Error\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nMAE penalizes absolute residuals.\n\n## Math\n\n$$\\mathrm{MAE} = \\frac{1}{n}\\sum_i |y_i - \\hat{y}_i|$$\n\n## Function\n\n```python\ndef mae(y: list[float], y_hat: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/mae-loss/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/mae-loss/python/mae_loss.py",
          "language": "python",
          "content": "def mae(y: list[float], y_hat: list[float]) -> float:\n    return sum(abs(a - b) for a, b in zip(y, y_hat)) / len(y)\n"
        },
        {
          "path": "modules/ml/deep-learning/mae-loss/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn mae(y: &[f64], y_hat: &[f64]) -> f64 {\n    let sum: f64 = y.iter().zip(y_hat.iter()).map(|(a, b)| (a - b).abs()).sum();\n    sum / y.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "mse-loss",
      "title": "Mean Squared Error",
      "path": "modules/ml/deep-learning/mse-loss",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Mean Squared Error\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nMSE penalizes squared residuals.\n\n## Math\n\n$$\\mathrm{MSE} = \\frac{1}{n}\\sum_i (y_i - \\hat{y}_i)^2$$\n\n## Function\n\n```python\ndef mse(y: list[float], y_hat: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/mse-loss/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/mse-loss/python/mse_loss.py",
          "language": "python",
          "content": "def mse(y: list[float], y_hat: list[float]) -> float:\n    return sum((a - b) ** 2 for a, b in zip(y, y_hat)) / len(y)\n"
        },
        {
          "path": "modules/ml/deep-learning/mse-loss/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn mse(y: &[f64], y_hat: &[f64]) -> f64 {\n    let sum: f64 = y.iter().zip(y_hat.iter()).map(|(a, b)| (a - b).powi(2)).sum();\n    sum / y.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "neuron-weights-bias-activation",
      "title": "Neuron, Weights, Bias, Activation",
      "path": "modules/ml/deep-learning/neuron-weights-bias-activation",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Neuron, Weights, Bias, Activation\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nA neuron computes a weighted sum plus bias, then applies an activation.\n\n## Math\n\n$$y = sigma(w \u00b7 x + b)$$\n\n## Function\n\n```python\ndef neuron(x: list[float], w: list[float], b: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/neuron-weights-bias-activation/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/neuron-weights-bias-activation/python/neuron_weights_bias_activation.py",
          "language": "python",
          "content": "import math\n\n\ndef neuron(x: list[float], w: list[float], b: float) -> float:\n    z = sum(wi * xi for wi, xi in zip(w, x)) + b\n    return 1 / (1 + math.exp(-z))\n"
        },
        {
          "path": "modules/ml/deep-learning/neuron-weights-bias-activation/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn neuron(x: &[f64], w: &[f64], b: f64) -> f64 {\n    let mut z = b;\n    for (wi, xi) in w.iter().zip(x.iter()) {\n        z += wi * xi;\n    }\n    1.0 / (1.0 + (-z).exp())\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "rmse-loss",
      "title": "Root Mean Squared Error",
      "path": "modules/ml/deep-learning/rmse-loss",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Root Mean Squared Error\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nRMSE is the square root of MSE.\n\n## Math\n\n$$\\mathrm{RMSE} = \\sqrt{\\mathrm{MSE}}$$\n\n## Function\n\n```python\ndef rmse(y: list[float], y_hat: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/rmse-loss/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/rmse-loss/python/rmse_loss.py",
          "language": "python",
          "content": "import math\n\n\ndef rmse(y: list[float], y_hat: list[float]) -> float:\n    mse = sum((a - b) ** 2 for a, b in zip(y, y_hat)) / len(y)\n    return math.sqrt(mse)\n"
        },
        {
          "path": "modules/ml/deep-learning/rmse-loss/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn rmse(y: &[f64], y_hat: &[f64]) -> f64 {\n    let sum: f64 = y.iter().zip(y_hat.iter()).map(|(a, b)| (a - b).powi(2)).sum();\n    (sum / y.len() as f64).sqrt()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "rmsnorm",
      "title": "RMSNorm",
      "path": "modules/ml/deep-learning/rmsnorm",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# RMSNorm\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nRMSNorm normalizes by root-mean-square without centering.\n\n## Math\n\n$$y = \\frac{x}{\\sqrt{\\frac{1}{d}\\sum_{i=1}^{d} x_i^2 + \\epsilon}}$$\n\n## Function\n\n```python\ndef rmsnorm(x: list[float], eps: float = 1e-5) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/rmsnorm/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/rmsnorm/python/rmsnorm.py",
          "language": "python",
          "content": "import math\n\n\ndef rmsnorm(x: list[float], eps: float = 1e-5) -> list[float]:\n    rms = math.sqrt(sum(v * v for v in x) / len(x) + eps)\n    return [v / rms for v in x]\n"
        },
        {
          "path": "modules/ml/deep-learning/rmsnorm/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn rmsnorm(x: &[f64], eps: f64) -> Vec<f64> {\n    let rms = (x.iter().map(|v| v * v).sum::<f64>() / x.len() as f64 + eps).sqrt();\n    x.iter().map(|v| v / rms).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "vanishing-exploding-gradients",
      "title": "Vanishing and Exploding Gradients",
      "path": "modules/ml/deep-learning/vanishing-exploding-gradients",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Vanishing and Exploding Gradients\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nRepeated multiplication by small or large values shrinks or blows up gradients.\n\n## Math\n\n$$g_L = g_0 * \u03a0 w_i$$\n\n## Function\n\n```python\ndef gradient_chain(weights: list[float], grad: float = 1.0) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/vanishing-exploding-gradients/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/vanishing-exploding-gradients/python/vanishing_exploding_gradients.py",
          "language": "python",
          "content": "def gradient_chain(weights: list[float], grad: float = 1.0) -> float:\n    out = grad\n    for w in weights:\n        out *= w\n    return out\n"
        },
        {
          "path": "modules/ml/deep-learning/vanishing-exploding-gradients/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn gradient_chain(weights: &[f64], grad: f64) -> f64 {\n    let mut out = grad;\n    for w in weights {\n        out *= w;\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "weight-decay",
      "title": "Weight Decay",
      "path": "modules/ml/deep-learning/weight-decay",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Weight Decay\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nWeight decay shrinks weights during optimization, decoupled in AdamW.\n\n## Math\n\n$$w \\leftarrow w - \\text{lr}\\left(g + \\lambda w\\right)$$\n\n## Function\n\n```python\ndef weight_decay_step(w: float, grad: float, lr: float, lam: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/weight-decay/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/weight-decay/python/weight_decay.py",
          "language": "python",
          "content": "def weight_decay_step(w: float, grad: float, lr: float, lam: float) -> float:\n    return w - lr * (grad + lam * w)\n"
        },
        {
          "path": "modules/ml/deep-learning/weight-decay/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn weight_decay_step(w: f64, grad: f64, lr: f64, lam: f64) -> f64 {\n    w - lr * (grad + lam * w)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "xavier-initialization",
      "title": "Xavier/Glorot Initialization",
      "path": "modules/ml/deep-learning/xavier-initialization",
      "summary": "> Track: `ml` | Topic: `deep-learning`",
      "readme": "# Xavier/Glorot Initialization\n\n> Track: `ml` | Topic: `deep-learning`\n\n## Concept\n\nXavier init keeps variance stable for symmetric activations.\n\n## Math\n\n$$W \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{fan_{in}+fan_{out}}}, \\sqrt{\\frac{6}{fan_{in}+fan_{out}}}\\right)$$\n\n## Function\n\n```python\ndef xavier_uniform(fan_in: int, fan_out: int, seed: int = 0) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/deep-learning/xavier-initialization/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/deep-learning/xavier-initialization/python/xavier_initialization.py",
          "language": "python",
          "content": "import math\n\n\ndef xavier_uniform(fan_in: int, fan_out: int, seed: int = 0) -> list[float]:\n    limit = math.sqrt(6 / (fan_in + fan_out))\n    # simple LCG for deterministic demo\n    values = []\n    state = seed\n    for _ in range(fan_in * fan_out):\n        state = (1103515245 * state + 12345) % (2**31)\n        values.append(-limit + (state / (2**31 - 1)) * (2 * limit))\n    return values\n"
        },
        {
          "path": "modules/ml/deep-learning/xavier-initialization/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn xavier_uniform(fan_in: usize, fan_out: usize, seed: u64) -> Vec<f64> {\n    let limit = (6.0 / (fan_in + fan_out) as f64).sqrt();\n    let mut state = seed;\n    let mut out = Vec::with_capacity(fan_in * fan_out);\n    for _ in 0..fan_in * fan_out {\n        state = (1103515245u64.wrapping_mul(state).wrapping_add(12345)) % (1u64 << 31);\n        let frac = state as f64 / ((1u64 << 31) - 1) as f64;\n        out.push(-limit + frac * (2.0 * limit));\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "accuracy",
      "title": "Accuracy",
      "path": "modules/ml/evaluation/accuracy",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Accuracy\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nAccuracy is the fraction of correct predictions.\n\n## Math\n\n$$accuracy = correct / N$$\n\n## Function\n\n```python\ndef accuracy(y_true: list[int], y_pred: list[int]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/accuracy/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/accuracy/python/accuracy.py",
          "language": "python",
          "content": "def accuracy(y_true: list[int], y_pred: list[int]) -> float:\n    correct = sum(1 for a, b in zip(y_true, y_pred) if a == b)\n    return correct / len(y_true)\n"
        },
        {
          "path": "modules/ml/evaluation/accuracy/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn accuracy(y_true: &[i32], y_pred: &[i32]) -> f64 {\n    let mut correct = 0;\n    for (a, b) in y_true.iter().zip(y_pred.iter()) {\n        if a == b { correct += 1; }\n    }\n    correct as f64 / y_true.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "calinski-harabasz",
      "title": "Calinski-Harabasz Index",
      "path": "modules/ml/evaluation/calinski-harabasz",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Calinski-Harabasz Index\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nCalinski-Harabasz compares between- and within-cluster dispersion.\n\n## Math\n\n$$CH = (B/(k-1)) / (W/(n-k))$$\n\n## Function\n\n```python\ndef calinski_harabasz(b: float, w: float, k: int, n: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/calinski-harabasz/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/calinski-harabasz/python/calinski_harabasz.py",
          "language": "python",
          "content": "def calinski_harabasz(b: float, w: float, k: int, n: int) -> float:\n    return (b / (k - 1)) / (w / (n - k))\n"
        },
        {
          "path": "modules/ml/evaluation/calinski-harabasz/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn calinski_harabasz(b: f64, w: f64, k: i32, n: i32) -> f64 {\n    (b / (k as f64 - 1.0)) / (w / (n as f64 - k as f64))\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "confusion-matrix",
      "title": "Confusion Matrix",
      "path": "modules/ml/evaluation/confusion-matrix",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Confusion Matrix\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nConfusion matrix counts TP, FP, FN, TN.\n\n## Math\n\n$$[[TN, FP],[FN, TP]]$$\n\n## Function\n\n```python\ndef confusion_matrix(y_true: list[int], y_pred: list[int]) -> list[list[int]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/confusion-matrix/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/confusion-matrix/python/confusion_matrix.py",
          "language": "python",
          "content": "def confusion_matrix(y_true: list[int], y_pred: list[int]) -> list[list[int]]:\n    tn = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 0)\n    fp = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)\n    fn = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 0)\n    tp = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1)\n    return [[tn, fp], [fn, tp]]\n"
        },
        {
          "path": "modules/ml/evaluation/confusion-matrix/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn confusion_matrix(y_true: &[i32], y_pred: &[i32]) -> [[i32; 2]; 2] {\n    let mut tn = 0;\n    let mut fp = 0;\n    let mut fn_ = 0;\n    let mut tp = 0;\n    for (t, p) in y_true.iter().zip(y_pred.iter()) {\n        if *t == 0 && *p == 0 { tn += 1; }\n        if *t == 0 && *p == 1 { fp += 1; }\n        if *t == 1 && *p == 0 { fn_ += 1; }\n        if *t == 1 && *p == 1 { tp += 1; }\n    }\n    [[tn, fp], [fn_, tp]]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "davies-bouldin",
      "title": "Davies-Bouldin Index",
      "path": "modules/ml/evaluation/davies-bouldin",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Davies-Bouldin Index\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nDavies-Bouldin averages cluster similarity; lower is better.\n\n## Math\n\n$$DB = \\frac{1}{K}\\sum_i \\max_{j \\ne i} \\frac{s_i + s_j}{d_{ij}}$$\n\n## Function\n\n```python\ndef davies_bouldin(si: float, sj: float, dij: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/davies-bouldin/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/davies-bouldin/python/davies_bouldin.py",
          "language": "python",
          "content": "def davies_bouldin(si: float, sj: float, dij: float) -> float:\n    return (si + sj) / dij if dij > 0 else 0.0\n"
        },
        {
          "path": "modules/ml/evaluation/davies-bouldin/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn davies_bouldin(si: f64, sj: f64, dij: f64) -> f64 {\n    if dij > 0.0 { (si + sj) / dij } else { 0.0 }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "dice-score",
      "title": "Dice Score",
      "path": "modules/ml/evaluation/dice-score",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Dice Score\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nDice score measures overlap, common in segmentation.\n\n## Math\n\n$$\\text{2|A\u2229B|/(|A|+|B|)}$$\n\n## Function\n\n```python\ndef dice(a: set[int], b: set[int]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/dice-score/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/dice-score/python/dice_score.py",
          "language": "python",
          "content": "def dice(a: set[int], b: set[int]) -> float:\n    inter = len(a & b)\n    return 2 * inter / (len(a) + len(b)) if (len(a) + len(b)) > 0 else 0.0\n"
        },
        {
          "path": "modules/ml/evaluation/dice-score/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashSet;\n\npub fn dice(a: &HashSet<i32>, b: &HashSet<i32>) -> f64 {\n    let inter = a.intersection(b).count() as f64;\n    let denom = (a.len() + b.len()) as f64;\n    if denom == 0.0 { 0.0 } else { 2.0 * inter / denom }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "f1-score",
      "title": "F1 Score",
      "path": "modules/ml/evaluation/f1-score",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# F1 Score\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nF1 is the harmonic mean of precision and recall.\n\n## Math\n\n$$F1 = 2PR/(P+R)$$\n\n## Function\n\n```python\ndef f1_score(precision: float, recall: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/f1-score/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/f1-score/python/f1_score.py",
          "language": "python",
          "content": "def f1_score(precision: float, recall: float) -> float:\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n"
        },
        {
          "path": "modules/ml/evaluation/f1-score/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn f1_score(precision: f64, recall: f64) -> f64 {\n    if precision + recall == 0.0 { 0.0 } else { 2.0 * precision * recall / (precision + recall) }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "gini-impurity",
      "title": "Gini Impurity",
      "path": "modules/ml/evaluation/gini-impurity",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Gini Impurity\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nGini impurity measures class mixing in a node.\n\n## Math\n\n$$\\mathrm{Gini} = 1 - \\sum_c p_c^2$$\n\n## Function\n\n```python\ndef gini(labels: list[int]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/gini-impurity/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/gini-impurity/python/gini_impurity.py",
          "language": "python",
          "content": "from collections import Counter\n\n\ndef gini(labels: list[int]) -> float:\n    counts = Counter(labels)\n    n = len(labels)\n    return 1 - sum((c / n) ** 2 for c in counts.values())\n"
        },
        {
          "path": "modules/ml/evaluation/gini-impurity/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashMap;\n\npub fn gini(labels: &[i32]) -> f64 {\n    let mut counts: HashMap<i32, usize> = HashMap::new();\n    for label in labels {\n        *counts.entry(*label).or_insert(0) += 1;\n    }\n    let n = labels.len() as f64;\n    let mut sum_sq = 0.0;\n    for cnt in counts.values() {\n        let p = *cnt as f64 / n;\n        sum_sq += p * p;\n    }\n    1.0 - sum_sq\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "jaccard-index",
      "title": "Jaccard Index",
      "path": "modules/ml/evaluation/jaccard-index",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Jaccard Index\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nJaccard measures overlap between sets.\n\n## Math\n\n$$\\text{|A\u2229B| / |A\u222aB|}$$\n\n## Function\n\n```python\ndef jaccard(a: set[int], b: set[int]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/jaccard-index/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/jaccard-index/python/jaccard_index.py",
          "language": "python",
          "content": "def jaccard(a: set[int], b: set[int]) -> float:\n    inter = len(a & b)\n    union = len(a | b)\n    return inter / union if union > 0 else 0.0\n"
        },
        {
          "path": "modules/ml/evaluation/jaccard-index/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashSet;\n\npub fn jaccard(a: &HashSet<i32>, b: &HashSet<i32>) -> f64 {\n    let inter = a.intersection(b).count() as f64;\n    let union = a.union(b).count() as f64;\n    if union == 0.0 { 0.0 } else { inter / union }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "mae-vs-mse",
      "title": "MAE vs MSE",
      "path": "modules/ml/evaluation/mae-vs-mse",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# MAE vs MSE\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nCompare MAE and MSE to see sensitivity to outliers.\n\n## Math\n\n$$MAE uses |e|; MSE uses e^2.$$\n\n## Function\n\n```python\ndef mae_mse(y: list[float], y_hat: list[float]) -> tuple[float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/mae-vs-mse/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/mae-vs-mse/python/mae_vs_mse.py",
          "language": "python",
          "content": "def mae_mse(y: list[float], y_hat: list[float]) -> tuple[float, float]:\n    mae = sum(abs(a - b) for a, b in zip(y, y_hat)) / len(y)\n    mse = sum((a - b) ** 2 for a, b in zip(y, y_hat)) / len(y)\n    return mae, mse\n"
        },
        {
          "path": "modules/ml/evaluation/mae-vs-mse/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn mae_mse(y: &[f64], y_hat: &[f64]) -> (f64, f64) {\n    let mae = y.iter().zip(y_hat.iter()).map(|(a, b)| (a - b).abs()).sum::<f64>() / y.len() as f64;\n    let mse = y.iter().zip(y_hat.iter()).map(|(a, b)| (a - b).powi(2)).sum::<f64>() / y.len() as f64;\n    (mae, mse)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "matthews-correlation",
      "title": "Matthews Correlation Coefficient",
      "path": "modules/ml/evaluation/matthews-correlation",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Matthews Correlation Coefficient\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nMCC balances all confusion matrix terms.\n\n## Math\n\n$$\\mathrm{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$\n\n## Function\n\n```python\ndef mcc(tp: int, tn: int, fp: int, fn: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/matthews-correlation/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/matthews-correlation/python/matthews_correlation.py",
          "language": "python",
          "content": "import math\n\n\ndef mcc(tp: int, tn: int, fp: int, fn: int) -> float:\n    denom = math.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n    if denom == 0:\n        return 0.0\n    return (tp * tn - fp * fn) / denom\n"
        },
        {
          "path": "modules/ml/evaluation/matthews-correlation/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn mcc(tp: i32, tn: i32, fp: i32, fn_: i32) -> f64 {\n    let denom = ((tp + fp) as f64 * (tp + fn_) as f64 * (tn + fp) as f64 * (tn + fn_) as f64).sqrt();\n    if denom == 0.0 { 0.0 } else { (tp * tn - fp * fn_) as f64 / denom }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "precision-recall",
      "title": "Precision and Recall",
      "path": "modules/ml/evaluation/precision-recall",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Precision and Recall\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nPrecision measures correctness of positive predictions; recall measures coverage.\n\n## Math\n\n$$precision=TP/(TP+FP), recall=TP/(TP+FN)$$\n\n## Function\n\n```python\ndef precision_recall(y_true: list[int], y_pred: list[int]) -> tuple[float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/precision-recall/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/precision-recall/python/precision_recall.py",
          "language": "python",
          "content": "def precision_recall(y_true: list[int], y_pred: list[int]) -> tuple[float, float]:\n    tp = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1)\n    fp = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)\n    fn = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 0)\n    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n    return precision, recall\n"
        },
        {
          "path": "modules/ml/evaluation/precision-recall/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn precision_recall(y_true: &[i32], y_pred: &[i32]) -> (f64, f64) {\n    let mut tp = 0;\n    let mut fp = 0;\n    let mut fn_ = 0;\n    for (t, p) in y_true.iter().zip(y_pred.iter()) {\n        if *t == 1 && *p == 1 { tp += 1; }\n        if *t == 0 && *p == 1 { fp += 1; }\n        if *t == 1 && *p == 0 { fn_ += 1; }\n    }\n    let precision = if tp + fp > 0 { tp as f64 / (tp + fp) as f64 } else { 0.0 };\n    let recall = if tp + fn_ > 0 { tp as f64 / (tp + fn_) as f64 } else { 0.0 };\n    (precision, recall)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "r2-score",
      "title": "R2 Score",
      "path": "modules/ml/evaluation/r2-score",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# R2 Score\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nR2 measures explained variance relative to a mean baseline.\n\n## Math\n\n$$R2 = 1 - SS_res/SS_tot$$\n\n## Function\n\n```python\ndef r2_score(y: list[float], y_hat: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/r2-score/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/r2-score/python/r2_score.py",
          "language": "python",
          "content": "def r2_score(y: list[float], y_hat: list[float]) -> float:\n    mean = sum(y) / len(y)\n    ss_res = sum((a - b) ** 2 for a, b in zip(y, y_hat))\n    ss_tot = sum((a - mean) ** 2 for a in y)\n    return 1 - ss_res / ss_tot if ss_tot > 0 else 0.0\n"
        },
        {
          "path": "modules/ml/evaluation/r2-score/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn r2_score(y: &[f64], y_hat: &[f64]) -> f64 {\n    let mean: f64 = y.iter().sum::<f64>() / y.len() as f64;\n    let ss_res: f64 = y.iter().zip(y_hat.iter()).map(|(a, b)| (a - b).powi(2)).sum();\n    let ss_tot: f64 = y.iter().map(|a| (a - mean).powi(2)).sum();\n    if ss_tot > 0.0 { 1.0 - ss_res / ss_tot } else { 0.0 }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "roc-auc",
      "title": "ROC-AUC",
      "path": "modules/ml/evaluation/roc-auc",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# ROC-AUC\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nROC-AUC measures ranking quality over thresholds.\n\n## Math\n\n$$AUC = area under TPR vs FPR curve.$$\n\n## Function\n\n```python\ndef auc(fpr: list[float], tpr: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/roc-auc/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/roc-auc/python/roc_auc.py",
          "language": "python",
          "content": "def auc(fpr: list[float], tpr: list[float]) -> float:\n    area = 0.0\n    for i in range(1, len(fpr)):\n        area += (fpr[i] - fpr[i - 1]) * (tpr[i] + tpr[i - 1]) / 2\n    return area\n"
        },
        {
          "path": "modules/ml/evaluation/roc-auc/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn auc(fpr: &[f64], tpr: &[f64]) -> f64 {\n    let mut area = 0.0;\n    for i in 1..fpr.len() {\n        area += (fpr[i] - fpr[i - 1]) * (tpr[i] + tpr[i - 1]) / 2.0;\n    }\n    area\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "silhouette-score",
      "title": "Silhouette Score",
      "path": "modules/ml/evaluation/silhouette-score",
      "summary": "> Track: `ml` | Topic: `evaluation`",
      "readme": "# Silhouette Score\n\n> Track: `ml` | Topic: `evaluation`\n\n## Concept\n\nSilhouette compares intra-cluster to nearest-cluster distance.\n\n## Math\n\n$$s = \\frac{b - a}{\\max(a,b)}$$\n\n## Function\n\n```python\ndef silhouette(a: float, b: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/evaluation/silhouette-score/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/evaluation/silhouette-score/python/silhouette_score.py",
          "language": "python",
          "content": "def silhouette(a: float, b: float) -> float:\n    if max(a, b) == 0:\n        return 0.0\n    return (b - a) / max(a, b)\n"
        },
        {
          "path": "modules/ml/evaluation/silhouette-score/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn silhouette(a: f64, b: f64) -> f64 {\n    let m = a.max(b);\n    if m == 0.0 { 0.0 } else { (b - a) / m }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "beta-binomial",
      "title": "Bayesian Inference (Beta-Binomial)",
      "path": "modules/ml/fundamentals/beta-binomial",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Bayesian Inference (Beta-Binomial)\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nBeta prior and Binomial likelihood yield Beta posterior.\n\n## Math\n\n$$\\alpha' = \\alpha + \\text{successes},\\ \\beta' = \\beta + \\text{failures}$$\n\n## Function\n\n```python\ndef beta_posterior(alpha: float, beta: float, successes: int, failures: int) -> tuple[float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/beta-binomial/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/beta-binomial/python/beta_binomial.py",
          "language": "python",
          "content": "def beta_posterior(alpha: float, beta: float, successes: int, failures: int) -> tuple[float, float]:\n    return alpha + successes, beta + failures\n"
        },
        {
          "path": "modules/ml/fundamentals/beta-binomial/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn beta_posterior(alpha: f64, beta: f64, successes: i32, failures: i32) -> (f64, f64) {\n    (alpha + successes as f64, beta + failures as f64)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "convex-vs-nonconvex",
      "title": "Convex vs Non-Convex",
      "path": "modules/ml/fundamentals/convex-vs-nonconvex",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Convex vs Non-Convex\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nConvex functions have a single global minimum.\n\n## Math\n\n$$f(x)=ax^2+bx+c \\text{ is convex if } a \\ge 0$$\n\n## Function\n\n```python\ndef is_convex_quadratic(a: float) -> bool:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/convex-vs-nonconvex/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/convex-vs-nonconvex/python/convex_vs_nonconvex.py",
          "language": "python",
          "content": "def is_convex_quadratic(a: float) -> bool:\n    return a >= 0\n"
        },
        {
          "path": "modules/ml/fundamentals/convex-vs-nonconvex/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn is_convex_quadratic(a: f64) -> bool {\n    a >= 0.0\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "cosine-similarity",
      "title": "Cosine Similarity",
      "path": "modules/ml/fundamentals/cosine-similarity",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Cosine Similarity\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nCosine similarity measures angle between vectors.\n\n## Math\n\n$$cos = (a\u00b7b) / (||a|| ||b||)$$\n\n## Function\n\n```python\ndef cosine_similarity(a: list[float], b: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/cosine-similarity/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/cosine-similarity/python/cosine_similarity.py",
          "language": "python",
          "content": "import math\n\n\ndef cosine_similarity(a: list[float], b: list[float]) -> float:\n    dot = sum(x * y for x, y in zip(a, b))\n    na = math.sqrt(sum(x * x for x in a))\n    nb = math.sqrt(sum(y * y for y in b))\n    return dot / (na * nb)\n"
        },
        {
          "path": "modules/ml/fundamentals/cosine-similarity/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn cosine_similarity(a: &[f64], b: &[f64]) -> f64 {\n    let dot: f64 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();\n    let na = a.iter().map(|x| x * x).sum::<f64>().sqrt();\n    let nb = b.iter().map(|y| y * y).sum::<f64>().sqrt();\n    dot / (na * nb)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "covariance",
      "title": "Covariance",
      "path": "modules/ml/fundamentals/covariance",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Covariance\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nCovariance measures how two variables vary together.\n\n## Math\n\n$$\\mathrm{cov}(X,Y)=\\mathbb{E}[(X-\\mu_X)(Y-\\mu_Y)]$$\n\n## Function\n\n```python\ndef covariance(x: list[float], y: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/covariance/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/covariance/python/covariance.py",
          "language": "python",
          "content": "def covariance(x: list[float], y: list[float]) -> float:\n    mean_x = sum(x) / len(x)\n    mean_y = sum(y) / len(y)\n    return sum((a - mean_x) * (b - mean_y) for a, b in zip(x, y)) / len(x)\n"
        },
        {
          "path": "modules/ml/fundamentals/covariance/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn covariance(x: &[f64], y: &[f64]) -> f64 {\n    let mean_x: f64 = x.iter().sum::<f64>() / x.len() as f64;\n    let mean_y: f64 = y.iter().sum::<f64>() / y.len() as f64;\n    x.iter()\n        .zip(y.iter())\n        .map(|(a, b)| (a - mean_x) * (b - mean_y))\n        .sum::<f64>()\n        / x.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "distributions",
      "title": "Distributions",
      "path": "modules/ml/fundamentals/distributions",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Distributions\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nA probability distribution assigns probabilities to outcomes.\n\n## Math\n\n$$\\sum_i p_i = 1$$\n\n## Function\n\n```python\ndef normalize_probs(values: list[float]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/distributions/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/distributions/python/distributions.py",
          "language": "python",
          "content": "def normalize_probs(values: list[float]) -> list[float]:\n    s = sum(values)\n    return [v / s for v in values]\n"
        },
        {
          "path": "modules/ml/fundamentals/distributions/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn normalize_probs(values: &[f64]) -> Vec<f64> {\n    let sum: f64 = values.iter().sum();\n    values.iter().map(|v| v / sum).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "elbo",
      "title": "ELBO",
      "path": "modules/ml/fundamentals/elbo",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# ELBO\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nELBO lower-bounds log-likelihood in variational inference.\n\n## Math\n\n$$\\mathrm{ELBO} = \\mathbb{E}_q[\\log p(x|z)] - \\mathrm{KL}(q\\|p)$$\n\n## Function\n\n```python\ndef elbo(recon: float, kl: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/elbo/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/elbo/python/elbo.py",
          "language": "python",
          "content": "def elbo(recon: float, kl: float) -> float:\n    return recon - kl\n"
        },
        {
          "path": "modules/ml/fundamentals/elbo/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn elbo(recon: f64, kl: f64) -> f64 {\n    recon - kl\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "empirical-pmf",
      "title": "Empirical PMF",
      "path": "modules/ml/fundamentals/empirical-pmf",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Empirical PMF\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nEmpirical PMF estimates probability from samples.\n\n## Math\n\n$$p(x)=count(x)/N$$\n\n## Function\n\n```python\ndef empirical_pmf(samples: list[int]) -> dict[int, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/empirical-pmf/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/empirical-pmf/python/empirical_pmf.py",
          "language": "python",
          "content": "from collections import Counter\n\n\ndef empirical_pmf(samples: list[int]) -> dict[int, float]:\n    counts = Counter(samples)\n    n = len(samples)\n    return {k: v / n for k, v in counts.items()}\n"
        },
        {
          "path": "modules/ml/fundamentals/empirical-pmf/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashMap;\n\npub fn empirical_pmf(samples: &[i32]) -> HashMap<i32, f64> {\n    let mut counts: HashMap<i32, usize> = HashMap::new();\n    for s in samples {\n        *counts.entry(*s).or_insert(0) += 1;\n    }\n    let n = samples.len() as f64;\n    let mut pmf = HashMap::new();\n    for (k, v) in counts {\n        pmf.insert(k, v as f64 / n);\n    }\n    pmf\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "expectation",
      "title": "Expectation",
      "path": "modules/ml/fundamentals/expectation",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Expectation\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nExpectation is the average value of a random variable.\n\n## Math\n\n$$\\mathbb{E}[X] = \\sum_i x_i p_i$$\n\n## Function\n\n```python\ndef expectation(values: list[float], probs: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/expectation/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/expectation/python/expectation.py",
          "language": "python",
          "content": "def expectation(values: list[float], probs: list[float]) -> float:\n    return sum(v * p for v, p in zip(values, probs))\n"
        },
        {
          "path": "modules/ml/fundamentals/expectation/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn expectation(values: &[f64], probs: &[f64]) -> f64 {\n    values.iter().zip(probs.iter()).map(|(v, p)| v * p).sum()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "gradient-descent",
      "title": "Gradient Descent",
      "path": "modules/ml/fundamentals/gradient-descent",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Gradient Descent\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nGradient descent updates parameters along negative gradient.\n\n## Math\n\n$$x = x - lr * grad$$\n\n## Function\n\n```python\ndef gd_step(x: float, grad: float, lr: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/gradient-descent/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/gradient-descent/python/gradient_descent.py",
          "language": "python",
          "content": "def gd_step(x: float, grad: float, lr: float) -> float:\n    return x - lr * grad\n"
        },
        {
          "path": "modules/ml/fundamentals/gradient-descent/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn gd_step(x: f64, grad: f64, lr: f64) -> f64 {\n    x - lr * grad\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "hessian",
      "title": "Hessian",
      "path": "modules/ml/fundamentals/hessian",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Hessian\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nHessian is the matrix of second derivatives.\n\n## Math\n\n$$H_ij = \u2202^2 f / \u2202x_i \u2202x_j$$\n\n## Function\n\n```python\ndef hessian_quadratic(a: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/hessian/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/hessian/python/hessian.py",
          "language": "python",
          "content": "def hessian_quadratic(a: float) -> float:\n    return 2 * a\n"
        },
        {
          "path": "modules/ml/fundamentals/hessian/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn hessian_quadratic(a: f64) -> f64 {\n    2.0 * a\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "jacobian",
      "title": "Jacobian",
      "path": "modules/ml/fundamentals/jacobian",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Jacobian\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nJacobian contains partial derivatives of vector-valued functions.\n\n## Math\n\n$$J_ij = \u2202f_i/\u2202x_j$$\n\n## Function\n\n```python\ndef jacobian(f1: callable, f2: callable, x: float, y: float, eps: float = 1e-5) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/jacobian/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/jacobian/python/jacobian.py",
          "language": "python",
          "content": "def jacobian(f1, f2, x: float, y: float, eps: float = 1e-5) -> list[list[float]]:\n    df1_dx = (f1(x + eps, y) - f1(x - eps, y)) / (2 * eps)\n    df1_dy = (f1(x, y + eps) - f1(x, y - eps)) / (2 * eps)\n    df2_dx = (f2(x + eps, y) - f2(x - eps, y)) / (2 * eps)\n    df2_dy = (f2(x, y + eps) - f2(x, y - eps)) / (2 * eps)\n    return [[df1_dx, df1_dy], [df2_dx, df2_dy]]\n"
        },
        {
          "path": "modules/ml/fundamentals/jacobian/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn jacobian(f1: fn(f64, f64) -> f64, f2: fn(f64, f64) -> f64, x: f64, y: f64, eps: f64) -> [[f64; 2]; 2] {\n    let df1_dx = (f1(x + eps, y) - f1(x - eps, y)) / (2.0 * eps);\n    let df1_dy = (f1(x, y + eps) - f1(x, y - eps)) / (2.0 * eps);\n    let df2_dx = (f2(x + eps, y) - f2(x - eps, y)) / (2.0 * eps);\n    let df2_dy = (f2(x, y + eps) - f2(x, y - eps)) / (2.0 * eps);\n    [[df1_dx, df1_dy], [df2_dx, df2_dy]]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "jensen-shannon-divergence",
      "title": "Jensen\u2013Shannon Divergence",
      "path": "modules/ml/fundamentals/jensen-shannon-divergence",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Jensen\u2013Shannon Divergence\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nJS divergence symmetrizes KL using the mean distribution.\n\n## Math\n\n$$JS(p,q) = 0.5 KL(p||m)+0.5 KL(q||m)$$\n\n## Function\n\n```python\ndef js(p: list[float], q: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/jensen-shannon-divergence/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/jensen-shannon-divergence/python/jensen_shannon_divergence.py",
          "language": "python",
          "content": "import math\n\n\ndef js(p: list[float], q: list[float]) -> float:\n    m = [(pi + qi) / 2 for pi, qi in zip(p, q)]\n    def kl(a, b):\n        return sum(ai * math.log(ai / bi) for ai, bi in zip(a, b) if ai > 0 and bi > 0)\n    return 0.5 * kl(p, m) + 0.5 * kl(q, m)\n"
        },
        {
          "path": "modules/ml/fundamentals/jensen-shannon-divergence/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn js(p: &[f64], q: &[f64]) -> f64 {\n    let m: Vec<f64> = p.iter().zip(q.iter()).map(|(a, b)| (a + b) / 2.0).collect();\n    let kl = |a: &[f64], b: &[f64]| -> f64 {\n        let mut total = 0.0;\n        for (&ai, &bi) in a.iter().zip(b.iter()) {\n            if ai > 0.0 && bi > 0.0 {\n                total += ai * (ai / bi).ln();\n            }\n        }\n        total\n    };\n    0.5 * kl(p, &m) + 0.5 * kl(q, &m)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "kl-divergence",
      "title": "KL Divergence",
      "path": "modules/ml/fundamentals/kl-divergence",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# KL Divergence\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nKL divergence measures how one distribution diverges from another.\n\n## Math\n\n$$\\mathrm{KL}(p\\|q) = \\sum_i p_i \\log\\left(\\frac{p_i}{q_i}\\right)$$\n\n## Function\n\n```python\ndef kl(p: list[float], q: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/kl-divergence/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/kl-divergence/python/kl_divergence.py",
          "language": "python",
          "content": "import math\n\n\ndef kl(p: list[float], q: list[float]) -> float:\n    total = 0.0\n    for pi, qi in zip(p, q):\n        if pi > 0 and qi > 0:\n            total += pi * math.log(pi / qi)\n    return total\n"
        },
        {
          "path": "modules/ml/fundamentals/kl-divergence/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn kl(p: &[f64], q: &[f64]) -> f64 {\n    let mut total = 0.0;\n    for (&pi, &qi) in p.iter().zip(q.iter()) {\n        if pi > 0.0 && qi > 0.0 {\n            total += pi * (pi / qi).ln();\n        }\n    }\n    total\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "markov-chains",
      "title": "Markov Chains",
      "path": "modules/ml/fundamentals/markov-chains",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Markov Chains\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nMarkov chains evolve distributions via transition matrices.\n\n## Math\n\n$$p_{t+1} = p_t T$$\n\n## Function\n\n```python\ndef next_distribution(p: list[float], t: list[list[float]]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/markov-chains/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/markov-chains/python/markov_chains.py",
          "language": "python",
          "content": "def next_distribution(p: list[float], t: list[list[float]]) -> list[float]:\n    return [sum(p[j] * t[j][i] for j in range(len(p))) for i in range(len(t[0]))]\n"
        },
        {
          "path": "modules/ml/fundamentals/markov-chains/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn next_distribution(p: &[f64], t: &[Vec<f64>]) -> Vec<f64> {\n    let mut out = vec![0.0; t[0].len()];\n    for i in 0..t[0].len() {\n        let mut sum = 0.0;\n        for j in 0..p.len() {\n            sum += p[j] * t[j][i];\n        }\n        out[i] = sum;\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "mutual-information",
      "title": "Mutual Information",
      "path": "modules/ml/fundamentals/mutual-information",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Mutual Information\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nMutual information measures shared information between variables.\n\n## Math\n\n$$I(X;Y)=\\sum_{x,y} p(x,y)\\log\\left(\\frac{p(x,y)}{p(x)p(y)}\\right)$$\n\n## Function\n\n```python\ndef mutual_information(joint: list[list[float]]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/mutual-information/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/mutual-information/python/mutual_information.py",
          "language": "python",
          "content": "import math\n\n\ndef mutual_information(joint: list[list[float]]) -> float:\n    px = [sum(row) for row in joint]\n    py = [sum(joint[i][j] for i in range(len(joint))) for j in range(len(joint[0]))]\n    mi = 0.0\n    for i in range(len(joint)):\n        for j in range(len(joint[0])):\n            pxy = joint[i][j]\n            if pxy > 0 and px[i] > 0 and py[j] > 0:\n                mi += pxy * math.log(pxy / (px[i] * py[j]))\n    return mi\n"
        },
        {
          "path": "modules/ml/fundamentals/mutual-information/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn mutual_information(joint: &[Vec<f64>]) -> f64 {\n    let px: Vec<f64> = joint.iter().map(|row| row.iter().sum()).collect();\n    let mut py = vec![0.0; joint[0].len()];\n    for i in 0..joint.len() {\n        for j in 0..joint[0].len() {\n            py[j] += joint[i][j];\n        }\n    }\n    let mut mi = 0.0;\n    for i in 0..joint.len() {\n        for j in 0..joint[0].len() {\n            let pxy = joint[i][j];\n            if pxy > 0.0 && px[i] > 0.0 && py[j] > 0.0 {\n                mi += pxy * (pxy / (px[i] * py[j])).ln();\n            }\n        }\n    }\n    mi\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "newtons-method",
      "title": "Newton's Method",
      "path": "modules/ml/fundamentals/newtons-method",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Newton's Method\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nNewton's method uses second derivatives for faster convergence.\n\n## Math\n\n$$x = x - f'(x)/f''(x)$$\n\n## Function\n\n```python\ndef newton_step(x: float, f_prime: float, f_double_prime: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/newtons-method/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/newtons-method/python/newtons_method.py",
          "language": "python",
          "content": "def newton_step(x: float, f_prime: float, f_double_prime: float) -> float:\n    return x - f_prime / f_double_prime\n"
        },
        {
          "path": "modules/ml/fundamentals/newtons-method/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn newton_step(x: f64, f_prime: f64, f_double_prime: f64) -> f64 {\n    x - f_prime / f_double_prime\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "svd",
      "title": "SVD",
      "path": "modules/ml/fundamentals/svd",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# SVD\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nSVD factorizes a matrix into $U \\Sigma V^{\\top}$.\n\n## Math\n\n$$A = U \\Sigma V^{\\top}$$\n\n## Function\n\n```python\ndef singular_values_2x2(a: list[list[float]]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/svd/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/svd/python/svd.py",
          "language": "python",
          "content": "import math\n\n\ndef singular_values_2x2(a: list[list[float]]) -> list[float]:\n    # compute eigenvalues of A^T A\n    at_a = [\n        [a[0][0] ** 2 + a[1][0] ** 2, a[0][0] * a[0][1] + a[1][0] * a[1][1]],\n        [a[0][0] * a[0][1] + a[1][0] * a[1][1], a[0][1] ** 2 + a[1][1] ** 2],\n    ]\n    trace = at_a[0][0] + at_a[1][1]\n    det = at_a[0][0] * at_a[1][1] - at_a[0][1] * at_a[1][0]\n    eig1 = trace / 2 + math.sqrt(max(0.0, (trace / 2) ** 2 - det))\n    eig2 = trace / 2 - math.sqrt(max(0.0, (trace / 2) ** 2 - det))\n    return [math.sqrt(eig1), math.sqrt(eig2)]\n"
        },
        {
          "path": "modules/ml/fundamentals/svd/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn singular_values_2x2(a: [[f64; 2]; 2]) -> [f64; 2] {\n    let at_a = [\n        [a[0][0] * a[0][0] + a[1][0] * a[1][0], a[0][0] * a[0][1] + a[1][0] * a[1][1]],\n        [a[0][0] * a[0][1] + a[1][0] * a[1][1], a[0][1] * a[0][1] + a[1][1] * a[1][1]],\n    ];\n    let trace = at_a[0][0] + at_a[1][1];\n    let det = at_a[0][0] * at_a[1][1] - at_a[0][1] * at_a[1][0];\n    let disc = ((trace / 2.0).powi(2) - det).max(0.0).sqrt();\n    let eig1 = trace / 2.0 + disc;\n    let eig2 = trace / 2.0 - disc;\n    [eig1.sqrt(), eig2.sqrt()]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "two-sample-t-test",
      "title": "Two-Sample t-test",
      "path": "modules/ml/fundamentals/two-sample-t-test",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Two-Sample t-test\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nTwo-sample t-test compares means of two groups.\n\n## Math\n\n$$t = \\frac{m_1 - m_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}$$\n\n## Function\n\n```python\ndef t_stat(x: list[float], y: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/two-sample-t-test/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/two-sample-t-test/python/two_sample_t_test.py",
          "language": "python",
          "content": "import math\n\n\ndef t_stat(x: list[float], y: list[float]) -> float:\n    m1 = sum(x) / len(x)\n    m2 = sum(y) / len(y)\n    v1 = sum((v - m1) ** 2 for v in x) / (len(x) - 1)\n    v2 = sum((v - m2) ** 2 for v in y) / (len(y) - 1)\n    return (m1 - m2) / math.sqrt(v1 / len(x) + v2 / len(y))\n"
        },
        {
          "path": "modules/ml/fundamentals/two-sample-t-test/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn t_stat(x: &[f64], y: &[f64]) -> f64 {\n    let m1: f64 = x.iter().sum::<f64>() / x.len() as f64;\n    let m2: f64 = y.iter().sum::<f64>() / y.len() as f64;\n    let v1: f64 = x.iter().map(|v| (v - m1).powi(2)).sum::<f64>() / (x.len() as f64 - 1.0);\n    let v2: f64 = y.iter().map(|v| (v - m2).powi(2)).sum::<f64>() / (y.len() as f64 - 1.0);\n    (m1 - m2) / (v1 / x.len() as f64 + v2 / y.len() as f64).sqrt()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "vectors-matrices",
      "title": "Vectors and Matrices",
      "path": "modules/ml/fundamentals/vectors-matrices",
      "summary": "> Track: `ml` | Topic: `fundamentals`",
      "readme": "# Vectors and Matrices\n\n> Track: `ml` | Topic: `fundamentals`\n\n## Concept\n\nVectors and matrices represent data and linear transforms.\n\n## Math\n\n$$y = A x$$\n\n## Function\n\n```python\ndef matvec(a: list[list[float]], x: list[float]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/fundamentals/vectors-matrices/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/fundamentals/vectors-matrices/python/vectors_matrices.py",
          "language": "python",
          "content": "def matvec(a: list[list[float]], x: list[float]) -> list[float]:\n    return [sum(ai * xi for ai, xi in zip(row, x)) for row in a]\n"
        },
        {
          "path": "modules/ml/fundamentals/vectors-matrices/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn matvec(a: &[Vec<f64>], x: &[f64]) -> Vec<f64> {\n    a.iter().map(|row| row.iter().zip(x.iter()).map(|(ai, xi)| ai * xi).sum()).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "generative",
      "slug": "diffusion-guidance-tradeoffs",
      "title": "Diffusion Sampling and Guidance Trade-offs",
      "path": "modules/ml/generative/diffusion-guidance-tradeoffs",
      "summary": "> Track: `ml` | Topic: `generative`",
      "readme": "# Diffusion Sampling and Guidance Trade-offs\n\n> Track: `ml` | Topic: `generative`\n\n## Concept\n\nClassifier-free guidance is a technique that steers diffusion model sampling toward a conditioning signal (e.g., a text prompt) by amplifying the difference between the conditional and unconditional noise predictions. During training, the model learns both a conditional prediction $\\epsilon_\\theta(x_t, c)$ and an unconditional prediction $\\epsilon_\\theta(x_t)$ by randomly dropping the condition $c$. At inference, the two predictions are combined with a guidance scale $w$ that controls how strongly the condition influences generation.\n\nThe guidance scale creates a fundamental trade-off between fidelity and diversity. When $w = 0$, sampling is fully unconditional and produces maximum diversity but ignores the prompt. As $w$ increases, samples align more closely with the condition -- colors become more vivid, objects more recognizable -- but the distribution narrows and diversity drops. Very high guidance values produce oversaturated, artifact-prone images.\n\nThis trade-off mirrors the precision-recall trade-off in classification. Higher guidance improves precision (samples that match the condition) at the cost of recall (coverage of the full conditional distribution). In practice, $w$ between 3 and 15 is typical, and the optimal value depends on the application. Creative tasks favor lower guidance for variety, while tasks requiring exact prompt adherence favor higher guidance.\n\n## Math\n\nThe guided noise prediction combines conditional and unconditional estimates:\n\n$$\\tilde{\\epsilon} = (1 + w)\\,\\epsilon_\\theta(x_t, c) - w\\,\\epsilon_\\theta(x_t)$$\n\nThis is equivalent to extrapolating away from the unconditional prediction:\n\n$$\\tilde{\\epsilon} = \\epsilon_\\theta(x_t) + (1 + w)\\bigl(\\epsilon_\\theta(x_t, c) - \\epsilon_\\theta(x_t)\\bigr)$$\n\n- $w$ -- guidance scale (0 = unconditional, higher = stronger conditioning)\n- $\\epsilon_\\theta(x_t, c)$ -- noise prediction conditioned on $c$\n- $\\epsilon_\\theta(x_t)$ -- unconditional noise prediction (condition dropped)\n- $c$ -- conditioning signal (e.g., text embedding, class label)\n\n## Key Points\n\n- At $w = 0$ the model samples unconditionally; increasing $w$ trades diversity for stronger adherence to the conditioning signal.\n- Typical guidance scales are $w \\in [3, 15]$; values beyond this range tend to produce oversaturated images with artifacts.\n- Classifier-free guidance requires no external classifier -- the model is trained to handle both conditional and unconditional denoising by randomly dropping the condition.\n- The fidelity-diversity trade-off from guidance is analogous to the temperature parameter in language models: higher guidance concentrates the output distribution.\n- Guidance can be applied per-step, and dynamic schedules (e.g., higher guidance early, lower late) can improve results.\n\n## Function\n\n```python\ndef guided_step(base: float, cond: float, scale: float) -> float:\n```\n\n- `base` -- unconditional noise prediction $\\epsilon_\\theta(x_t)$\n- `cond` -- conditional noise prediction $\\epsilon_\\theta(x_t, c)$\n- `scale` -- guidance scale $w$\n\n## Run tests\n\n```bash\npytest modules/ml/generative/diffusion-guidance-tradeoffs/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/generative/diffusion-guidance-tradeoffs/python/diffusion_guidance_tradeoffs.py",
          "language": "python",
          "content": "def guided_step(base: float, cond: float, scale: float) -> float:\n    return base + scale * (cond - base)\n"
        },
        {
          "path": "modules/ml/generative/diffusion-guidance-tradeoffs/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn guided_step(base: f64, cond: f64, scale: f64) -> f64 {\n    base + scale * (cond - base)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "generative",
      "slug": "diffusion-models",
      "title": "Diffusion Models",
      "path": "modules/ml/generative/diffusion-models",
      "summary": "> Track: `ml` | Topic: `generative`",
      "readme": "# Diffusion Models\n\n> Track: `ml` | Topic: `generative`\n\n## Concept\n\nDiffusion models are a class of generative models built on two processes: a forward process that gradually adds Gaussian noise to data over $T$ timesteps until the signal is destroyed, and a learned reverse process that denoises step-by-step to recover the original data distribution. The forward process requires no learning -- it is a fixed Markov chain defined by a noise schedule $\\{\\beta_t\\}$. The reverse process is parameterized by a neural network $\\epsilon_\\theta$ trained to predict the noise added at each step.\n\nThe training objective simplifies to a denoising score-matching loss: the network learns to predict the noise $\\epsilon$ that was added to a clean sample $x_0$ to produce a noisy sample $x_t$. At generation time, the model starts from pure noise $x_T \\sim \\mathcal{N}(0, I)$ and iteratively denoises through all $T$ steps to produce a sample.\n\nDiffusion models achieve state-of-the-art image quality and diversity, surpassing GANs on benchmarks like FID. The main drawback is sampling speed, since generation requires hundreds or thousands of sequential denoising steps. Accelerated samplers such as DDIM reduce this to tens of steps by converting the stochastic process into a deterministic ODE, making diffusion models practical for real applications.\n\n## Math\n\nThe forward process adds noise at each timestep:\n\n$$q(x_t | x_{t-1}) = \\mathcal{N}\\!\\left(x_t;\\, \\sqrt{1 - \\beta_t}\\, x_{t-1},\\, \\beta_t I\\right)$$\n\nThe closed-form noisy sample at timestep $t$ is:\n\n$$x_t = \\sqrt{\\bar{\\alpha}_t}\\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\, \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n\nThe simplified training loss is:\n\n$$\\mathcal{L} = \\mathbb{E}_{t, x_0, \\epsilon}\\!\\left[\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\right]$$\n\n- $\\beta_t$ -- noise variance at step $t$\n- $\\bar{\\alpha}_t = \\prod_{s=1}^{t}(1 - \\beta_s)$ -- cumulative signal retention\n- $\\epsilon_\\theta$ -- neural network predicting the noise component\n\n## Key Points\n\n- Diffusion models produce the highest-quality samples among current generative approaches, with excellent mode coverage and diversity.\n- Sampling is slow by default ($T = 1000$ steps), but DDIM and other ODE-based solvers can reduce this to 20-50 steps with minimal quality loss.\n- The forward noising process uses a fixed schedule ($\\beta_t$) and requires no training; only the reverse denoiser is learned.\n- Diffusion models connect to score-based models through the equivalence between predicting noise and estimating the score $\\nabla_x \\log p(x)$.\n\n## Function\n\n```python\ndef add_noise(x: float, noise: float, alpha: float) -> float:\n```\n\n- `x` -- clean data sample $x_0$\n- `noise` -- Gaussian noise sample $\\epsilon$\n- `alpha` -- cumulative signal retention factor $\\bar{\\alpha}_t$\n\n## Run tests\n\n```bash\npytest modules/ml/generative/diffusion-models/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/generative/diffusion-models/python/diffusion_models.py",
          "language": "python",
          "content": "import math\n\n\ndef add_noise(x: float, noise: float, alpha: float) -> float:\n    return math.sqrt(alpha) * x + math.sqrt(1 - alpha) * noise\n"
        },
        {
          "path": "modules/ml/generative/diffusion-models/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn add_noise(x: f64, noise: f64, alpha: f64) -> f64 {\n    alpha.sqrt() * x + (1.0 - alpha).sqrt() * noise\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "generative",
      "slug": "gan-mode-collapse",
      "title": "GAN Instability and Mode Collapse",
      "path": "modules/ml/generative/gan-mode-collapse",
      "summary": "> Track: `ml` | Topic: `generative`",
      "readme": "# GAN Instability and Mode Collapse\n\n> Track: `ml` | Topic: `generative`\n\n## Concept\n\nMode collapse is the most common failure mode in GAN training. It occurs when the generator learns to map many different noise vectors $z$ to only a small number of distinct outputs, effectively \"collapsing\" onto a few modes of the data distribution while ignoring the rest. The discriminator may correctly reject these repetitive samples, but the generator oscillates between modes rather than covering the full distribution.\n\nThe root cause lies in the minimax dynamics. When the discriminator identifies a mode the generator is missing, the generator shifts to cover that mode -- but often abandons previously covered modes in the process. This cyclic behavior means the generator never settles on producing the full diversity of the training data.\n\nSeveral practical remedies exist. Minibatch discrimination lets the discriminator compare samples within a batch, penalizing low diversity directly. Unrolled GANs compute generator gradients through multiple discriminator steps, giving $G$ a longer-horizon view. Feature matching replaces the adversarial signal with a loss based on matching intermediate discriminator features. Monitoring sample entropy or pairwise distances during training is the simplest way to detect collapse early.\n\n## Math\n\nThe standard GAN value function is:\n\n$$V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]$$\n\nMode collapse occurs when $G$ maps diverse inputs to few outputs. A diversity metric based on sample variance is:\n\n$$\\text{diversity} = \\frac{1}{N}\\sum_{i=1}^{N}(s_i - \\bar{s})^2$$\n\n- $s_i$ -- the $i$-th generated sample\n- $\\bar{s}$ -- mean of the generated samples\n- $N$ -- number of samples in the evaluation batch\n\n## Key Points\n\n- Detect mode collapse by monitoring the variance or pairwise distances of generated samples; a sudden drop signals the generator is producing repetitive outputs.\n- Minibatch discrimination is a direct fix: it augments the discriminator with cross-sample statistics so it can penalize low-diversity batches.\n- Wasserstein loss (WGAN) provides smoother gradients and reduces the incentive for the generator to focus on a single mode.\n- Mode collapse and training instability are related but distinct: the generator can be unstable (oscillating losses) without collapsing, and can collapse while losses appear stable.\n- Evaluation metrics like FID and Inception Score can mask mode collapse; always inspect generated samples visually.\n\n## Function\n\n```python\ndef diversity_score(samples: list[float]) -> float:\n```\n\n- `samples` -- list of generated sample values to evaluate for diversity\n\n## Run tests\n\n```bash\npytest modules/ml/generative/gan-mode-collapse/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/generative/gan-mode-collapse/python/gan_mode_collapse.py",
          "language": "python",
          "content": "def diversity_score(samples: list[float]) -> float:\n    return len(set(samples)) / len(samples) if samples else 0.0\n"
        },
        {
          "path": "modules/ml/generative/gan-mode-collapse/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashSet;\n\npub fn diversity_score(samples: &[i32]) -> f64 {\n    if samples.is_empty() { return 0.0; }\n    let set: HashSet<i32> = samples.iter().copied().collect();\n    set.len() as f64 / samples.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "generative",
      "slug": "gan",
      "title": "GAN",
      "path": "modules/ml/generative/gan",
      "summary": "> Track: `ml` | Topic: `generative`",
      "readme": "# GAN\n\n> Track: `ml` | Topic: `generative`\n\n## Concept\n\nA Generative Adversarial Network (GAN) consists of two neural networks -- a generator $G$ and a discriminator $D$ -- trained simultaneously in a minimax game. The generator maps random noise $z$ to synthetic samples, while the discriminator tries to distinguish real data from generated data. Training alternates between updating $D$ to better classify real versus fake, and updating $G$ to better fool $D$.\n\nGANs matter because they can produce remarkably sharp, high-fidelity samples without requiring an explicit density model. The adversarial setup avoids the pixel-wise loss functions that cause blurriness in other generative models. However, this two-player game is inherently unstable: if one player becomes too strong, gradients for the other vanish and training collapses.\n\nThe original GAN objective can suffer from vanishing gradients when $D$ is optimal. Wasserstein GAN (WGAN) replaces the JS-divergence with the Earth Mover distance, providing smoother gradients and more stable training. Other stabilization techniques include spectral normalization, gradient penalties, and progressive growing.\n\n## Math\n\nThe minimax objective for the GAN value function is:\n\n$$\\min_G \\max_D \\; \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]$$\n\nThe discriminator loss for a single sample pair is:\n\n$$\\mathcal{L}_D = -\\log D(x) - \\log(1 - D(G(z)))$$\n\n- $D(x)$ -- discriminator output (probability that $x$ is real)\n- $G(z)$ -- generator output given noise $z$\n- $p_z$ -- prior distribution over latent noise (typically $\\mathcal{N}(0, I)$)\n\n## Key Points\n\n- GAN training is inherently unstable because the generator and discriminator can oscillate rather than converge to a Nash equilibrium.\n- Mode collapse is the most common failure: the generator learns to produce only a few outputs that fool the discriminator, ignoring the full data distribution.\n- Wasserstein loss with gradient penalty (WGAN-GP) provides more meaningful gradients and is often the first remedy for training instability.\n- GANs produce sharper samples than VAEs but offer no direct way to compute likelihoods or encode data into a latent space.\n\n## Function\n\n```python\ndef gan_loss(d_real: float, d_fake: float) -> float:\n```\n\n- `d_real` -- discriminator output on a real sample, $D(x)$\n- `d_fake` -- discriminator output on a generated sample, $D(G(z))$\n\n## Run tests\n\n```bash\npytest modules/ml/generative/gan/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/generative/gan/python/gan.py",
          "language": "python",
          "content": "import math\n\n\ndef gan_loss(d_real: float, d_fake: float) -> float:\n    return -math.log(d_real) - math.log(1 - d_fake)\n"
        },
        {
          "path": "modules/ml/generative/gan/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn gan_loss(d_real: f64, d_fake: f64) -> f64 {\n    -(d_real.ln()) - (1.0 - d_fake).ln()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "generative",
      "slug": "model-selection",
      "title": "Choose GAN vs VAE vs Diffusion",
      "path": "modules/ml/generative/model-selection",
      "summary": "> Track: `ml` | Topic: `generative`",
      "readme": "# Choose GAN vs VAE vs Diffusion\n\n> Track: `ml` | Topic: `generative`\n\n## Concept\n\nChoosing among GANs, VAEs, and diffusion models requires balancing several practical criteria: sample quality (fidelity), diversity (mode coverage), sampling speed, training stability, and whether a structured latent space is needed. No single model dominates on every axis, so the right choice depends on the application.\n\nGANs produce sharp, high-fidelity samples and generate them quickly in a single forward pass, but they suffer from training instability and mode collapse, and they provide no latent density or likelihood estimate. VAEs offer stable training and a smooth, interpretable latent space with tractable likelihood bounds, making them ideal for representation learning and interpolation, though their samples tend to be blurrier. Diffusion models achieve the best sample quality and mode coverage, surpassing GANs on benchmarks like FID, but at the cost of slow iterative sampling.\n\nIn practice the decision often comes down to constraints. If you need real-time generation (e.g., game assets), GANs or VAEs are preferred. If you need the best possible quality and can afford slower inference (e.g., image synthesis pipelines), diffusion models are the current state of the art. If you need a meaningful latent space for downstream tasks, VAEs are the natural choice. Hybrid approaches like latent diffusion models combine a VAE encoder with a diffusion process in the latent space to get the best of both worlds.\n\n## Math\n\nThere is no single formula for model selection. The comparison is driven by empirical metrics.\n\n| Criterion | GAN | VAE | Diffusion |\n|---|---|---|---|\n| Sample fidelity | High | Medium | Highest |\n| Mode coverage | Low-Medium | High | High |\n| Sampling speed | Fast (1 pass) | Fast (1 pass) | Slow ($T$ steps) |\n| Training stability | Unstable | Stable | Stable |\n| Latent space | No | Smooth | No |\n\nFID (Frechet Inception Distance) is the standard quantitative comparison:\n\n$$\\text{FID} = \\|\\mu_r - \\mu_g\\|^2 + \\text{Tr}\\!\\left(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{\\frac{1}{2}}\\right)$$\n\n- $\\mu_r, \\Sigma_r$ -- mean and covariance of real data features\n- $\\mu_g, \\Sigma_g$ -- mean and covariance of generated data features\n\n## Key Points\n\n- Use GANs when sampling speed and sharpness matter most and you can invest effort in stabilizing training.\n- Use VAEs when you need a smooth, interpretable latent space for interpolation, editing, or downstream representation learning.\n- Use diffusion models when sample quality is the top priority and slow sampling is acceptable.\n- Latent diffusion models (e.g., Stable Diffusion) combine VAE compression with diffusion in latent space, offering a strong practical compromise.\n- FID is the most common single metric for comparison, but always consider diversity, speed, and application-specific requirements alongside it.\n\n## Function\n\n```python\ndef choose_model(priority: str) -> str:\n```\n\n- `priority` -- a string indicating the primary requirement: `\"fidelity\"`, `\"speed\"`, `\"latent_space\"`, or `\"quality\"`\n\n## Run tests\n\n```bash\npytest modules/ml/generative/model-selection/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/generative/model-selection/python/model_selection.py",
          "language": "python",
          "content": "def choose_model(priority: str) -> str:\n    priority = priority.lower()\n    if priority == \"speed\":\n        return \"gan\"\n    if priority == \"diversity\":\n        return \"diffusion\"\n    return \"vae\"\n"
        },
        {
          "path": "modules/ml/generative/model-selection/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn choose_model(priority: &str) -> String {\n    match priority.to_lowercase().as_str() {\n        \"speed\" => \"gan\".to_string(),\n        \"diversity\" => \"diffusion\".to_string(),\n        _ => \"vae\".to_string(),\n    }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "generative",
      "slug": "vae-posterior-collapse",
      "title": "VAE Blurry Samples and Posterior Collapse",
      "path": "modules/ml/generative/vae-posterior-collapse",
      "summary": "> Track: `ml` | Topic: `generative`",
      "readme": "# VAE Blurry Samples and Posterior Collapse\n\n> Track: `ml` | Topic: `generative`\n\n## Concept\n\nPosterior collapse is a pathology in VAE training where the decoder learns to ignore the latent variable $z$ entirely. When this happens, the approximate posterior $q_\\phi(z|x)$ collapses to match the prior $p(z)$, causing the KL divergence term in the ELBO to approach zero. The model degenerates into a standard autoregressive or feedforward decoder with no meaningful latent representation.\n\nThis failure is especially common when the decoder is powerful (e.g., an autoregressive model like an LSTM or Transformer). A strong decoder can reconstruct $x$ without extracting information from $z$, so the optimizer finds it easiest to simply set $q_\\phi(z|x) \\approx p(z)$ and pay zero KL cost. The result is a latent space that carries no information and cannot be used for interpolation or controlled generation.\n\nThe most widely used remedy is KL annealing: the KL term weight starts at zero and is gradually increased to its full value over the course of training, giving the encoder time to learn useful representations before the regularization penalty kicks in. The free-bits strategy sets a minimum target for the KL per latent dimension, preventing any single dimension from collapsing. Reducing decoder capacity can also help by forcing the model to rely on $z$.\n\n## Math\n\nThe ELBO objective decomposes as:\n\n$$\\text{ELBO} = \\underbrace{\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]}_{\\text{reconstruction}} - \\underbrace{D_{KL}(q_\\phi(z|x) \\| p(z))}_{\\text{regularization}}$$\n\nDuring posterior collapse, the KL term vanishes:\n\n$$q_\\phi(z|x) \\approx p(z) \\implies D_{KL}(q_\\phi(z|x) \\| p(z)) \\approx 0$$\n\nKL annealing introduces a weight $\\lambda$ that ramps from 0 to 1:\n\n$$\\mathcal{L} = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\lambda \\cdot D_{KL}(q_\\phi(z|x) \\| p(z))$$\n\n- $\\lambda$ -- annealing coefficient, linearly increased from 0 to 1 during training\n- $q_\\phi(z|x)$ -- approximate posterior (encoder output)\n- $p(z)$ -- prior distribution, typically $\\mathcal{N}(0, I)$\n\n## Key Points\n\n- The hallmark of posterior collapse is a KL term near zero combined with poor latent representations; the decoder reconstructs without using $z$.\n- KL annealing (warmup) is the simplest fix: start with $\\lambda = 0$ and linearly ramp to $\\lambda = 1$ over training, giving the encoder time to learn before regularization dominates.\n- The free-bits strategy enforces a minimum KL per dimension (e.g., $\\geq 0.25$ nats), preventing individual latent dimensions from being ignored.\n- Stronger decoders make posterior collapse worse; deliberately limiting decoder capacity forces the model to rely on the latent code.\n- Monitoring per-dimension KL values during training is the most reliable way to detect which latent dimensions have collapsed.\n\n## Function\n\n```python\ndef kl_is_low(kl: float, threshold: float = 0.1) -> bool:\n```\n\n- `kl` -- measured KL divergence value $D_{KL}(q(z|x) \\| p(z))$\n- `threshold` -- KL value below which posterior collapse is flagged (default 0.1)\n\n## Run tests\n\n```bash\npytest modules/ml/generative/vae-posterior-collapse/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/generative/vae-posterior-collapse/python/vae_posterior_collapse.py",
          "language": "python",
          "content": "def kl_is_low(kl: float, threshold: float = 0.1) -> bool:\n    return kl < threshold\n"
        },
        {
          "path": "modules/ml/generative/vae-posterior-collapse/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn kl_is_low(kl: f64, threshold: f64) -> bool {\n    kl < threshold\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "generative",
      "slug": "vae",
      "title": "VAE",
      "path": "modules/ml/generative/vae",
      "summary": "> Track: `ml` | Topic: `generative`",
      "readme": "# VAE\n\n> Track: `ml` | Topic: `generative`\n\n## Concept\n\nA Variational Autoencoder (VAE) is a generative model that learns a latent representation by combining a probabilistic encoder with a decoder. The encoder $q_\\phi(z|x)$ maps input data to a distribution in latent space, and the decoder $p_\\theta(x|z)$ reconstructs data from latent samples. Training maximizes a lower bound on the log-likelihood, known as the Evidence Lower Bound (ELBO).\n\nThe key innovation is the reparameterization trick, which allows gradients to flow through the stochastic sampling step. Instead of sampling $z$ directly from $q_\\phi(z|x)$, the encoder outputs $\\mu$ and $\\sigma$, and $z$ is computed deterministically from an auxiliary noise variable $\\epsilon$. This makes the entire network end-to-end differentiable.\n\nVAEs produce a smooth, continuous latent space where nearby points decode to similar outputs, making them well-suited for interpolation and controlled generation. The trade-off is that samples tend to be blurrier than those from GANs, because the reconstruction loss averages over possible outputs. However, VAEs offer stable training and a principled probabilistic framework with tractable likelihood bounds.\n\n## Math\n\nThe ELBO objective that VAEs maximize is:\n\n$$\\text{ELBO} = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) \\| p(z))$$\n\nThe reparameterization trick computes latent samples as:\n\n$$z = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n\n- $q_\\phi(z|x)$ -- encoder (approximate posterior) parameterized by $\\phi$\n- $p_\\theta(x|z)$ -- decoder (likelihood) parameterized by $\\theta$\n- $p(z)$ -- prior, typically $\\mathcal{N}(0, I)$\n- $D_{KL}$ -- Kullback-Leibler divergence regularizing the latent distribution\n\n## Key Points\n\n- The ELBO balances reconstruction quality (first term) against latent regularization (KL term); tuning the weight between them controls sample sharpness versus latent smoothness.\n- VAE samples are typically blurrier than GAN outputs because the Gaussian decoder averages over modes, but training is much more stable.\n- The smooth latent space enables meaningful interpolation between data points -- a property GANs do not guarantee.\n- Posterior collapse occurs when the decoder becomes powerful enough to ignore $z$, causing the KL term to vanish (see VAE Posterior Collapse module).\n\n## Function\n\n```python\ndef elbo(recon: float, kl: float) -> float:\n```\n\n- `recon` -- reconstruction log-likelihood term, $\\mathbb{E}_{q(z|x)}[\\log p(x|z)]$\n- `kl` -- KL divergence between the approximate posterior and the prior, $D_{KL}(q(z|x) \\| p(z))$\n\n## Run tests\n\n```bash\npytest modules/ml/generative/vae/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/generative/vae/python/vae.py",
          "language": "python",
          "content": "def elbo(recon: float, kl: float) -> float:\n    return recon - kl\n"
        },
        {
          "path": "modules/ml/generative/vae/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn elbo(recon: f64, kl: f64) -> f64 {\n    recon - kl\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "attention-causal",
      "title": "Masked Attention (Causal Mask)",
      "path": "modules/ml/llm/attention-causal",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Masked Attention (Causal Mask)\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nIn autoregressive language models, each token can only attend to itself and\nprevious tokens. This is enforced by a **causal (lower-triangular) mask** applied\nto the attention scores before softmax.\n\nWithout the mask, token at position _i_ could \"see\" tokens at positions _i+1, i+2, \u2026_,\nleaking future information during training. The mask sets those attention scores\nto \u2212\u221e so that softmax drives them to zero.\n\nMulti-head attention applies the same mask to every head independently.\nThe mask is a constant for a given sequence length and can be precomputed once.\n\n## Math\n\n$$\\text{Scaled dot-product attention with causal mask:}$$\n\n```\nscores = Q @ K^T / sqrt(d_k)           # (seq_len, seq_len)\n\nmask[i][j] = 0      if j <= i\n           = -inf    if j >  i\n\nmasked = scores + mask\nweights = softmax(masked, dim=-1)       # (seq_len, seq_len)\noutput  = weights @ V                   # (seq_len, d_v)\n```\n\n- $\\text{\\texttt{Q}, \\texttt{K} \u2014 queries and keys, shape \\texttt{(seq\\_len, d\\_k)}}$\n- $\\text{\\texttt{V} \u2014 values, shape \\texttt{(seq\\_len, d\\_v)}}$\n- $\\text{\\texttt{d\\_k} \u2014 key dimension (scaling factor prevents large dot products)}$\n\n## Function\n\n```python\ndef causal_self_attention(Q, K, V) -> np.ndarray\n```\n\n- `Q` \u2014 query matrix, shape `(seq_len, d_k)`\n- `K` \u2014 key matrix, shape `(seq_len, d_k)`\n- `V` \u2014 value matrix, shape `(seq_len, d_v)`\n- Returns \u2014 output of shape `(seq_len, d_v)`, each row attending only to current and earlier positions\n\nHelper functions:\n\n```python\ndef causal_mask(seq_len) -> np.ndarray    # upper-tri of -inf\ndef softmax(x, axis=-1) -> np.ndarray     # numerically stable\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/attention-causal/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/attention-causal/python/attention.py",
          "language": "python",
          "content": "\"\"\"Causal (masked) self-attention from scratch using NumPy.\"\"\"\n\nimport numpy as np\n\n\ndef softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n    \"\"\"Numerically stable softmax.\"\"\"\n    x_max = np.max(x, axis=axis, keepdims=True)\n    e_x = np.exp(x - x_max)\n    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n\n\ndef causal_mask(seq_len: int) -> np.ndarray:\n    \"\"\"Return an upper-triangular matrix of -inf (causal mask).\n\n    Shape: (seq_len, seq_len). mask[i][j] = -inf if j > i, else 0.\n    \"\"\"\n    mask = np.zeros((seq_len, seq_len))\n    mask[np.triu_indices(seq_len, k=1)] = -np.inf\n    return mask\n\n\ndef causal_self_attention(\n    Q: np.ndarray, K: np.ndarray, V: np.ndarray\n) -> np.ndarray:\n    \"\"\"Compute causal self-attention.\n\n    Args:\n        Q: queries, shape (seq_len, d_k)\n        K: keys,    shape (seq_len, d_k)\n        V: values,  shape (seq_len, d_v)\n\n    Returns:\n        output of shape (seq_len, d_v)\n    \"\"\"\n    seq_len, d_k = Q.shape\n    scores = Q @ K.T / np.sqrt(d_k)\n    scores = scores + causal_mask(seq_len)\n    weights = softmax(scores, axis=-1)\n    return weights @ V\n"
        },
        {
          "path": "modules/ml/llm/attention-causal/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn causal_mask(seq_len: usize) -> Vec<Vec<f64>> {\n    let mut mask = vec![vec![0.0; seq_len]; seq_len];\n    for i in 0..seq_len {\n        for j in (i + 1)..seq_len {\n            mask[i][j] = f64::NEG_INFINITY;\n        }\n    }\n    mask\n}\n\nfn softmax(row: &[f64]) -> Vec<f64> {\n    let m = row\n        .iter()\n        .copied()\n        .fold(f64::NEG_INFINITY, f64::max);\n    let exps: Vec<f64> = row.iter().map(|x| (x - m).exp()).collect();\n    let sum: f64 = exps.iter().sum();\n    exps.iter().map(|e| e / sum).collect()\n}\n\nfn matmul(a: &[Vec<f64>], b: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = b[0].len();\n    let mut out = vec![vec![0.0; cols]; rows];\n    for i in 0..rows {\n        for j in 0..cols {\n            let mut acc = 0.0;\n            for k in 0..b.len() {\n                acc += a[i][k] * b[k][j];\n            }\n            out[i][j] = acc;\n        }\n    }\n    out\n}\n\nfn transpose(a: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = a[0].len();\n    let mut out = vec![vec![0.0; rows]; cols];\n    for i in 0..rows {\n        for j in 0..cols {\n            out[j][i] = a[i][j];\n        }\n    }\n    out\n}\n\npub fn causal_self_attention(q: &[Vec<f64>], k: &[Vec<f64>], v: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let dk = k[0].len() as f64;\n    let scores = matmul(q, &transpose(k));\n    let mask = causal_mask(scores.len());\n    let mut masked = scores.clone();\n    for i in 0..scores.len() {\n        for j in 0..scores[0].len() {\n            masked[i][j] = scores[i][j] / dk.sqrt() + mask[i][j];\n        }\n    }\n    let weights: Vec<Vec<f64>> = masked.iter().map(|row| softmax(row)).collect();\n    matmul(&weights, v)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "dpo",
      "title": "DPO (Direct Preference Optimization)",
      "path": "modules/ml/llm/dpo",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# DPO (Direct Preference Optimization)\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nDPO optimizes policy preferences directly using a reference model.\n\n## Math\n\n$$\\text{Demo loss: } L = -\\log\\left(\\sigma\\left(\\beta(\\Delta \\log \\pi - \\Delta \\log \\pi_{\\text{ref}})\\right)\\right)$$\n\n## Function\n\n```python\ndef dpo_loss(delta_logp: float, delta_logp_ref: float, beta: float = 0.1) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/dpo/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/dpo/python/dpo.py",
          "language": "python",
          "content": "import math\n\n\ndef dpo_loss(delta_logp: float, delta_logp_ref: float, beta: float = 0.1) -> float:\n    diff = beta * (delta_logp - delta_logp_ref)\n    return -math.log(1 / (1 + math.exp(-diff)))\n"
        },
        {
          "path": "modules/ml/llm/dpo/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn dpo_loss(delta_logp: f64, delta_logp_ref: f64, beta: f64) -> f64 {\n    let diff = beta * (delta_logp - delta_logp_ref);\n    -1.0 / (1.0 + (-diff).exp()).ln()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "embeddings",
      "title": "Embeddings",
      "path": "modules/ml/llm/embeddings",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Embeddings\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nEmbeddings map discrete token ids to continuous vectors used by the model.\n\n## Math\n\n$$Given embedding matrix \\texttt{E \u2208 R\\^\\{\\}\\{V\u00d7d\\}}, the embedding for token \\texttt{i} is \\texttt{E[i]}.$$\n\n## Function\n\n```python\ndef embed(tokens: list[int], embeddings: list[list[float]]) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/embeddings/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/embeddings/python/embeddings.py",
          "language": "python",
          "content": "from __future__ import annotations\n\n\ndef embed(tokens: list[int], embeddings: list[list[float]]) -> list[list[float]]:\n    return [embeddings[i] for i in tokens]\n"
        },
        {
          "path": "modules/ml/llm/embeddings/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn embed(tokens: &[usize], embeddings: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    tokens.iter().map(|&i| embeddings[i].clone()).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "fp16-bf16-fp8",
      "title": "FP16/BF16/FP8 Precision",
      "path": "modules/ml/llm/fp16-bf16-fp8",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# FP16/BF16/FP8 Precision\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nLower-precision formats trade accuracy for speed and memory.\n\n## Math\n\n$$\\text{Quantize a float by rounding mantissa bits (demo).}$$\n\n## Function\n\n```python\ndef quantize_fp(x: float, mantissa_bits: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/fp16-bf16-fp8/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/fp16-bf16-fp8/python/fp16_bf16_fp8.py",
          "language": "python",
          "content": "import math\n\n\ndef quantize_fp(x: float, mantissa_bits: int) -> float:\n    if x == 0.0:\n        return 0.0\n    exp = int(math.floor(math.log2(abs(x))))\n    scale = 2 ** (mantissa_bits - exp)\n    return round(x * scale) / scale\n"
        },
        {
          "path": "modules/ml/llm/fp16-bf16-fp8/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn quantize_fp(x: f64, mantissa_bits: i32) -> f64 {\n    if x == 0.0 {\n        return 0.0;\n    }\n    let exp = x.abs().log2().floor() as i32;\n    let scale = 2f64.powi(mantissa_bits - exp);\n    (x * scale).round() / scale\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "inference-head-pruning",
      "title": "Inference Head Pruning",
      "path": "modules/ml/llm/inference-head-pruning",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Inference Head Pruning\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nHead pruning removes less useful attention heads to reduce compute at inference.\n\n## Math\n\n$$\\text{Prune by slicing head blocks from weight matrices.}$$\n\n## Function\n\n```python\ndef prune_heads(weights: list[list[float]], keep: list[int], head_dim: int) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/inference-head-pruning/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/inference-head-pruning/python/inference_head_pruning.py",
          "language": "python",
          "content": "from __future__ import annotations\n\n\ndef prune_heads(weights: list[list[float]], keep: list[int], head_dim: int) -> list[list[float]]:\n    pruned = []\n    for row in weights:\n        new_row = []\n        for h in keep:\n            start = h * head_dim\n            end = start + head_dim\n            new_row.extend(row[start:end])\n        pruned.append(new_row)\n    return pruned\n"
        },
        {
          "path": "modules/ml/llm/inference-head-pruning/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn prune_heads(weights: &[Vec<f64>], keep: &[usize], head_dim: usize) -> Vec<Vec<f64>> {\n    let mut out = Vec::with_capacity(weights.len());\n    for row in weights {\n        let mut new_row = Vec::new();\n        for &h in keep {\n            let start = h * head_dim;\n            let end = start + head_dim;\n            new_row.extend_from_slice(&row[start..end]);\n        }\n        out.push(new_row);\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "int8-int4-quantization",
      "title": "INT8/INT4 Quantization",
      "path": "modules/ml/llm/int8-int4-quantization",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# INT8/INT4 Quantization\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nInteger quantization maps floats to a small integer range with a scale.\n\n## Math\n\n$$Quantize: q = round(x / s), dequantize: x\u0302 = q * s.$$\n\n## Function\n\n```python\ndef quantize_int(x: float, bits: int, scale: float) -> int:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/int8-int4-quantization/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/int8-int4-quantization/python/int8_int4_quantization.py",
          "language": "python",
          "content": "def quantize_int(x: float, bits: int, scale: float) -> int:\n    qmax = 2 ** (bits - 1) - 1\n    q = int(round(x / scale))\n    return max(-qmax, min(qmax, q))\n"
        },
        {
          "path": "modules/ml/llm/int8-int4-quantization/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn quantize_int(x: f64, bits: i32, scale: f64) -> i32 {\n    let qmax = (1 << (bits - 1)) - 1;\n    let q = (x / scale).round() as i32;\n    q.max(-qmax).min(qmax)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "kl-regularization",
      "title": "KL Regularization",
      "path": "modules/ml/llm/kl-regularization",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# KL Regularization\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nKL regularization keeps a policy close to a reference distribution.\n\n## Math\n\n$$L = \\beta\\, \\mathrm{KL}(p\\|q)$$\n\n## Function\n\n```python\ndef kl_penalty(p: list[float], q: list[float], beta: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/kl-regularization/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/kl-regularization/python/kl_regularization.py",
          "language": "python",
          "content": "import math\n\n\ndef kl_penalty(p: list[float], q: list[float], beta: float) -> float:\n    kl = 0.0\n    for pi, qi in zip(p, q):\n        if pi > 0 and qi > 0:\n            kl += pi * math.log(pi / qi)\n    return beta * kl\n"
        },
        {
          "path": "modules/ml/llm/kl-regularization/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn kl_penalty(p: &[f64], q: &[f64], beta: f64) -> f64 {\n    let mut kl = 0.0;\n    for (&pi, &qi) in p.iter().zip(q.iter()) {\n        if pi > 0.0 && qi > 0.0 {\n            kl += pi * (pi / qi).ln();\n        }\n    }\n    beta * kl\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "lora",
      "title": "LoRA",
      "path": "modules/ml/llm/lora",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# LoRA\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nLoRA applies a low-rank update $\\Delta W = AB$ while keeping base weights frozen.\n\n## Math\n\n$$W' = W + \\frac{\\alpha}{r} AB$$\n\n## Function\n\n```python\ndef lora_update(w: list[list[float]], a: list[list[float]], b: list[list[float]], alpha: float) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/lora/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/lora/python/lora.py",
          "language": "python",
          "content": "from __future__ import annotations\n\n\ndef _matmul(a: list[list[float]], b: list[list[float]]) -> list[list[float]]:\n    out = []\n    for i in range(len(a)):\n        row = []\n        for j in range(len(b[0])):\n            row.append(sum(a[i][k] * b[k][j] for k in range(len(b))))\n        out.append(row)\n    return out\n\n\ndef lora_update(w: list[list[float]], a: list[list[float]], b: list[list[float]], alpha: float) -> list[list[float]]:\n    r = len(b)\n    delta = _matmul(a, b)\n    scale = alpha / r\n    return [[w[i][j] + scale * delta[i][j] for j in range(len(w[0]))] for i in range(len(w))]\n"
        },
        {
          "path": "modules/ml/llm/lora/rust/src/lib.rs",
          "language": "rust",
          "content": "fn matmul(a: &[Vec<f64>], b: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = b[0].len();\n    let mut out = vec![vec![0.0; cols]; rows];\n    for i in 0..rows {\n        for j in 0..cols {\n            let mut acc = 0.0;\n            for k in 0..b.len() {\n                acc += a[i][k] * b[k][j];\n            }\n            out[i][j] = acc;\n        }\n    }\n    out\n}\n\npub fn lora_update(w: &[Vec<f64>], a: &[Vec<f64>], b: &[Vec<f64>], alpha: f64) -> Vec<Vec<f64>> {\n    let r = b.len() as f64;\n    let delta = matmul(a, b);\n    let scale = alpha / r;\n    let mut out = w.to_vec();\n    for i in 0..w.len() {\n        for j in 0..w[0].len() {\n            out[i][j] += scale * delta[i][j];\n        }\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "moe-routing",
      "title": "MoE Routing",
      "path": "modules/ml/llm/moe-routing",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# MoE Routing\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nMixture-of-Experts routes tokens to top-k experts based on gating scores.\n\n## Math\n\n$$Select top-k gates g_i; output = sum(g_i * expert_i).$$\n\n## Function\n\n```python\ndef moe_combine(experts: list[list[float]], gates: list[float], k: int) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/moe-routing/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/moe-routing/python/moe_routing.py",
          "language": "python",
          "content": "def moe_combine(experts: list[list[float]], gates: list[float], k: int) -> list[float]:\n    pairs = sorted(enumerate(gates), key=lambda x: x[1], reverse=True)[:k]\n    total = sum(score for _, score in pairs)\n    weights = [(idx, score / total) for idx, score in pairs]\n    out = [0.0 for _ in experts[0]]\n    for idx, w in weights:\n        for j, val in enumerate(experts[idx]):\n            out[j] += w * val\n    return out\n"
        },
        {
          "path": "modules/ml/llm/moe-routing/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn moe_combine(experts: &[Vec<f64>], gates: &[f64], k: usize) -> Vec<f64> {\n    let mut pairs: Vec<(usize, f64)> = gates.iter().copied().enumerate().collect();\n    pairs.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());\n    let top = &pairs[..k];\n    let total: f64 = top.iter().map(|(_, s)| *s).sum();\n    let mut out = vec![0.0; experts[0].len()];\n    for (idx, score) in top {\n        let weight = score / total;\n        for j in 0..experts[*idx].len() {\n            out[j] += weight * experts[*idx][j];\n        }\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "multi-head-attention",
      "title": "Multi-Head Attention",
      "path": "modules/ml/llm/multi-head-attention",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Multi-Head Attention\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nMulti-head attention runs attention in parallel subspaces and concatenates results.\n\n## Math\n\n$$Split Q,K,V into heads: head_i = Attention(Q_i,K_i,V_i).$$\n\n## Function\n\n```python\ndef multi_head_attention(q: list[list[float]], k: list[list[float]], v: list[list[float]], heads: int) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/multi-head-attention/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/multi-head-attention/python/multi_head_attention.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nimport math\n\n\ndef _softmax(row: list[float]) -> list[float]:\n    m = max(row)\n    exps = [math.exp(x - m) for x in row]\n    s = sum(exps)\n    return [e / s for e in exps]\n\n\ndef _matmul(a: list[list[float]], b: list[list[float]]) -> list[list[float]]:\n    out = []\n    for i in range(len(a)):\n        row = []\n        for j in range(len(b[0])):\n            row.append(sum(a[i][k] * b[k][j] for k in range(len(b))))\n        out.append(row)\n    return out\n\n\ndef _transpose(a: list[list[float]]) -> list[list[float]]:\n    return [list(col) for col in zip(*a)]\n\n\ndef _attention(q: list[list[float]], k: list[list[float]], v: list[list[float]]) -> list[list[float]]:\n    dk = len(k[0])\n    scores = _matmul(q, _transpose(k))\n    scaled = [[val / math.sqrt(dk) for val in row] for row in scores]\n    weights = [_softmax(row) for row in scaled]\n    return _matmul(weights, v)\n\n\ndef multi_head_attention(q: list[list[float]], k: list[list[float]], v: list[list[float]], heads: int) -> list[list[float]]:\n    d_model = len(q[0])\n    head_dim = d_model // heads\n    outputs = []\n    for h in range(heads):\n        q_h = [row[h * head_dim:(h + 1) * head_dim] for row in q]\n        k_h = [row[h * head_dim:(h + 1) * head_dim] for row in k]\n        v_h = [row[h * head_dim:(h + 1) * head_dim] for row in v]\n        outputs.append(_attention(q_h, k_h, v_h))\n    merged = []\n    for i in range(len(q)):\n        merged_row = []\n        for h in range(heads):\n            merged_row.extend(outputs[h][i])\n        merged.append(merged_row)\n    return merged\n"
        },
        {
          "path": "modules/ml/llm/multi-head-attention/rust/src/lib.rs",
          "language": "rust",
          "content": "fn softmax(row: &[f64]) -> Vec<f64> {\n    let m = row.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let exps: Vec<f64> = row.iter().map(|x| (x - m).exp()).collect();\n    let sum: f64 = exps.iter().sum();\n    exps.iter().map(|e| e / sum).collect()\n}\n\nfn matmul(a: &[Vec<f64>], b: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = b[0].len();\n    let mut out = vec![vec![0.0; cols]; rows];\n    for i in 0..rows {\n        for j in 0..cols {\n            let mut acc = 0.0;\n            for k in 0..b.len() {\n                acc += a[i][k] * b[k][j];\n            }\n            out[i][j] = acc;\n        }\n    }\n    out\n}\n\nfn transpose(a: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = a[0].len();\n    let mut out = vec![vec![0.0; rows]; cols];\n    for i in 0..rows {\n        for j in 0..cols {\n            out[j][i] = a[i][j];\n        }\n    }\n    out\n}\n\nfn attention(q: &[Vec<f64>], k: &[Vec<f64>], v: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let dk = k[0].len() as f64;\n    let scores = matmul(q, &transpose(k));\n    let scaled: Vec<Vec<f64>> = scores\n        .iter()\n        .map(|row| row.iter().map(|x| x / dk.sqrt()).collect())\n        .collect();\n    let weights: Vec<Vec<f64>> = scaled.iter().map(|row| softmax(row)).collect();\n    matmul(&weights, v)\n}\n\npub fn multi_head_attention(q: &[Vec<f64>], k: &[Vec<f64>], v: &[Vec<f64>], heads: usize) -> Vec<Vec<f64>> {\n    let d_model = q[0].len();\n    let head_dim = d_model / heads;\n    let mut outputs: Vec<Vec<Vec<f64>>> = Vec::with_capacity(heads);\n    for h in 0..heads {\n        let start = h * head_dim;\n        let end = (h + 1) * head_dim;\n        let q_h: Vec<Vec<f64>> = q.iter().map(|row| row[start..end].to_vec()).collect();\n        let k_h: Vec<Vec<f64>> = k.iter().map(|row| row[start..end].to_vec()).collect();\n        let v_h: Vec<Vec<f64>> = v.iter().map(|row| row[start..end].to_vec()).collect();\n        outputs.push(attention(&q_h, &k_h, &v_h));\n    }\n    let mut merged = vec![vec![0.0; d_model]; q.len()];\n    for i in 0..q.len() {\n        let mut offset = 0;\n        for h in 0..heads {\n            for val in &outputs[h][i] {\n                merged[i][offset] = *val;\n                offset += 1;\n            }\n        }\n    }\n    merged\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "positional-encoding",
      "title": "Positional Encoding",
      "path": "modules/ml/llm/positional-encoding",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Positional Encoding\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nPositional encodings add order information to token embeddings.\n\n## Math\n\n$$Sinusoidal: \\texttt{PE[pos,2i]=sin(pos/10000\\^\\{\\}\\{2i/d\\})}, \\texttt{PE[pos,2i+1]=cos(pos/10000\\^\\{\\}\\{2i/d\\})}.$$\n\n## Function\n\n```python\ndef sinusoidal_position(pos: int, d_model: int) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/positional-encoding/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/positional-encoding/python/positional_encoding.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nimport math\n\n\ndef sinusoidal_position(pos: int, d_model: int) -> list[float]:\n    values = []\n    for i in range(d_model):\n        angle = pos / (10000 ** (2 * (i // 2) / d_model))\n        if i % 2 == 0:\n            values.append(math.sin(angle))\n        else:\n            values.append(math.cos(angle))\n    return values\n"
        },
        {
          "path": "modules/ml/llm/positional-encoding/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn sinusoidal_position(pos: usize, d_model: usize) -> Vec<f64> {\n    let mut values = Vec::with_capacity(d_model);\n    for i in 0..d_model {\n        let angle = (pos as f64) / 10000f64.powf(2.0 * ((i / 2) as f64) / d_model as f64);\n        if i % 2 == 0 {\n            values.push(angle.sin());\n        } else {\n            values.push(angle.cos());\n        }\n    }\n    values\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "preference-learning",
      "title": "Preference Learning",
      "path": "modules/ml/llm/preference-learning",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Preference Learning\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nPreference learning optimizes models with pairwise comparisons between outputs.\n\n## Math\n\n$$\\text{Pairwise logistic loss: } L = -\\log\\left(\\sigma(\\text{score}_{\\text{chosen}} - \\text{score}_{\\text{rejected}})\\right)$$\n\n## Function\n\n```python\ndef preference_loss(score_chosen: float, score_rejected: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/preference-learning/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/preference-learning/python/preference_learning.py",
          "language": "python",
          "content": "import math\n\n\ndef preference_loss(score_chosen: float, score_rejected: float) -> float:\n    diff = score_chosen - score_rejected\n    return -math.log(1 / (1 + math.exp(-diff)))\n"
        },
        {
          "path": "modules/ml/llm/preference-learning/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn preference_loss(score_chosen: f64, score_rejected: f64) -> f64 {\n    let diff = score_chosen - score_rejected;\n    -1.0 / (1.0 + (-diff).exp()).ln()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "pretraining",
      "title": "Pretraining (Next-Token Loss)",
      "path": "modules/ml/llm/pretraining",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Pretraining (Next-Token Loss)\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nPretraining optimizes next-token prediction on large corpora.\n\n## Math\n\n$$\\text{Cross-entropy: } L = -\\log\\left(\\mathrm{softmax}(\\text{logits})_{\\text{target}}\\right)$$\n\n## Function\n\n```python\ndef next_token_loss(logits: list[list[float]], targets: list[int]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/pretraining/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/pretraining/python/pretraining.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nimport math\n\n\ndef _softmax(row: list[float]) -> list[float]:\n    m = max(row)\n    exps = [math.exp(x - m) for x in row]\n    s = sum(exps)\n    return [e / s for e in exps]\n\n\ndef next_token_loss(logits: list[list[float]], targets: list[int]) -> float:\n    total = 0.0\n    for row, tgt in zip(logits, targets):\n        probs = _softmax(row)\n        total += -math.log(probs[tgt])\n    return total / len(targets)\n"
        },
        {
          "path": "modules/ml/llm/pretraining/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn next_token_loss(logits: &[Vec<f64>], targets: &[usize]) -> f64 {\n    let mut total = 0.0;\n    for (row, &tgt) in logits.iter().zip(targets.iter()) {\n        let m = row.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n        let exps: Vec<f64> = row.iter().map(|x| (x - m).exp()).collect();\n        let sum: f64 = exps.iter().sum();\n        let prob = exps[tgt] / sum;\n        total += -prob.ln();\n    }\n    total / targets.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "ptx-anchoring",
      "title": "PTX Anchoring",
      "path": "modules/ml/llm/ptx-anchoring",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# PTX Anchoring\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nPTX anchoring mixes a small amount of pretraining loss during alignment to retain base behavior.\n\n## Math\n\n$$L = (1-\\alpha) L_{\\text{align}} + \\alpha L_{\\text{ptx}}$$\n\n## Function\n\n```python\ndef anchored_loss(align_loss: float, ptx_loss: float, alpha: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/ptx-anchoring/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/ptx-anchoring/python/ptx_anchoring.py",
          "language": "python",
          "content": "def anchored_loss(align_loss: float, ptx_loss: float, alpha: float) -> float:\n    return (1 - alpha) * align_loss + alpha * ptx_loss\n"
        },
        {
          "path": "modules/ml/llm/ptx-anchoring/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn anchored_loss(align_loss: f64, ptx_loss: f64, alpha: f64) -> f64 {\n    (1.0 - alpha) * align_loss + alpha * ptx_loss\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "qlora",
      "title": "QLoRA",
      "path": "modules/ml/llm/qlora",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# QLoRA\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nQLoRA combines low-rank updates with quantized base weights.\n\n## Math\n\n$$\\text{Quantize base weights, then apply LoRA-style low-rank update.}$$\n\n## Function\n\n```python\ndef qlora_update(w: list[list[float]], a: list[list[float]], b: list[list[float]], alpha: float, scale: float) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/qlora/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/qlora/python/qlora.py",
          "language": "python",
          "content": "from __future__ import annotations\n\n\ndef _matmul(a: list[list[float]], b: list[list[float]]) -> list[list[float]]:\n    out = []\n    for i in range(len(a)):\n        row = []\n        for j in range(len(b[0])):\n            row.append(sum(a[i][k] * b[k][j] for k in range(len(b))))\n        out.append(row)\n    return out\n\n\ndef _quantize(w: list[list[float]], scale: float) -> list[list[float]]:\n    return [[round(val / scale) * scale for val in row] for row in w]\n\n\ndef qlora_update(w: list[list[float]], a: list[list[float]], b: list[list[float]], alpha: float, scale: float) -> list[list[float]]:\n    w_q = _quantize(w, scale)\n    r = len(b)\n    delta = _matmul(a, b)\n    factor = alpha / r\n    return [[w_q[i][j] + factor * delta[i][j] for j in range(len(w_q[0]))] for i in range(len(w_q))]\n"
        },
        {
          "path": "modules/ml/llm/qlora/rust/src/lib.rs",
          "language": "rust",
          "content": "fn matmul(a: &[Vec<f64>], b: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = b[0].len();\n    let mut out = vec![vec![0.0; cols]; rows];\n    for i in 0..rows {\n        for j in 0..cols {\n            let mut acc = 0.0;\n            for k in 0..b.len() {\n                acc += a[i][k] * b[k][j];\n            }\n            out[i][j] = acc;\n        }\n    }\n    out\n}\n\nfn quantize(w: &[Vec<f64>], scale: f64) -> Vec<Vec<f64>> {\n    w.iter()\n        .map(|row| row.iter().map(|v| (v / scale).round() * scale).collect())\n        .collect()\n}\n\npub fn qlora_update(w: &[Vec<f64>], a: &[Vec<f64>], b: &[Vec<f64>], alpha: f64, scale: f64) -> Vec<Vec<f64>> {\n    let w_q = quantize(w, scale);\n    let r = b.len() as f64;\n    let delta = matmul(a, b);\n    let factor = alpha / r;\n    let mut out = w_q;\n    for i in 0..out.len() {\n        for j in 0..out[0].len() {\n            out[i][j] += factor * delta[i][j];\n        }\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "rlhf",
      "title": "RLHF (Reward Model Loss)",
      "path": "modules/ml/llm/rlhf",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# RLHF (Reward Model Loss)\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nRLHF uses a reward model trained on preference pairs before policy optimization.\n\n## Math\n\n$$\\text{Reward model loss uses the pairwise logistic objective.}$$\n\n## Function\n\n```python\ndef reward_model_loss(chosen: float, rejected: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/rlhf/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/rlhf/python/rlhf.py",
          "language": "python",
          "content": "import math\n\n\ndef reward_model_loss(chosen: float, rejected: float) -> float:\n    diff = chosen - rejected\n    return -math.log(1 / (1 + math.exp(-diff)))\n"
        },
        {
          "path": "modules/ml/llm/rlhf/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn reward_model_loss(chosen: f64, rejected: f64) -> f64 {\n    let diff = chosen - rejected;\n    -1.0 / (1.0 + (-diff).exp()).ln()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "self-attention",
      "title": "Self-Attention",
      "path": "modules/ml/llm/self-attention",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Self-Attention\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nSelf-attention lets each token attend to all others by comparing queries and keys.\n\n## Math\n\n$$\\mathrm{Attention}(Q,K,V) = \\mathrm{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V$$\n\n## Function\n\n```python\ndef self_attention(q: list[list[float]], k: list[list[float]], v: list[list[float]]) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/self-attention/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/self-attention/python/self_attention.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nimport math\n\n\ndef _softmax(row: list[float]) -> list[float]:\n    m = max(row)\n    exps = [math.exp(x - m) for x in row]\n    s = sum(exps)\n    return [e / s for e in exps]\n\n\ndef _matmul(a: list[list[float]], b: list[list[float]]) -> list[list[float]]:\n    out = []\n    for i in range(len(a)):\n        row = []\n        for j in range(len(b[0])):\n            row.append(sum(a[i][k] * b[k][j] for k in range(len(b))))\n        out.append(row)\n    return out\n\n\ndef _transpose(a: list[list[float]]) -> list[list[float]]:\n    return [list(col) for col in zip(*a)]\n\n\ndef self_attention(q: list[list[float]], k: list[list[float]], v: list[list[float]]) -> list[list[float]]:\n    dk = len(k[0])\n    scores = _matmul(q, _transpose(k))\n    scaled = [[val / math.sqrt(dk) for val in row] for row in scores]\n    weights = [_softmax(row) for row in scaled]\n    return _matmul(weights, v)\n"
        },
        {
          "path": "modules/ml/llm/self-attention/rust/src/lib.rs",
          "language": "rust",
          "content": "fn softmax(row: &[f64]) -> Vec<f64> {\n    let m = row.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let exps: Vec<f64> = row.iter().map(|x| (x - m).exp()).collect();\n    let sum: f64 = exps.iter().sum();\n    exps.iter().map(|e| e / sum).collect()\n}\n\nfn matmul(a: &[Vec<f64>], b: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = b[0].len();\n    let mut out = vec![vec![0.0; cols]; rows];\n    for i in 0..rows {\n        for j in 0..cols {\n            let mut acc = 0.0;\n            for k in 0..b.len() {\n                acc += a[i][k] * b[k][j];\n            }\n            out[i][j] = acc;\n        }\n    }\n    out\n}\n\nfn transpose(a: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = a[0].len();\n    let mut out = vec![vec![0.0; rows]; cols];\n    for i in 0..rows {\n        for j in 0..cols {\n            out[j][i] = a[i][j];\n        }\n    }\n    out\n}\n\npub fn self_attention(q: &[Vec<f64>], k: &[Vec<f64>], v: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let dk = k[0].len() as f64;\n    let scores = matmul(q, &transpose(k));\n    let scaled: Vec<Vec<f64>> = scores\n        .iter()\n        .map(|row| row.iter().map(|x| x / dk.sqrt()).collect())\n        .collect();\n    let weights: Vec<Vec<f64>> = scaled.iter().map(|row| softmax(row)).collect();\n    matmul(&weights, v)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "sparse-attention",
      "title": "Sparse Attention",
      "path": "modules/ml/llm/sparse-attention",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Sparse Attention\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nSparse attention limits attention to a local window to reduce complexity.\n\n## Math\n\n$$Mask scores outside a window so softmax only sees nearby positions.$$\n\n## Function\n\n```python\ndef window_mask(seq_len: int, window: int) -> list[list[int]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/sparse-attention/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/sparse-attention/python/sparse_attention.py",
          "language": "python",
          "content": "def window_mask(seq_len: int, window: int) -> list[list[int]]:\n    mask = [[0 for _ in range(seq_len)] for _ in range(seq_len)]\n    for i in range(seq_len):\n        for j in range(seq_len):\n            if abs(i - j) <= window:\n                mask[i][j] = 1\n    return mask\n"
        },
        {
          "path": "modules/ml/llm/sparse-attention/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn window_mask(seq_len: usize, window: usize) -> Vec<Vec<i32>> {\n    let mut mask = vec![vec![0; seq_len]; seq_len];\n    for i in 0..seq_len {\n        for j in 0..seq_len {\n            let dist = if i > j { i - j } else { j - i };\n            if dist <= window {\n                mask[i][j] = 1;\n            }\n        }\n    }\n    mask\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "supervised-fine-tuning",
      "title": "Supervised Fine-Tuning",
      "path": "modules/ml/llm/supervised-fine-tuning",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Supervised Fine-Tuning\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nSFT trains on curated prompt-response pairs with teacher forcing.\n\n## Math\n\n$$\\text{Masked cross-entropy over supervised target tokens.}$$\n\n## Function\n\n```python\ndef sft_loss(logits: list[list[float]], targets: list[int], mask: list[int]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/supervised-fine-tuning/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/supervised-fine-tuning/python/supervised_fine_tuning.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nimport math\n\n\ndef _softmax(row: list[float]) -> list[float]:\n    m = max(row)\n    exps = [math.exp(x - m) for x in row]\n    s = sum(exps)\n    return [e / s for e in exps]\n\n\ndef sft_loss(logits: list[list[float]], targets: list[int], mask: list[int]) -> float:\n    total = 0.0\n    count = 0\n    for row, tgt, keep in zip(logits, targets, mask):\n        if not keep:\n            continue\n        probs = _softmax(row)\n        total += -math.log(probs[tgt])\n        count += 1\n    return total / max(count, 1)\n"
        },
        {
          "path": "modules/ml/llm/supervised-fine-tuning/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn sft_loss(logits: &[Vec<f64>], targets: &[usize], mask: &[u8]) -> f64 {\n    let mut total = 0.0;\n    let mut count = 0;\n    for ((row, &tgt), &keep) in logits.iter().zip(targets.iter()).zip(mask.iter()) {\n        if keep == 0 {\n            continue;\n        }\n        let m = row.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n        let exps: Vec<f64> = row.iter().map(|x| (x - m).exp()).collect();\n        let sum: f64 = exps.iter().sum();\n        let prob = exps[tgt] / sum;\n        total += -prob.ln();\n        count += 1;\n    }\n    if count == 0 { 0.0 } else { total / count as f64 }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "tokenization",
      "title": "Tokenization",
      "path": "modules/ml/llm/tokenization",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Tokenization\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nTokenization maps raw text into discrete tokens that a model can embed.\n\n## Math\n\n$$Let \\texttt{V} be the vocabulary. Tokenization is a mapping \\texttt{f: text -> [id\\_1, ..., id\\_n]}.$$\n\n## Function\n\n```python\ndef tokenize(text: str, vocab: dict[str, int]) -> list[int]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/tokenization/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/tokenization/python/tokenization.py",
          "language": "python",
          "content": "from __future__ import annotations\n\ndef build_vocab(texts: list[str]) -> dict[str, int]:\n    vocab: dict[str, int] = {}\n    for text in texts:\n        for token in text.lower().split():\n            if token not in vocab:\n                vocab[token] = len(vocab)\n    return vocab\n\n\ndef tokenize(text: str, vocab: dict[str, int]) -> list[int]:\n    return [vocab[token] for token in text.lower().split() if token in vocab]\n"
        },
        {
          "path": "modules/ml/llm/tokenization/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashMap;\n\npub fn build_vocab(texts: &[&str]) -> HashMap<String, usize> {\n    let mut vocab = HashMap::new();\n    for text in texts {\n        for token in text.to_lowercase().split_whitespace() {\n            if !vocab.contains_key(token) {\n                let id = vocab.len();\n                vocab.insert(token.to_string(), id);\n            }\n        }\n    }\n    vocab\n}\n\npub fn tokenize(text: &str, vocab: &HashMap<String, usize>) -> Vec<usize> {\n    text.to_lowercase()\n        .split_whitespace()\n        .filter_map(|tok| vocab.get(tok).copied())\n        .collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "transformer",
      "title": "Transformer Block",
      "path": "modules/ml/llm/transformer",
      "summary": "> Track: `ml` | Topic: `llm`",
      "readme": "# Transformer Block\n\n> Track: `ml` | Topic: `llm`\n\n## Concept\n\nA transformer block applies self-attention, a feed-forward network, and residual connections.\n\n## Math\n\n$$Block(x) = x + Attention(x); then x = x + FFN(x).$$\n\n## Function\n\n```python\ndef transformer_block(x: list[list[float]], w1: list[list[float]], w2: list[list[float]]) -> list[list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/llm/transformer/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/llm/transformer/python/transformer.py",
          "language": "python",
          "content": "from __future__ import annotations\n\nimport math\n\n\ndef _softmax(row: list[float]) -> list[float]:\n    m = max(row)\n    exps = [math.exp(x - m) for x in row]\n    s = sum(exps)\n    return [e / s for e in exps]\n\n\ndef _matmul(a: list[list[float]], b: list[list[float]]) -> list[list[float]]:\n    out = []\n    for i in range(len(a)):\n        row = []\n        for j in range(len(b[0])):\n            row.append(sum(a[i][k] * b[k][j] for k in range(len(b))))\n        out.append(row)\n    return out\n\n\ndef _transpose(a: list[list[float]]) -> list[list[float]]:\n    return [list(col) for col in zip(*a)]\n\n\ndef _self_attention(x: list[list[float]]) -> list[list[float]]:\n    dk = len(x[0])\n    scores = _matmul(x, _transpose(x))\n    scaled = [[val / math.sqrt(dk) for val in row] for row in scores]\n    weights = [_softmax(row) for row in scaled]\n    return _matmul(weights, x)\n\n\ndef _relu_row(row: list[float]) -> list[float]:\n    return [max(0.0, x) for x in row]\n\n\ndef transformer_block(x: list[list[float]], w1: list[list[float]], w2: list[list[float]]) -> list[list[float]]:\n    attn = _self_attention(x)\n    res1 = [[x[i][j] + attn[i][j] for j in range(len(x[0]))] for i in range(len(x))]\n    ffn_hidden = _matmul(res1, w1)\n    ffn_hidden = [_relu_row(row) for row in ffn_hidden]\n    ffn = _matmul(ffn_hidden, w2)\n    res2 = [[res1[i][j] + ffn[i][j] for j in range(len(ffn[0]))] for i in range(len(ffn))]\n    return res2\n"
        },
        {
          "path": "modules/ml/llm/transformer/rust/src/lib.rs",
          "language": "rust",
          "content": "fn softmax(row: &[f64]) -> Vec<f64> {\n    let m = row.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let exps: Vec<f64> = row.iter().map(|x| (x - m).exp()).collect();\n    let sum: f64 = exps.iter().sum();\n    exps.iter().map(|e| e / sum).collect()\n}\n\nfn matmul(a: &[Vec<f64>], b: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = b[0].len();\n    let mut out = vec![vec![0.0; cols]; rows];\n    for i in 0..rows {\n        for j in 0..cols {\n            let mut acc = 0.0;\n            for k in 0..b.len() {\n                acc += a[i][k] * b[k][j];\n            }\n            out[i][j] = acc;\n        }\n    }\n    out\n}\n\nfn transpose(a: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let rows = a.len();\n    let cols = a[0].len();\n    let mut out = vec![vec![0.0; rows]; cols];\n    for i in 0..rows {\n        for j in 0..cols {\n            out[j][i] = a[i][j];\n        }\n    }\n    out\n}\n\nfn relu(row: &[f64]) -> Vec<f64> {\n    row.iter().map(|x| x.max(0.0)).collect()\n}\n\nfn self_attention(x: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let dk = x[0].len() as f64;\n    let scores = matmul(x, &transpose(x));\n    let scaled: Vec<Vec<f64>> = scores\n        .iter()\n        .map(|row| row.iter().map(|x| x / dk.sqrt()).collect())\n        .collect();\n    let weights: Vec<Vec<f64>> = scaled.iter().map(|row| softmax(row)).collect();\n    matmul(&weights, x)\n}\n\npub fn transformer_block(x: &[Vec<f64>], w1: &[Vec<f64>], w2: &[Vec<f64>]) -> Vec<Vec<f64>> {\n    let attn = self_attention(x);\n    let mut res1 = x.to_vec();\n    for i in 0..res1.len() {\n        for j in 0..res1[0].len() {\n            res1[i][j] += attn[i][j];\n        }\n    }\n    let ffn_hidden = matmul(&res1, w1);\n    let ffn_hidden: Vec<Vec<f64>> = ffn_hidden.iter().map(|row| relu(row)).collect();\n    let ffn = matmul(&ffn_hidden, w2);\n    let mut res2 = res1.clone();\n    for i in 0..res2.len() {\n        for j in 0..res2[0].len() {\n            res2[i][j] += ffn[i][j];\n        }\n    }\n    res2\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "ab-testing",
      "title": "A/B Testing",
      "path": "modules/ml/mlops/ab-testing",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# A/B Testing\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nA/B testing compares conversion rates between variants.\n\n## Math\n\n$$rate = conversions / trials$$\n\n## Function\n\n```python\ndef conversion_rate(conversions: int, trials: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/ab-testing/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/ab-testing/python/ab_testing.py",
          "language": "python",
          "content": "def conversion_rate(conversions: int, trials: int) -> float:\n    return conversions / trials if trials > 0 else 0.0\n"
        },
        {
          "path": "modules/ml/mlops/ab-testing/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn conversion_rate(conversions: i32, trials: i32) -> f64 {\n    if trials > 0 { conversions as f64 / trials as f64 } else { 0.0 }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "batch-vs-realtime",
      "title": "Batch vs Real-Time Inference",
      "path": "modules/ml/mlops/batch-vs-realtime",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# Batch vs Real-Time Inference\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nBatching trades latency for throughput.\n\n## Math\n\n$$throughput_batch > throughput_realtime$$\n\n## Function\n\n```python\ndef choose_mode(batch_size: int) -> str:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/batch-vs-realtime/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/batch-vs-realtime/python/batch_vs_realtime.py",
          "language": "python",
          "content": "def choose_mode(batch_size: int) -> str:\n    return \"batch\" if batch_size > 1 else \"realtime\"\n"
        },
        {
          "path": "modules/ml/mlops/batch-vs-realtime/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn choose_mode(batch_size: usize) -> String {\n    if batch_size > 1 { \"batch\".to_string() } else { \"realtime\".to_string() }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "canary-deployment",
      "title": "Canary Deployment",
      "path": "modules/ml/mlops/canary-deployment",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# Canary Deployment\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nCanary routes a small fraction of traffic to a new model.\n\n## Math\n\n$$new_pct = new / total$$\n\n## Function\n\n```python\ndef split_traffic(total: int, canary_pct: float) -> tuple[int, int]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/canary-deployment/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/canary-deployment/python/canary_deployment.py",
          "language": "python",
          "content": "def split_traffic(total: int, canary_pct: float) -> tuple[int, int]:\n    canary = int(total * canary_pct)\n    return canary, total - canary\n"
        },
        {
          "path": "modules/ml/mlops/canary-deployment/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn split_traffic(total: i32, canary_pct: f64) -> (i32, i32) {\n    let canary = (total as f64 * canary_pct) as i32;\n    (canary, total - canary)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "data-quality-checks",
      "title": "Data Quality Checks",
      "path": "modules/ml/mlops/data-quality-checks",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# Data Quality Checks\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nCheck missing values or out-of-range features.\n\n## Math\n\n$$missing_rate = missing / N$$\n\n## Function\n\n```python\ndef missing_rate(values: list[float | None]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/data-quality-checks/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/data-quality-checks/python/data_quality_checks.py",
          "language": "python",
          "content": "from typing import Optional\n\n\ndef missing_rate(values: list[Optional[float]]) -> float:\n    missing = sum(1 for v in values if v is None)\n    return missing / len(values)\n"
        },
        {
          "path": "modules/ml/mlops/data-quality-checks/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn missing_rate(values: &[Option<f64>]) -> f64 {\n    let missing = values.iter().filter(|v| v.is_none()).count();\n    missing as f64 / values.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "etl-pipeline",
      "title": "ETL Pipeline",
      "path": "modules/ml/mlops/etl-pipeline",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# ETL Pipeline\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nETL extracts raw data, transforms it, and loads it for training or serving.\n\n## Math\n\n$$x' = \\frac{x - \\mu}{\\sigma}$$\n\n## Function\n\n```python\ndef normalize(values: list[float]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/etl-pipeline/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/etl-pipeline/python/etl_pipeline.py",
          "language": "python",
          "content": "import math\n\n\ndef normalize(values: list[float]) -> list[float]:\n    mean = sum(values) / len(values)\n    var = sum((v - mean) ** 2 for v in values) / len(values)\n    std = math.sqrt(var) if var > 0 else 1.0\n    return [(v - mean) / std for v in values]\n"
        },
        {
          "path": "modules/ml/mlops/etl-pipeline/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn normalize(values: &[f64]) -> Vec<f64> {\n    let mean: f64 = values.iter().sum::<f64>() / values.len() as f64;\n    let var: f64 = values.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / values.len() as f64;\n    let std = if var > 0.0 { var.sqrt() } else { 1.0 };\n    values.iter().map(|v| (v - mean) / std).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "feature-drift-psi",
      "title": "Feature Drift Detection (PSI)",
      "path": "modules/ml/mlops/feature-drift-psi",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# Feature Drift Detection (PSI)\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nPSI compares expected vs actual distributions.\n\n## Math\n\n$$\\mathrm{PSI} = \\sum_i (a_i - e_i) \\ln\\left(\\frac{a_i}{e_i}\\right)$$\n\n## Function\n\n```python\ndef psi(expected: list[float], actual: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/feature-drift-psi/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/feature-drift-psi/python/feature_drift_psi.py",
          "language": "python",
          "content": "import math\n\n\ndef psi(expected: list[float], actual: list[float]) -> float:\n    total = 0.0\n    for e, a in zip(expected, actual):\n        if e > 0 and a > 0:\n            total += (a - e) * math.log(a / e)\n    return total\n"
        },
        {
          "path": "modules/ml/mlops/feature-drift-psi/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn psi(expected: &[f64], actual: &[f64]) -> f64 {\n    let mut total = 0.0;\n    for (&e, &a) in expected.iter().zip(actual.iter()) {\n        if e > 0.0 && a > 0.0 {\n            total += (a - e) * (a / e).ln();\n        }\n    }\n    total\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "offline-online-inference",
      "title": "Offline vs Online Inference",
      "path": "modules/ml/mlops/offline-online-inference",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# Offline vs Online Inference\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nOffline runs in batches; online responds per request.\n\n## Math\n\n$$latency_online << latency_batch$$\n\n## Function\n\n```python\ndef is_online(latency_ms: float, threshold_ms: float = 100.0) -> bool:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/offline-online-inference/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/offline-online-inference/python/offline_online_inference.py",
          "language": "python",
          "content": "def is_online(latency_ms: float, threshold_ms: float = 100.0) -> bool:\n    return latency_ms <= threshold_ms\n"
        },
        {
          "path": "modules/ml/mlops/offline-online-inference/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn is_online(latency_ms: f64, threshold_ms: f64) -> bool {\n    latency_ms <= threshold_ms\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "prediction-monitoring",
      "title": "Prediction Distribution Monitoring",
      "path": "modules/ml/mlops/prediction-monitoring",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# Prediction Distribution Monitoring\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nTrack shifts in prediction mean or variance over time.\n\n## Math\n\n$$\\Delta \\mu = \\left|\\mu_{\\text{new}} - \\mu_{\\text{old}}\\right|$$\n\n## Function\n\n```python\ndef mean_shift(old: list[float], new: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/prediction-monitoring/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/prediction-monitoring/python/prediction_monitoring.py",
          "language": "python",
          "content": "def mean_shift(old: list[float], new: list[float]) -> float:\n    mean_old = sum(old) / len(old)\n    mean_new = sum(new) / len(new)\n    return abs(mean_new - mean_old)\n"
        },
        {
          "path": "modules/ml/mlops/prediction-monitoring/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn mean_shift(old: &[f64], new: &[f64]) -> f64 {\n    let mean_old: f64 = old.iter().sum::<f64>() / old.len() as f64;\n    let mean_new: f64 = new.iter().sum::<f64>() / new.len() as f64;\n    (mean_new - mean_old).abs()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "request-batching",
      "title": "Request Batching",
      "path": "modules/ml/mlops/request-batching",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# Request Batching\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nBatch requests to improve throughput.\n\n## Math\n\n$$batches = ceil(n / batch_size)$$\n\n## Function\n\n```python\ndef batch_requests(n: int, batch_size: int) -> int:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/request-batching/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/request-batching/python/request_batching.py",
          "language": "python",
          "content": "import math\n\n\ndef batch_requests(n: int, batch_size: int) -> int:\n    return math.ceil(n / batch_size)\n"
        },
        {
          "path": "modules/ml/mlops/request-batching/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn batch_requests(n: usize, batch_size: usize) -> usize {\n    (n + batch_size - 1) / batch_size\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "sla-metrics",
      "title": "SLA Metrics",
      "path": "modules/ml/mlops/sla-metrics",
      "summary": "> Track: `ml` | Topic: `mlops`",
      "readme": "# SLA Metrics\n\n> Track: `ml` | Topic: `mlops`\n\n## Concept\n\nSLA metrics track latency or error thresholds.\n\n## Math\n\n$$violation_rate = violations / total$$\n\n## Function\n\n```python\ndef violation_rate(violations: int, total: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/mlops/sla-metrics/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/mlops/sla-metrics/python/sla_metrics.py",
          "language": "python",
          "content": "def violation_rate(violations: int, total: int) -> float:\n    return violations / total if total > 0 else 0.0\n"
        },
        {
          "path": "modules/ml/mlops/sla-metrics/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn violation_rate(violations: i32, total: i32) -> f64 {\n    if total > 0 { violations as f64 / total as f64 } else { 0.0 }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "adaboost",
      "title": "AdaBoost",
      "path": "modules/ml/models/adaboost",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# AdaBoost\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nAdaBoost reweights samples to focus on errors.\n\n## Math\n\n$$w_i \\leftarrow w_i \\cdot \\exp\\left(\\alpha \\mathbb{I}[\\text{misclassified}]\\right)$$\n\n## Function\n\n```python\ndef update_weights(weights: list[float], errors: list[int], alpha: float) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/adaboost/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/adaboost/python/adaboost.py",
          "language": "python",
          "content": "import math\n\n\ndef update_weights(weights: list[float], errors: list[int], alpha: float) -> list[float]:\n    updated = [w * math.exp(alpha * e) for w, e in zip(weights, errors)]\n    s = sum(updated)\n    return [w / s for w in updated]\n"
        },
        {
          "path": "modules/ml/models/adaboost/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn update_weights(weights: &[f64], errors: &[i32], alpha: f64) -> Vec<f64> {\n    let mut updated: Vec<f64> = weights\n        .iter()\n        .zip(errors.iter())\n        .map(|(w, e)| w * (alpha * (*e as f64)).exp())\n        .collect();\n    let sum: f64 = updated.iter().sum();\n    for w in updated.iter_mut() {\n        *w /= sum;\n    }\n    updated\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "bernoulli-naive-bayes",
      "title": "Bernoulli Naive Bayes",
      "path": "modules/ml/models/bernoulli-naive-bayes",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Bernoulli Naive Bayes\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nBernoulli NB models binary features per class.\n\n## Math\n\n$$p(x|y)=\u220f p_i^{x_i} (1-p_i)^{1-x_i}$$\n\n## Function\n\n```python\ndef bernoulli_log_likelihood(x: list[int], prob: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/bernoulli-naive-bayes/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/bernoulli-naive-bayes/python/bernoulli_naive_bayes.py",
          "language": "python",
          "content": "import math\n\n\ndef bernoulli_log_likelihood(x: list[int], prob: list[float]) -> float:\n    ll = 0.0\n    for xi, p in zip(x, prob):\n        ll += xi * math.log(p) + (1 - xi) * math.log(1 - p)\n    return ll\n"
        },
        {
          "path": "modules/ml/models/bernoulli-naive-bayes/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn bernoulli_log_likelihood(x: &[i32], prob: &[f64]) -> f64 {\n    let mut ll = 0.0;\n    for (&xi, &p) in x.iter().zip(prob.iter()) {\n        ll += (xi as f64) * p.ln() + (1.0 - xi as f64) * (1.0 - p).ln();\n    }\n    ll\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "dbscan",
      "title": "DBSCAN",
      "path": "modules/ml/models/dbscan",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# DBSCAN\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nDBSCAN groups points by density using eps-neighborhoods.\n\n## Math\n\n$$\\mathrm{neighbors}(x) = \\{y \\mid \\mathrm{dist}(x,y) \\le \\varepsilon\\}$$\n\n## Function\n\n```python\ndef neighbors(points: list[list[float]], idx: int, eps: float) -> list[int]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/dbscan/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/dbscan/python/dbscan.py",
          "language": "python",
          "content": "def neighbors(points: list[list[float]], idx: int, eps: float) -> list[int]:\n    base = points[idx]\n    out = []\n    for i, p in enumerate(points):\n        dist = sum((a - b) ** 2 for a, b in zip(base, p)) ** 0.5\n        if dist <= eps:\n            out.append(i)\n    return out\n"
        },
        {
          "path": "modules/ml/models/dbscan/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn neighbors(points: &[Vec<f64>], idx: usize, eps: f64) -> Vec<usize> {\n    let base = &points[idx];\n    let mut out = Vec::new();\n    for (i, p) in points.iter().enumerate() {\n        let dist = base\n            .iter()\n            .zip(p.iter())\n            .map(|(a, b)| (a - b).powi(2))\n            .sum::<f64>()\n            .sqrt();\n        if dist <= eps {\n            out.push(i);\n        }\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "decision-trees",
      "title": "Decision Trees",
      "path": "modules/ml/models/decision-trees",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Decision Trees\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nDecision trees split data to reduce impurity.\n\n## Math\n\n$$\\mathrm{Gini} = 1 - \\sum_c p_c^2$$\n\n## Function\n\n```python\ndef gini_impurity(labels: list[int]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/decision-trees/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/decision-trees/python/decision_trees.py",
          "language": "python",
          "content": "from collections import Counter\n\n\ndef gini_impurity(labels: list[int]) -> float:\n    counts = Counter(labels)\n    n = len(labels)\n    return 1 - sum((c / n) ** 2 for c in counts.values())\n"
        },
        {
          "path": "modules/ml/models/decision-trees/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashMap;\n\npub fn gini_impurity(labels: &[i32]) -> f64 {\n    let mut counts: HashMap<i32, usize> = HashMap::new();\n    for label in labels {\n        *counts.entry(*label).or_insert(0) += 1;\n    }\n    let n = labels.len() as f64;\n    let mut sum_sq = 0.0;\n    for cnt in counts.values() {\n        let p = *cnt as f64 / n;\n        sum_sq += p * p;\n    }\n    1.0 - sum_sq\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "elastic-net",
      "title": "Elastic Net",
      "path": "modules/ml/models/elastic-net",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Elastic Net\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nElastic Net combines L1 (Lasso) and L2 (Ridge) penalties to balance sparsity and stability.\n\n## Math\n\n$$L = L_0 + \\lambda_1 \\lVert w \\rVert_1 + \\lambda_2 \\lVert w \\rVert_2^2$$\n\n## Function\n\n```python\ndef elastic_net_penalty(w: list[float], l1: float, l2: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/elastic-net/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/elastic-net/python/elastic_net.py",
          "language": "python",
          "content": "def elastic_net_penalty(w: list[float], l1: float, l2: float) -> float:\n    return l1 * sum(abs(v) for v in w) + l2 * sum(v * v for v in w)\n"
        },
        {
          "path": "modules/ml/models/elastic-net/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn elastic_net_penalty(w: &[f64], l1: f64, l2: f64) -> f64 {\n    let l1_term: f64 = w.iter().map(|v| v.abs()).sum();\n    let l2_term: f64 = w.iter().map(|v| v * v).sum();\n    l1 * l1_term + l2 * l2_term\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "gaussian-naive-bayes",
      "title": "Gaussian Naive Bayes",
      "path": "modules/ml/models/gaussian-naive-bayes",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Gaussian Naive Bayes\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nGaussian NB assumes features are independent Gaussians per class.\n\n## Math\n\n$$p(x|y)=\\prod_i \\mathcal{N}(x_i;\\mu_i,\\sigma_i^2)$$\n\n## Function\n\n```python\ndef gaussian_log_likelihood(x: list[float], mean: list[float], var: list[float]) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/gaussian-naive-bayes/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/gaussian-naive-bayes/python/gaussian_naive_bayes.py",
          "language": "python",
          "content": "import math\n\n\ndef gaussian_log_likelihood(x: list[float], mean: list[float], var: list[float]) -> float:\n    ll = 0.0\n    for xi, mu, v in zip(x, mean, var):\n        ll += -0.5 * (math.log(2 * math.pi * v) + ((xi - mu) ** 2) / v)\n    return ll\n"
        },
        {
          "path": "modules/ml/models/gaussian-naive-bayes/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn gaussian_log_likelihood(x: &[f64], mean: &[f64], var: &[f64]) -> f64 {\n    let mut ll = 0.0;\n    for ((xi, mu), v) in x.iter().zip(mean.iter()).zip(var.iter()) {\n        ll += -0.5 * ((2.0 * std::f64::consts::PI * v).ln() + (xi - mu).powi(2) / v);\n    }\n    ll\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "gaussian-process-regression",
      "title": "Gaussian Process Regression",
      "path": "modules/ml/models/gaussian-process-regression",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Gaussian Process Regression\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nGaussian Process Regression (GPR) places a GP prior over functions and uses\nkernel-based similarity to produce a predictive distribution.\n\n## Math\n\n$$k(x,x') = \\exp\\left(-\\frac{\\lVert x-x' \\rVert^2}{2\\ell^2}\\right)$$\n\n$$f(x) \\sim \\mathcal{GP}(0, k), \\quad y = f(x) + \\varepsilon,\\ \\varepsilon \\sim \\mathcal{N}(0, \\sigma_n^2)$$\n\n$$K = K(X,X) + \\sigma_n^2 I,\\quad K_* = K(X, X_*),\\quad K_{**} = K(X_*, X_*)$$\n\n$$\\mu_* = K_*^\\top K^{-1} y,\\quad \\Sigma_* = K_{**} - K_*^\\top K^{-1} K_*$$\n\n## Function\n\n```python\ndef gp_posterior_predict(\n    x_train: list[list[float]],\n    y_train: list[float],\n    x_test: list[list[float]],\n    length_scale: float,\n    noise: float,\n) -> tuple[list[float], list[float]]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/gaussian-process-regression/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/gaussian-process-regression/python/gaussian_process_regression.py",
          "language": "python",
          "content": "import math\n\n\ndef rbf_kernel(x: list[float], y: list[float], length_scale: float) -> float:\n    if length_scale <= 0:\n        raise ValueError(\"length_scale must be positive\")\n    dist2 = sum((a - b) ** 2 for a, b in zip(x, y))\n    return math.exp(-dist2 / (2 * length_scale ** 2))\n\n\ndef kernel_matrix(\n    xs: list[list[float]], ys: list[list[float]], length_scale: float\n) -> list[list[float]]:\n    return [[rbf_kernel(x, y, length_scale) for y in ys] for x in xs]\n\n\ndef _matvec(a: list[list[float]], v: list[float]) -> list[float]:\n    return [sum(a[i][k] * v[k] for k in range(len(v))) for i in range(len(a))]\n\n\ndef _invert_matrix(a: list[list[float]]) -> list[list[float]]:\n    n = len(a)\n    if n == 0 or any(len(row) != n for row in a):\n        raise ValueError(\"matrix must be square\")\n\n    # Gauss-Jordan elimination (sufficient for small learning examples)\n    aug = [row[:] + [1.0 if i == j else 0.0 for j in range(n)] for i, row in enumerate(a)]\n    for col in range(n):\n        pivot_row = max(range(col, n), key=lambda r: abs(aug[r][col]))\n        if abs(aug[pivot_row][col]) < 1e-12:\n            raise ValueError(\"matrix is singular\")\n        if pivot_row != col:\n            aug[col], aug[pivot_row] = aug[pivot_row], aug[col]\n\n        pivot = aug[col][col]\n        for j in range(2 * n):\n            aug[col][j] /= pivot\n\n        for r in range(n):\n            if r == col:\n                continue\n            factor = aug[r][col]\n            if factor == 0:\n                continue\n            for j in range(2 * n):\n                aug[r][j] -= factor * aug[col][j]\n\n    return [row[n:] for row in aug]\n\n\ndef gp_posterior_predict(\n    x_train: list[list[float]],\n    y_train: list[float],\n    x_test: list[list[float]],\n    length_scale: float,\n    noise: float,\n) -> tuple[list[float], list[float]]:\n    if len(x_train) != len(y_train):\n        raise ValueError(\"x_train and y_train must have the same length\")\n    if noise < 0:\n        raise ValueError(\"noise must be non-negative\")\n\n    k_xx = kernel_matrix(x_train, x_train, length_scale)\n    for i in range(len(k_xx)):\n        k_xx[i][i] += noise ** 2\n\n    k_inv = _invert_matrix(k_xx)\n    k_xs = kernel_matrix(x_train, x_test, length_scale)\n    k_ss = kernel_matrix(x_test, x_test, length_scale)\n\n    alpha = _matvec(k_inv, y_train)\n    mean = [\n        sum(k_xs[i][j] * alpha[i] for i in range(len(x_train)))\n        for j in range(len(x_test))\n    ]\n\n    variance = []\n    for j in range(len(x_test)):\n        k_col = [k_xs[i][j] for i in range(len(x_train))]\n        temp = _matvec(k_inv, k_col)\n        v = sum(k_col[i] * temp[i] for i in range(len(k_col)))\n        var = k_ss[j][j] - v\n        variance.append(var if var > 0.0 else 0.0)\n\n    return mean, variance\n"
        },
        {
          "path": "modules/ml/models/gaussian-process-regression/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn rbf_kernel(x: &[f64], y: &[f64], length_scale: f64) -> f64 {\n    let dist2: f64 = x.iter().zip(y.iter()).map(|(a, b)| (a - b).powi(2)).sum();\n    (-dist2 / (2.0 * length_scale * length_scale)).exp()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "k-means",
      "title": "K-Means",
      "path": "modules/ml/models/k-means",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# K-Means\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nK-Means assigns points to nearest centroid.\n\n## Math\n\n$$c_i = \\arg\\min_j \\lVert x_i - \\mu_j \\rVert$$\n\n## Function\n\n```python\ndef assign(points: list[list[float]], centroids: list[list[float]]) -> list[int]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/k-means/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/k-means/python/k_means.py",
          "language": "python",
          "content": "def assign(points: list[list[float]], centroids: list[list[float]]) -> list[int]:\n    assignments = []\n    for p in points:\n        dists = [sum((pi - ci) ** 2 for pi, ci in zip(p, c)) for c in centroids]\n        assignments.append(min(range(len(dists)), key=lambda i: dists[i]))\n    return assignments\n"
        },
        {
          "path": "modules/ml/models/k-means/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn assign(points: &[Vec<f64>], centroids: &[Vec<f64>]) -> Vec<usize> {\n    let mut assignments = Vec::with_capacity(points.len());\n    for p in points {\n        let mut best = 0;\n        let mut best_dist = f64::INFINITY;\n        for (i, c) in centroids.iter().enumerate() {\n            let dist: f64 = p.iter().zip(c.iter()).map(|(pi, ci)| (pi - ci).powi(2)).sum();\n            if dist < best_dist {\n                best_dist = dist;\n                best = i;\n            }\n        }\n        assignments.push(best);\n    }\n    assignments\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "knn",
      "title": "K-Nearest Neighbors",
      "path": "modules/ml/models/knn",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# K-Nearest Neighbors\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nKNN predicts by majority vote of nearest points.\n\n## Math\n\n$$\\hat{c} = \\arg\\max_c \\sum_{i \\in \\mathcal{N}_k} \\mathbb{I}[y_i = c]$$\n\n## Function\n\n```python\ndef knn_predict(distances: list[float], labels: list[int], k: int) -> int:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/knn/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/knn/python/knn.py",
          "language": "python",
          "content": "from collections import Counter\n\n\ndef knn_predict(distances: list[float], labels: list[int], k: int) -> int:\n    idxs = sorted(range(len(distances)), key=lambda i: distances[i])[:k]\n    counts = Counter(labels[i] for i in idxs)\n    return counts.most_common(1)[0][0]\n"
        },
        {
          "path": "modules/ml/models/knn/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashMap;\n\npub fn knn_predict(distances: &[f64], labels: &[i32], k: usize) -> i32 {\n    let mut idxs: Vec<usize> = (0..distances.len()).collect();\n    idxs.sort_by(|&a, &b| distances[a].partial_cmp(&distances[b]).unwrap());\n    let mut counts: HashMap<i32, usize> = HashMap::new();\n    for &i in idxs.iter().take(k) {\n        *counts.entry(labels[i]).or_insert(0) += 1;\n    }\n    counts.into_iter().max_by_key(|(_, c)| *c).unwrap().0\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "linear-regression",
      "title": "Linear Regression",
      "path": "modules/ml/models/linear-regression",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Linear Regression\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nLinear regression predicts a target as a linear function of features.\n\n## Math\n\n$$y = w \u00b7 x + b$$\n\n## Function\n\n```python\ndef predict(x: list[float], w: list[float], b: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/linear-regression/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/linear-regression/python/linear_regression.py",
          "language": "python",
          "content": "def predict(x: list[float], w: list[float], b: float) -> float:\n    return sum(wi * xi for wi, xi in zip(w, x)) + b\n"
        },
        {
          "path": "modules/ml/models/linear-regression/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn predict(x: &[f64], w: &[f64], b: f64) -> f64 {\n    let mut out = b;\n    for (wi, xi) in w.iter().zip(x.iter()) {\n        out += wi * xi;\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "logistic-regression",
      "title": "Logistic Regression",
      "path": "modules/ml/models/logistic-regression",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Logistic Regression\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nLogistic regression models class probability with a sigmoid.\n\n## Math\n\n$$p = 1/(1+e^{-(w\u00b7x+b)})$$\n\n## Function\n\n```python\ndef predict_proba(x: list[float], w: list[float], b: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/logistic-regression/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/logistic-regression/python/logistic_regression.py",
          "language": "python",
          "content": "import math\n\n\ndef predict_proba(x: list[float], w: list[float], b: float) -> float:\n    z = sum(wi * xi for wi, xi in zip(w, x)) + b\n    return 1 / (1 + math.exp(-z))\n"
        },
        {
          "path": "modules/ml/models/logistic-regression/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn predict_proba(x: &[f64], w: &[f64], b: f64) -> f64 {\n    let mut z = b;\n    for (wi, xi) in w.iter().zip(x.iter()) {\n        z += wi * xi;\n    }\n    1.0 / (1.0 + (-z).exp())\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "pca",
      "title": "PCA",
      "path": "modules/ml/models/pca",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# PCA\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nPCA finds directions of maximum variance.\n\n## Math\n\n$$v_1 = \\arg\\max_{\\lVert v \\rVert = 1} v^\\top \\Sigma v$$\n\n## Function\n\n```python\ndef pca_first_component_2d(points: list[list[float]]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/pca/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/pca/python/pca.py",
          "language": "python",
          "content": "import math\n\n\ndef pca_first_component_2d(points: list[list[float]]) -> list[float]:\n    n = len(points)\n    mean_x = sum(p[0] for p in points) / n\n    mean_y = sum(p[1] for p in points) / n\n    cov_xx = sum((p[0] - mean_x) ** 2 for p in points) / n\n    cov_yy = sum((p[1] - mean_y) ** 2 for p in points) / n\n    cov_xy = sum((p[0] - mean_x) * (p[1] - mean_y) for p in points) / n\n    # eigenvector for largest eigenvalue of 2x2\n    trace = cov_xx + cov_yy\n    det = cov_xx * cov_yy - cov_xy * cov_xy\n    eig1 = trace / 2 + math.sqrt(max(0.0, (trace / 2) ** 2 - det))\n    if cov_xy == 0:\n        vec = [1.0, 0.0] if cov_xx >= cov_yy else [0.0, 1.0]\n    else:\n        vec = [eig1 - cov_yy, cov_xy]\n    norm = math.sqrt(vec[0] ** 2 + vec[1] ** 2)\n    return [vec[0] / norm, vec[1] / norm]\n"
        },
        {
          "path": "modules/ml/models/pca/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn pca_first_component_2d(points: &[Vec<f64>]) -> Vec<f64> {\n    let n = points.len() as f64;\n    let mean_x: f64 = points.iter().map(|p| p[0]).sum::<f64>() / n;\n    let mean_y: f64 = points.iter().map(|p| p[1]).sum::<f64>() / n;\n    let cov_xx: f64 = points.iter().map(|p| (p[0] - mean_x).powi(2)).sum::<f64>() / n;\n    let cov_yy: f64 = points.iter().map(|p| (p[1] - mean_y).powi(2)).sum::<f64>() / n;\n    let cov_xy: f64 = points.iter().map(|p| (p[0] - mean_x) * (p[1] - mean_y)).sum::<f64>() / n;\n    let trace = cov_xx + cov_yy;\n    let det = cov_xx * cov_yy - cov_xy * cov_xy;\n    let eig1 = trace / 2.0 + ((trace / 2.0).powi(2) - det).max(0.0).sqrt();\n    let (mut vx, mut vy) = if cov_xy == 0.0 {\n        if cov_xx >= cov_yy { (1.0, 0.0) } else { (0.0, 1.0) }\n    } else {\n        (eig1 - cov_yy, cov_xy)\n    };\n    let norm = (vx * vx + vy * vy).sqrt();\n    vx /= norm;\n    vy /= norm;\n    vec![vx, vy]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "random-forest",
      "title": "Random Forest (Bagging)",
      "path": "modules/ml/models/random-forest",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Random Forest (Bagging)\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nRandom forests train many trees on bootstrap samples.\n\n## Math\n\n$$Bootstrap sample size = N with replacement.$$\n\n## Function\n\n```python\ndef bootstrap_indices(n: int, seed: int = 0) -> list[int]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/random-forest/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/random-forest/python/random_forest.py",
          "language": "python",
          "content": "def bootstrap_indices(n: int, seed: int = 0) -> list[int]:\n    idxs = []\n    state = seed\n    for _ in range(n):\n        state = (1103515245 * state + 12345) % (2**31)\n        idxs.append(state % n)\n    return idxs\n"
        },
        {
          "path": "modules/ml/models/random-forest/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn bootstrap_indices(n: usize, seed: u64) -> Vec<usize> {\n    let mut idxs = Vec::with_capacity(n);\n    let mut state = seed;\n    for _ in 0..n {\n        state = (1103515245u64.wrapping_mul(state).wrapping_add(12345)) % (1u64 << 31);\n        idxs.push((state as usize) % n);\n    }\n    idxs\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "softmax-regression",
      "title": "Softmax Regression",
      "path": "modules/ml/models/softmax-regression",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# Softmax Regression\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nSoftmax regression generalizes logistic regression to multiple classes.\n\n## Math\n\n$$p_i = \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)}$$\n\n## Function\n\n```python\ndef softmax(logits: list[float]) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/softmax-regression/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/softmax-regression/python/softmax_regression.py",
          "language": "python",
          "content": "import math\n\n\ndef softmax(logits: list[float]) -> list[float]:\n    m = max(logits)\n    exps = [math.exp(x - m) for x in logits]\n    s = sum(exps)\n    return [e / s for e in exps]\n"
        },
        {
          "path": "modules/ml/models/softmax-regression/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn softmax(logits: &[f64]) -> Vec<f64> {\n    let m = logits.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let exps: Vec<f64> = logits.iter().map(|x| (x - m).exp()).collect();\n    let sum: f64 = exps.iter().sum();\n    exps.iter().map(|e| e / sum).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "svm-pegasos",
      "title": "SVM (Pegasos)",
      "path": "modules/ml/models/svm-pegasos",
      "summary": "> Track: `ml` | Topic: `models`",
      "readme": "# SVM (Pegasos)\n\n> Track: `ml` | Topic: `models`\n\n## Concept\n\nPegasos is a stochastic subgradient method for SVMs.\n\n## Math\n\n$$\nw \\leftarrow\n\\begin{cases}\n(1-\\text{lr}\\,\\lambda)w + \\text{lr}\\, y x, & y(w^\\top x) < 1 \\\\\n(1-\\text{lr}\\,\\lambda)w, & \\text{otherwise}\n\\end{cases}\n$$\n\n## Function\n\n```python\ndef pegasos_step(w: list[float], x: list[float], y: int, lr: float, lam: float) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/models/svm-pegasos/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/models/svm-pegasos/python/svm_pegasos.py",
          "language": "python",
          "content": "def pegasos_step(w: list[float], x: list[float], y: int, lr: float, lam: float) -> list[float]:\n    dot = sum(wi * xi for wi, xi in zip(w, x))\n    scale = 1 - lr * lam\n    w = [wi * scale for wi in w]\n    if y * dot < 1:\n        w = [wi + lr * y * xi for wi, xi in zip(w, x)]\n    return w\n"
        },
        {
          "path": "modules/ml/models/svm-pegasos/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn pegasos_step(w: &[f64], x: &[f64], y: i32, lr: f64, lam: f64) -> Vec<f64> {\n    let dot: f64 = w.iter().zip(x.iter()).map(|(wi, xi)| wi * xi).sum();\n    let scale = 1.0 - lr * lam;\n    let mut out: Vec<f64> = w.iter().map(|wi| wi * scale).collect();\n    if (y as f64) * dot < 1.0 {\n        for i in 0..out.len() {\n            out[i] += lr * (y as f64) * x[i];\n        }\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "adagrad",
      "title": "Adagrad",
      "path": "modules/ml/optimization/adagrad",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Adagrad\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nAdagrad accumulates squared gradients for per-parameter learning rates.\n\n## Math\n\n$$\n\\begin{aligned}\nG_t &= G_{t-1} + g_t^2 \\\\\nw_{t+1} &= w_t - \\text{lr} \\frac{g_t}{\\sqrt{G_t} + \\epsilon}\n\\end{aligned}\n$$\n\n## Function\n\n```python\ndef adagrad_step(w: float, grad: float, g2: float, lr: float, eps: float) -> tuple[float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/adagrad/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/adagrad/python/adagrad.py",
          "language": "python",
          "content": "import math\n\n\ndef adagrad_step(w: float, grad: float, g2: float, lr: float, eps: float) -> tuple[float, float]:\n    g2 = g2 + grad ** 2\n    w = w - lr * grad / (math.sqrt(g2) + eps)\n    return w, g2\n"
        },
        {
          "path": "modules/ml/optimization/adagrad/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn adagrad_step(w: f64, grad: f64, g2: f64, lr: f64, eps: f64) -> (f64, f64) {\n    let g2_new = g2 + grad * grad;\n    let w_new = w - lr * grad / (g2_new.sqrt() + eps);\n    (w_new, g2_new)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "adam",
      "title": "Adam",
      "path": "modules/ml/optimization/adam",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Adam\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nAdam combines momentum and adaptive learning rates.\n\n## Math\n\n$$\n\\begin{aligned}\nm_t &= \\beta_1 m_{t-1} + (1-\\beta_1) g_t \\\\\nv_t &= \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 \\\\\nw_{t+1} &= w_t - \\text{lr} \\frac{m_t}{\\sqrt{v_t} + \\epsilon}\n\\end{aligned}\n$$\n\n## Function\n\n```python\ndef adam_step(w: float, grad: float, m: float, v: float, lr: float, beta1: float, beta2: float, eps: float) -> tuple[float, float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/adam/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/adam/python/adam.py",
          "language": "python",
          "content": "import math\n\n\ndef adam_step(w: float, grad: float, m: float, v: float, lr: float, beta1: float, beta2: float, eps: float) -> tuple[float, float, float]:\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    w = w - lr * m / (math.sqrt(v) + eps)\n    return w, m, v\n"
        },
        {
          "path": "modules/ml/optimization/adam/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn adam_step(w: f64, grad: f64, m: f64, v: f64, lr: f64, beta1: f64, beta2: f64, eps: f64) -> (f64, f64, f64) {\n    let m_new = beta1 * m + (1.0 - beta1) * grad;\n    let v_new = beta2 * v + (1.0 - beta2) * grad * grad;\n    let w_new = w - lr * m_new / (v_new.sqrt() + eps);\n    (w_new, m_new, v_new)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "adamw",
      "title": "AdamW",
      "path": "modules/ml/optimization/adamw",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# AdamW\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nAdamW decouples weight decay from adaptive updates.\n\n## Math\n\n$$w = w - lr * (m/\u221av) - lr*wd*w$$\n\n## Function\n\n```python\ndef adamw_step(w: float, grad: float, m: float, v: float, lr: float, wd: float, beta1: float, beta2: float, eps: float) -> tuple[float, float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/adamw/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/adamw/python/adamw.py",
          "language": "python",
          "content": "import math\n\n\ndef adamw_step(w: float, grad: float, m: float, v: float, lr: float, wd: float, beta1: float, beta2: float, eps: float) -> tuple[float, float, float]:\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    w = w - lr * m / (math.sqrt(v) + eps) - lr * wd * w\n    return w, m, v\n"
        },
        {
          "path": "modules/ml/optimization/adamw/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn adamw_step(w: f64, grad: f64, m: f64, v: f64, lr: f64, wd: f64, beta1: f64, beta2: f64, eps: f64) -> (f64, f64, f64) {\n    let m_new = beta1 * m + (1.0 - beta1) * grad;\n    let v_new = beta2 * v + (1.0 - beta2) * grad * grad;\n    let w_new = w - lr * m_new / (v_new.sqrt() + eps) - lr * wd * w;\n    (w_new, m_new, v_new)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "detect-nans",
      "title": "Detecting NaNs",
      "path": "modules/ml/optimization/detect-nans",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Detecting NaNs\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nCheck for NaNs to catch divergence early.\n\n## Math\n\n$$is_nan(x) = true if x != x$$\n\n## Function\n\n```python\ndef has_nan(values: list[float]) -> bool:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/detect-nans/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/detect-nans/python/detect_nans.py",
          "language": "python",
          "content": "import math\n\n\ndef has_nan(values: list[float]) -> bool:\n    return any(math.isnan(v) for v in values)\n"
        },
        {
          "path": "modules/ml/optimization/detect-nans/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn has_nan(values: &[f64]) -> bool {\n    values.iter().any(|v| v.is_nan())\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "gradient-clipping",
      "title": "Gradient Clipping",
      "path": "modules/ml/optimization/gradient-clipping",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Gradient Clipping\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nClip gradient norm to avoid exploding gradients.\n\n## Math\n\n$$g \\leftarrow g \\cdot \\min\\left(1, \\frac{\\text{clip}}{\\lVert g \\rVert}\\right)$$\n\n## Function\n\n```python\ndef clip_norm(grad: list[float], clip: float) -> list[float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/gradient-clipping/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/gradient-clipping/python/gradient_clipping.py",
          "language": "python",
          "content": "import math\n\n\ndef clip_norm(grad: list[float], clip: float) -> list[float]:\n    norm = math.sqrt(sum(g * g for g in grad))\n    if norm == 0 or norm <= clip:\n        return grad\n    scale = clip / norm\n    return [g * scale for g in grad]\n"
        },
        {
          "path": "modules/ml/optimization/gradient-clipping/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn clip_norm(grad: &[f64], clip: f64) -> Vec<f64> {\n    let norm = grad.iter().map(|g| g * g).sum::<f64>().sqrt();\n    if norm == 0.0 || norm <= clip {\n        return grad.to_vec();\n    }\n    let scale = clip / norm;\n    grad.iter().map(|g| g * scale).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "loss-scaling",
      "title": "Loss Scaling",
      "path": "modules/ml/optimization/loss-scaling",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Loss Scaling\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nLoss scaling multiplies loss to avoid underflow in mixed precision.\n\n## Math\n\n$$scaled_grad = grad * scale$$\n\n## Function\n\n```python\ndef scale_grad(grad: float, scale: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/loss-scaling/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/loss-scaling/python/loss_scaling.py",
          "language": "python",
          "content": "def scale_grad(grad: float, scale: float) -> float:\n    return grad * scale\n"
        },
        {
          "path": "modules/ml/optimization/loss-scaling/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn scale_grad(grad: f64, scale: f64) -> f64 {\n    grad * scale\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "lr-constant",
      "title": "Constant Learning Rate",
      "path": "modules/ml/optimization/lr-constant",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Constant Learning Rate\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nConstant LR keeps the step size fixed.\n\n## Math\n\n$$\\text{lr}_t = \\text{lr}$$\n\n## Function\n\n```python\ndef constant_lr(lr: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/lr-constant/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/lr-constant/python/lr_constant.py",
          "language": "python",
          "content": "def constant_lr(lr: float) -> float:\n    return lr\n"
        },
        {
          "path": "modules/ml/optimization/lr-constant/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn constant_lr(lr: f64) -> f64 { lr }\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "lr-cosine-decay",
      "title": "Cosine Decay",
      "path": "modules/ml/optimization/lr-cosine-decay",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Cosine Decay\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nCosine decay anneals LR smoothly to zero.\n\n## Math\n\n$$\\text{lr}_t = \\text{lr} \\cdot \\frac{1}{2}\\left(1+\\cos\\left(\\frac{\\pi t}{T}\\right)\\right)$$\n\n## Function\n\n```python\ndef cosine_decay(lr: float, t: int, t_max: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/lr-cosine-decay/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/lr-cosine-decay/python/lr_cosine_decay.py",
          "language": "python",
          "content": "import math\n\n\ndef cosine_decay(lr: float, t: int, t_max: int) -> float:\n    return lr * 0.5 * (1 + math.cos(math.pi * t / t_max))\n"
        },
        {
          "path": "modules/ml/optimization/lr-cosine-decay/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn cosine_decay(lr: f64, t: i32, t_max: i32) -> f64 {\n    lr * 0.5 * (1.0 + (std::f64::consts::PI * t as f64 / t_max as f64).cos())\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "lr-exponential-decay",
      "title": "Exponential Decay",
      "path": "modules/ml/optimization/lr-exponential-decay",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Exponential Decay\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nExponential decay reduces LR continuously.\n\n## Math\n\n$$\\text{lr}_t = \\text{lr} \\cdot \\exp(-k t)$$\n\n## Function\n\n```python\ndef exp_decay(lr: float, k: float, t: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/lr-exponential-decay/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/lr-exponential-decay/python/lr_exponential_decay.py",
          "language": "python",
          "content": "import math\n\n\ndef exp_decay(lr: float, k: float, t: float) -> float:\n    return lr * math.exp(-k * t)\n"
        },
        {
          "path": "modules/ml/optimization/lr-exponential-decay/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn exp_decay(lr: f64, k: f64, t: f64) -> f64 {\n    lr * (-k * t).exp()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "lr-step-decay",
      "title": "Step Decay",
      "path": "modules/ml/optimization/lr-step-decay",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Step Decay\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nStep decay drops LR by a factor every k steps.\n\n## Math\n\n$$\\text{lr}_t = \\text{lr} \\cdot \\gamma^{\\lfloor t / \\text{step} \\rfloor}$$\n\n## Function\n\n```python\ndef step_decay(lr: float, step: int, gamma: float, t: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/lr-step-decay/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/lr-step-decay/python/lr_step_decay.py",
          "language": "python",
          "content": "import math\n\n\ndef step_decay(lr: float, step: int, gamma: float, t: int) -> float:\n    return lr * (gamma ** (t // step))\n"
        },
        {
          "path": "modules/ml/optimization/lr-step-decay/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn step_decay(lr: f64, step: i32, gamma: f64, t: i32) -> f64 {\n    let k = t / step;\n    lr * gamma.powi(k)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "lr-warmup",
      "title": "Warmup",
      "path": "modules/ml/optimization/lr-warmup",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Warmup\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nWarmup linearly ramps LR at the start of training.\n\n## Math\n\n$$\\text{lr}_t = \\text{lr} \\cdot \\min\\left(1, \\frac{t}{T_{\\text{warmup}}}\\right)$$\n\n## Function\n\n```python\ndef warmup_lr(lr: float, t: int, warmup_steps: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/lr-warmup/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/lr-warmup/python/lr_warmup.py",
          "language": "python",
          "content": "def warmup_lr(lr: float, t: int, warmup_steps: int) -> float:\n    return lr * min(1.0, t / warmup_steps)\n"
        },
        {
          "path": "modules/ml/optimization/lr-warmup/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn warmup_lr(lr: f64, t: i32, warmup_steps: i32) -> f64 {\n    let factor = (t as f64 / warmup_steps as f64).min(1.0);\n    lr * factor\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "muon-optimizer",
      "title": "Muon Optimizer",
      "path": "modules/ml/optimization/muon-optimizer",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Muon Optimizer\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nMuon is a matrix-aware optimizer: it applies momentum, then orthogonalizes the\n2D update so its directions stay well-conditioned. Practical implementations\northogonalize each matrix update (often via Newton-Schulz iterations). This\nmodule uses a tiny Gram-Schmidt approximation for a learnable demo.\n\n## Math\n\n- $m_t = beta * m_{t-1} + g_t$\n- $Q_t = orthogonalize(m_t)$\n- $W_{t+1} = W_t - lr * Q_t$\n\n## Function\n\n```python\ndef muon_step(\n    weights: list[list[float]],\n    grad: list[list[float]],\n    velocity: list[list[float]] | None,\n    lr: float = 0.1,\n    momentum: float = 0.9,\n) -> tuple[list[list[float]], list[list[float]]]:\n```\n\n## Demo code\n\n```python\nweights = [[1.0, -1.0], [0.5, 0.25]]\ngrad = [[0.2, -0.1], [0.05, 0.4]]\nnew_weights, velocity = muon_step(weights, grad, None, lr=0.1, momentum=0.9)\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/muon-optimizer/python -q\ncargo test --manifest-path modules/ml/optimization/muon-optimizer/rust/Cargo.toml\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/muon-optimizer/python/muon_optimizer.py",
          "language": "python",
          "content": "\"\"\"Muon optimizer demo using Gram-Schmidt orthogonalization.\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom typing import List, Tuple\n\n\nMatrix = List[List[float]]\n\n\ndef _dot(a: List[float], b: List[float]) -> float:\n    return sum(x * y for x, y in zip(a, b))\n\n\ndef _norm(v: List[float]) -> float:\n    return math.sqrt(_dot(v, v))\n\n\ndef gram_schmidt_rows(mat: Matrix, eps: float = 1e-12) -> Matrix:\n    \"\"\"Return an orthonormalized copy of rows via Gram-Schmidt.\"\"\"\n    out: Matrix = []\n    for row in mat:\n        v = row[:]\n        for u in out:\n            proj = _dot(v, u)\n            v = [vi - proj * ui for vi, ui in zip(v, u)]\n        n = _norm(v)\n        if n < eps:\n            out.append([0.0 for _ in v])\n        else:\n            out.append([vi / n for vi in v])\n    return out\n\n\ndef muon_step(\n    weights: Matrix,\n    grad: Matrix,\n    velocity: Matrix | None,\n    lr: float = 0.1,\n    momentum: float = 0.9,\n) -> Tuple[Matrix, Matrix]:\n    \"\"\"Apply a Muon-style update and return (new_weights, new_velocity).\"\"\"\n    if velocity is None:\n        velocity = [[0.0 for _ in row] for row in grad]\n    new_velocity = [\n        [momentum * v + g for v, g in zip(v_row, g_row)]\n        for v_row, g_row in zip(velocity, grad)\n    ]\n    ortho = gram_schmidt_rows(new_velocity)\n    new_weights = [\n        [w - lr * u for w, u in zip(w_row, u_row)]\n        for w_row, u_row in zip(weights, ortho)\n    ]\n    return new_weights, new_velocity\n"
        },
        {
          "path": "modules/ml/optimization/muon-optimizer/rust/src/lib.rs",
          "language": "rust",
          "content": "pub type Matrix = Vec<Vec<f64>>;\n\nfn dot(a: &[f64], b: &[f64]) -> f64 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\nfn norm(v: &[f64]) -> f64 {\n    dot(v, v).sqrt()\n}\n\npub fn gram_schmidt_rows(mat: &Matrix, eps: f64) -> Matrix {\n    let mut out: Matrix = Vec::new();\n    for row in mat.iter() {\n        let mut v = row.clone();\n        for u in out.iter() {\n            let proj = dot(&v, u);\n            for (vi, ui) in v.iter_mut().zip(u.iter()) {\n                *vi -= proj * ui;\n            }\n        }\n        let n = norm(&v);\n        if n < eps {\n            out.push(vec![0.0; v.len()]);\n        } else {\n            out.push(v.iter().map(|vi| vi / n).collect());\n        }\n    }\n    out\n}\n\npub fn muon_step(\n    weights: &Matrix,\n    grad: &Matrix,\n    velocity: Option<&Matrix>,\n    lr: f64,\n    momentum: f64,\n) -> (Matrix, Matrix) {\n    let mut vel = match velocity {\n        Some(v) => v.clone(),\n        None => grad.iter().map(|row| vec![0.0; row.len()]).collect(),\n    };\n    for (v_row, g_row) in vel.iter_mut().zip(grad.iter()) {\n        for (v, g) in v_row.iter_mut().zip(g_row.iter()) {\n            *v = momentum * *v + *g;\n        }\n    }\n    let ortho = gram_schmidt_rows(&vel, 1e-12);\n    let mut new_weights = weights.clone();\n    for (w_row, u_row) in new_weights.iter_mut().zip(ortho.iter()) {\n        for (w, u) in w_row.iter_mut().zip(u_row.iter()) {\n            *w -= lr * u;\n        }\n    }\n    (new_weights, vel)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "nesterov",
      "title": "Nesterov Accelerated Gradient",
      "path": "modules/ml/optimization/nesterov",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# Nesterov Accelerated Gradient\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nNesterov uses a lookahead gradient for faster convergence.\n\n## Math\n\n$$\n\\begin{aligned}\nv_t &= \\mu v_{t-1} + g\\left(w_{t-1} - \\text{lr}\\,\\mu v_{t-1}\\right) \\\\\nw_{t+1} &= w_t - \\text{lr}\\, v_t\n\\end{aligned}\n$$\n\n## Function\n\n```python\ndef nesterov_step(w: float, grad: float, v: float, lr: float, mu: float) -> tuple[float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/nesterov/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/nesterov/python/nesterov.py",
          "language": "python",
          "content": "def nesterov_step(w: float, grad: float, v: float, lr: float, mu: float) -> tuple[float, float]:\n    v_new = mu * v + grad\n    w_new = w - lr * (mu * v_new + grad)\n    return w_new, v_new\n"
        },
        {
          "path": "modules/ml/optimization/nesterov/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn nesterov_step(w: f64, grad: f64, v: f64, lr: f64, mu: f64) -> (f64, f64) {\n    let v_new = mu * v + grad;\n    let w_new = w - lr * (mu * v_new + grad);\n    (w_new, v_new)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "rmsprop",
      "title": "RMSProp",
      "path": "modules/ml/optimization/rmsprop",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# RMSProp\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nRMSProp scales learning rates by running average of squared gradients.\n\n## Math\n\n$$\n\\begin{aligned}\nv_t &= \\beta v_{t-1} + (1-\\beta) g_t^2 \\\\\nw_{t+1} &= w_t - \\text{lr} \\frac{g_t}{\\sqrt{v_t} + \\epsilon}\n\\end{aligned}\n$$\n\n## Function\n\n```python\ndef rmsprop_step(w: float, grad: float, v: float, lr: float, beta: float, eps: float) -> tuple[float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/rmsprop/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/rmsprop/python/rmsprop.py",
          "language": "python",
          "content": "import math\n\n\ndef rmsprop_step(w: float, grad: float, v: float, lr: float, beta: float, eps: float) -> tuple[float, float]:\n    v = beta * v + (1 - beta) * (grad ** 2)\n    w = w - lr * grad / (math.sqrt(v) + eps)\n    return w, v\n"
        },
        {
          "path": "modules/ml/optimization/rmsprop/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn rmsprop_step(w: f64, grad: f64, v: f64, lr: f64, beta: f64, eps: f64) -> (f64, f64) {\n    let v_new = beta * v + (1.0 - beta) * grad * grad;\n    let w_new = w - lr * grad / (v_new.sqrt() + eps);\n    (w_new, v_new)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "sgd-momentum",
      "title": "SGD with Momentum",
      "path": "modules/ml/optimization/sgd-momentum",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# SGD with Momentum\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nMomentum accumulates velocity to smooth updates.\n\n## Math\n\n$$\n\\begin{aligned}\nv_t &= \\mu v_{t-1} + g_t \\\\\nw_{t+1} &= w_t - \\text{lr}\\, v_t\n\\end{aligned}\n$$\n\n## Function\n\n```python\ndef momentum_step(w: float, grad: float, v: float, lr: float, mu: float) -> tuple[float, float]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/sgd-momentum/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/sgd-momentum/python/sgd_momentum.py",
          "language": "python",
          "content": "def momentum_step(w: float, grad: float, v: float, lr: float, mu: float) -> tuple[float, float]:\n    v = mu * v + grad\n    w = w - lr * v\n    return w, v\n"
        },
        {
          "path": "modules/ml/optimization/sgd-momentum/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn momentum_step(w: f64, grad: f64, v: f64, lr: f64, mu: f64) -> (f64, f64) {\n    let v_new = mu * v + grad;\n    let w_new = w - lr * v_new;\n    (w_new, v_new)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "sgd",
      "title": "SGD",
      "path": "modules/ml/optimization/sgd",
      "summary": "> Track: `ml` | Topic: `optimization`",
      "readme": "# SGD\n\n> Track: `ml` | Topic: `optimization`\n\n## Concept\n\nSGD updates parameters by stepping opposite the gradient.\n\n## Math\n\n$$w = w - lr * grad$$\n\n## Function\n\n```python\ndef sgd_step(w: float, grad: float, lr: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/optimization/sgd/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/optimization/sgd/python/sgd.py",
          "language": "python",
          "content": "def sgd_step(w: float, grad: float, lr: float) -> float:\n    return w - lr * grad\n"
        },
        {
          "path": "modules/ml/optimization/sgd/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn sgd_step(w: f64, grad: f64, lr: f64) -> f64 {\n    w - lr * grad\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "bandit-epsilon-greedy",
      "title": "Epsilon-Greedy Bandit",
      "path": "modules/ml/reinforcement-learning/bandit-epsilon-greedy",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# Epsilon-Greedy Bandit\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nThe **multi-armed bandit** problem is a simplified RL setting: an agent\nrepeatedly chooses one of _K_ actions (arms) and receives a stochastic reward.\nThe goal is to maximize cumulative reward over time.\n\nThe **epsilon-greedy** strategy balances exploration and exploitation:\n\n- With probability $\\epsilon$, pick a random arm (explore)\n- With probability $1-\\epsilon$, pick the arm with the highest estimated value (exploit)\n\nThis is the simplest baseline for the explore-exploit tradeoff.\n$\\epsilon = 0$ is pure greedy; $\\epsilon = 1$ is pure random.\n\n## Math\n\nAction-value estimate updated incrementally after pulling arm $a$ and observing reward $r$:\n\n$$N_a \\leftarrow N_a + 1$$\n\n$$Q_a \\leftarrow Q_a + \\frac{1}{N_a}(r - Q_a)$$\n\n- $Q_a$ \u2014 estimated value of arm $a$ (running mean of rewards)\n- $N_a$ \u2014 number of times arm $a$ has been pulled\n- The update computes the sample mean incrementally\n\nArm selection:\n\n$$a_t = \\begin{cases} \\text{random arm from } \\{0, \\dots, K{-}1\\} & \\text{with probability } \\epsilon \\\\ \\arg\\max_a Q_a & \\text{with probability } 1 - \\epsilon \\end{cases}$$\n\n## Function\n\n```python\nclass EpsilonGreedyBandit:\n    def __init__(self, k: int, epsilon: float = 0.1, seed: int | None = None)\n    def select_arm(self) -> int\n    def update(self, arm: int, reward: float) -> None\n```\n\n- `k` \u2014 number of arms\n- `epsilon` \u2014 exploration probability (0 to 1)\n- `select_arm()` \u2014 returns the index of the chosen arm\n- `update(arm, reward)` \u2014 updates the value estimate for the given arm\n- `q_values` \u2014 list of current estimated values per arm\n- `counts` \u2014 list of pull counts per arm\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/bandit-epsilon-greedy/python -q\ncargo test --manifest-path modules/ml/reinforcement-learning/bandit-epsilon-greedy/rust/Cargo.toml\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/bandit-epsilon-greedy/python/bandit.py",
          "language": "python",
          "content": "\"\"\"Epsilon-greedy multi-armed bandit.\"\"\"\n\nimport random\nfrom typing import List\n\n\nclass EpsilonGreedyBandit:\n    \"\"\"Epsilon-greedy agent for the multi-armed bandit problem.\"\"\"\n\n    def __init__(self, k: int, epsilon: float = 0.1, seed: int | None = None):\n        \"\"\"\n        Args:\n            k: number of arms\n            epsilon: exploration probability (0 to 1)\n            seed: optional random seed for reproducibility\n        \"\"\"\n        self.k = k\n        self.epsilon = epsilon\n        self.q_values: List[float] = [0.0] * k  # estimated action values\n        self.counts: List[int] = [0] * k  # pull counts per arm\n        self._rng = random.Random(seed)\n\n    def select_arm(self) -> int:\n        \"\"\"Choose an arm using epsilon-greedy strategy.\"\"\"\n        if self._rng.random() < self.epsilon:\n            return self._rng.randint(0, self.k - 1)\n        # Greedy: pick arm with highest Q (break ties randomly)\n        max_q = max(self.q_values)\n        best_arms = [a for a in range(self.k) if self.q_values[a] == max_q]\n        return self._rng.choice(best_arms)\n\n    def update(self, arm: int, reward: float) -> None:\n        \"\"\"Update the value estimate for the chosen arm.\"\"\"\n        self.counts[arm] += 1\n        # Incremental mean update\n        self.q_values[arm] += (reward - self.q_values[arm]) / self.counts[arm]\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/bandit-epsilon-greedy/rust/src/lib.rs",
          "language": "rust",
          "content": "struct Lcg {\n    state: u64,\n}\n\nimpl Lcg {\n    fn new(seed: u64) -> Self {\n        Self { state: seed }\n    }\n\n    fn next_u32(&mut self) -> u32 {\n        self.state = self\n            .state\n            .wrapping_mul(6364136223846793005)\n            .wrapping_add(1);\n        (self.state >> 32) as u32\n    }\n\n    fn next_f64(&mut self) -> f64 {\n        let v = self.next_u32() as f64;\n        v / u32::MAX as f64\n    }\n\n    fn gen_range(&mut self, upper: usize) -> usize {\n        (self.next_u32() as usize) % upper\n    }\n}\n\npub struct EpsilonGreedyBandit {\n    k: usize,\n    epsilon: f64,\n    q_values: Vec<f64>,\n    counts: Vec<usize>,\n    rng: Lcg,\n}\n\nimpl EpsilonGreedyBandit {\n    pub fn new(k: usize, epsilon: f64, seed: u64) -> Self {\n        Self {\n            k,\n            epsilon,\n            q_values: vec![0.0; k],\n            counts: vec![0; k],\n            rng: Lcg::new(seed),\n        }\n    }\n\n    pub fn select_arm(&mut self) -> usize {\n        if self.rng.next_f64() < self.epsilon {\n            return self.rng.gen_range(self.k);\n        }\n        let max_q = self\n            .q_values\n            .iter()\n            .cloned()\n            .fold(f64::NEG_INFINITY, f64::max);\n        let mut best: Vec<usize> = self\n            .q_values\n            .iter()\n            .enumerate()\n            .filter(|(_, q)| **q == max_q)\n            .map(|(i, _)| i)\n            .collect();\n        if best.len() == 1 {\n            return best[0];\n        }\n        let idx = self.rng.gen_range(best.len());\n        best.swap_remove(idx)\n    }\n\n    pub fn update(&mut self, arm: usize, reward: f64) {\n        self.counts[arm] += 1;\n        let n = self.counts[arm] as f64;\n        self.q_values[arm] += (reward - self.q_values[arm]) / n;\n    }\n\n    pub fn q_values(&self) -> &[f64] {\n        &self.q_values\n    }\n\n    pub fn counts(&self) -> &[usize] {\n        &self.counts\n    }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "bandits",
      "title": "Bandits",
      "path": "modules/ml/reinforcement-learning/bandits",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# Bandits\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nBandits estimate action values from rewards.\n\n## Math\n\n$$Q <- Q + (1/N)(r - Q)$$\n\n## Function\n\n```python\ndef update_value(q: float, n: int, reward: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/bandits/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/bandits/python/bandits.py",
          "language": "python",
          "content": "def update_value(q: float, n: int, reward: float) -> float:\n    return q + (reward - q) / n\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/bandits/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn update_value(q: f64, n: i32, reward: f64) -> f64 {\n    q + (reward - q) / n as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "dpo-vs-ppo",
      "title": "DPO vs PPO",
      "path": "modules/ml/reinforcement-learning/dpo-vs-ppo",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# DPO vs PPO\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nCompare preference optimization to policy optimization.\n\n## Math\n\n$$\\text{DPO uses preference logits; PPO uses clipped policy ratios.}$$\n\n## Function\n\n```python\ndef compare_methods() -> list[str]:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/dpo-vs-ppo/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/dpo-vs-ppo/python/dpo_vs_ppo.py",
          "language": "python",
          "content": "def compare_methods() -> list[str]:\n    return [\"dpo\", \"ppo\"]\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/dpo-vs-ppo/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn compare_methods() -> Vec<&'static str> {\n    vec![\"dpo\", \"ppo\"]\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "exploration-exploitation",
      "title": "Exploration vs Exploitation",
      "path": "modules/ml/reinforcement-learning/exploration-exploitation",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# Exploration vs Exploitation\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nEpsilon controls how often to explore.\n\n## Math\n\n$$P(\\text{explore})=\\epsilon$$\n\n## Function\n\n```python\ndef explore_probability(epsilon: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/exploration-exploitation/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/exploration-exploitation/python/exploration_exploitation.py",
          "language": "python",
          "content": "def explore_probability(epsilon: float) -> float:\n    return max(0.0, min(1.0, epsilon))\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/exploration-exploitation/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn explore_probability(epsilon: f64) -> f64 {\n    if epsilon < 0.0 { 0.0 } else if epsilon > 1.0 { 1.0 } else { epsilon }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "group-based-optimization",
      "title": "Group-Based Optimization (GSPO / GRPO)",
      "path": "modules/ml/reinforcement-learning/group-based-optimization",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# Group-Based Optimization (GSPO / GRPO)\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nGroup-based policy optimization builds advantages by normalizing rewards within\na group of sampled responses. GRPO uses token-level importance ratios, while\nGSPO switches to a sequence-level ratio to reduce token-level instability.\n\n## Math\n\n- $A_i = \\frac{r_i - \\mu_r}{\\sigma_r + \\epsilon}$\n- $\\text{GRPO: } \\rho_{i,t} = \\exp(\\log \\pi_\\theta - \\log \\pi_{\\text{old}}),\\ \\text{ratio}_i = \\frac{1}{T}\\sum_t \\rho_{i,t}$\n- $\\text{GSPO: } \\text{ratio}_i = \\exp\\left(\\sum_t (\\log \\pi_\\theta - \\log \\pi_{\\text{old}})\\right)$\n- $\\text{Objective: } J = \\frac{1}{N}\\sum_i \\text{ratio}_i A_i$\n\n## Function\n\n```python\ndef group_advantages(rewards: list[float], eps: float = 1e-8) -> list[float]:\n\ndef grpo_objective(\n    old_logps: list[list[float]],\n    new_logps: list[list[float]],\n    rewards: list[float],\n) -> float:\n\ndef gspo_objective(\n    old_logps: list[list[float]],\n    new_logps: list[list[float]],\n    rewards: list[float],\n) -> float:\n```\n\n## Demo code\n\n```python\nold_logps = [[-0.1, -0.2], [-0.3, -0.1]]\nnew_logps = [[-0.15, -0.25], [-0.25, -0.05]]\nrewards = [0.2, 0.8]\nprint(grpo_objective(old_logps, new_logps, rewards))\nprint(gspo_objective(old_logps, new_logps, rewards))\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/group-based-optimization/python -q\ncargo test --manifest-path modules/ml/reinforcement-learning/group-based-optimization/rust/Cargo.toml\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/group-based-optimization/python/group_based_optimization.py",
          "language": "python",
          "content": "\"\"\"Group-based optimization demo for GRPO/GSPO.\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom typing import List\n\n\ndef group_advantages(rewards: List[float], eps: float = 1e-8) -> List[float]:\n    if not rewards:\n        return []\n    mean = sum(rewards) / len(rewards)\n    var = sum((r - mean) ** 2 for r in rewards) / len(rewards)\n    std = math.sqrt(var)\n    return [(r - mean) / (std + eps) for r in rewards]\n\n\ndef _sequence_ratio(old_logps: List[float], new_logps: List[float]) -> float:\n    return math.exp(sum(n - o for o, n in zip(old_logps, new_logps)))\n\n\ndef _token_ratio_mean(old_logps: List[float], new_logps: List[float]) -> float:\n    ratios = [math.exp(n - o) for o, n in zip(old_logps, new_logps)]\n    return sum(ratios) / len(ratios)\n\n\ndef grpo_objective(\n    old_logps: List[List[float]],\n    new_logps: List[List[float]],\n    rewards: List[float],\n) -> float:\n    advantages = group_advantages(rewards)\n    if not advantages:\n        return 0.0\n    total = 0.0\n    for old, new, adv in zip(old_logps, new_logps, advantages):\n        total += _token_ratio_mean(old, new) * adv\n    return total / len(advantages)\n\n\ndef gspo_objective(\n    old_logps: List[List[float]],\n    new_logps: List[List[float]],\n    rewards: List[float],\n) -> float:\n    advantages = group_advantages(rewards)\n    if not advantages:\n        return 0.0\n    total = 0.0\n    for old, new, adv in zip(old_logps, new_logps, advantages):\n        total += _sequence_ratio(old, new) * adv\n    return total / len(advantages)\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/group-based-optimization/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn group_advantages(rewards: &[f64]) -> Vec<f64> {\n    if rewards.is_empty() {\n        return Vec::new();\n    }\n    let mean = rewards.iter().sum::<f64>() / rewards.len() as f64;\n    let var = rewards\n        .iter()\n        .map(|r| (r - mean).powi(2))\n        .sum::<f64>()\n        / rewards.len() as f64;\n    let std = var.sqrt();\n    rewards\n        .iter()\n        .map(|r| (r - mean) / (std + 1e-8))\n        .collect()\n}\n\nfn sequence_ratio(old_logps: &[f64], new_logps: &[f64]) -> f64 {\n    let sum_delta: f64 = old_logps\n        .iter()\n        .zip(new_logps.iter())\n        .map(|(o, n)| n - o)\n        .sum();\n    sum_delta.exp()\n}\n\nfn token_ratio_mean(old_logps: &[f64], new_logps: &[f64]) -> f64 {\n    let ratios: f64 = old_logps\n        .iter()\n        .zip(new_logps.iter())\n        .map(|(o, n)| (n - o).exp())\n        .sum();\n    ratios / old_logps.len() as f64\n}\n\npub fn grpo_objective(old_logps: &[Vec<f64>], new_logps: &[Vec<f64>], rewards: &[f64]) -> f64 {\n    let advantages = group_advantages(rewards);\n    if advantages.is_empty() {\n        return 0.0;\n    }\n    let mut total = 0.0;\n    for ((old, new), adv) in old_logps.iter().zip(new_logps.iter()).zip(advantages.iter()) {\n        total += token_ratio_mean(old, new) * adv;\n    }\n    total / advantages.len() as f64\n}\n\npub fn gspo_objective(old_logps: &[Vec<f64>], new_logps: &[Vec<f64>], rewards: &[f64]) -> f64 {\n    let advantages = group_advantages(rewards);\n    if advantages.is_empty() {\n        return 0.0;\n    }\n    let mut total = 0.0;\n    for ((old, new), adv) in old_logps.iter().zip(new_logps.iter()).zip(advantages.iter()) {\n        total += sequence_ratio(old, new) * adv;\n    }\n    total / advantages.len() as f64\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "mdp",
      "title": "Markov Decision Process",
      "path": "modules/ml/reinforcement-learning/mdp",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# Markov Decision Process\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nAn MDP defines states, actions, transitions, and rewards.\n\n## Math\n\n$$\\text{P(s'|s,a) defines transition probabilities.}$$\n\n## Function\n\n```python\ndef transition_prob(transitions: dict[tuple[int, int], dict[int, float]], s: int, a: int, s_next: int) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/mdp/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/mdp/python/mdp.py",
          "language": "python",
          "content": "def transition_prob(transitions: dict[tuple[int, int], dict[int, float]], s: int, a: int, s_next: int) -> float:\n    return transitions.get((s, a), {}).get(s_next, 0.0)\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/mdp/rust/src/lib.rs",
          "language": "rust",
          "content": "use std::collections::HashMap;\n\npub fn transition_prob(\n    transitions: &HashMap<(i32, i32), HashMap<i32, f64>>,\n    s: i32,\n    a: i32,\n    s_next: i32,\n) -> f64 {\n    transitions\n        .get(&(s, a))\n        .and_then(|m| m.get(&s_next).copied())\n        .unwrap_or(0.0)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "ppo",
      "title": "PPO",
      "path": "modules/ml/reinforcement-learning/ppo",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# PPO\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nPPO clips policy updates to avoid large deviations.\n\n## Math\n\n$$L = \\min\\left(r_t A_t, \\operatorname{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A_t\\right)$$\n\n## Function\n\n```python\ndef clip_ratio(ratio: float, eps: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/ppo/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/ppo/python/ppo.py",
          "language": "python",
          "content": "def clip_ratio(ratio: float, eps: float) -> float:\n    return max(1 - eps, min(1 + eps, ratio))\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/ppo/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn clip_ratio(ratio: f64, eps: f64) -> f64 {\n    let lower = 1.0 - eps;\n    let upper = 1.0 + eps;\n    ratio.max(lower).min(upper)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "q-learning",
      "title": "Q-Learning",
      "path": "modules/ml/reinforcement-learning/q-learning",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# Q-Learning\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nQ-learning updates state-action values toward TD targets.\n\n## Math\n\n$$Q \\leftarrow Q + \\alpha\\left(r + \\gamma \\max_{a'} Q' - Q\\right)$$\n\n## Function\n\n```python\ndef q_update(q: float, reward: float, next_max: float, alpha: float, gamma: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/q-learning/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/q-learning/python/q_learning.py",
          "language": "python",
          "content": "def q_update(q: float, reward: float, next_max: float, alpha: float, gamma: float) -> float:\n    return q + alpha * (reward + gamma * next_max - q)\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/q-learning/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn q_update(q: f64, reward: f64, next_max: f64, alpha: f64, gamma: f64) -> f64 {\n    q + alpha * (reward + gamma * next_max - q)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "reinforce",
      "title": "REINFORCE",
      "path": "modules/ml/reinforcement-learning/reinforce",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# REINFORCE\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nREINFORCE updates policy parameters with reward-weighted gradients.\n\n## Math\n\n$$\\Delta \\theta \\propto R \\cdot \\nabla \\log \\pi(a|s)$$\n\n## Function\n\n```python\ndef reinforce_update(grad_logp: float, reward: float, lr: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/reinforce/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/reinforce/python/reinforce.py",
          "language": "python",
          "content": "def reinforce_update(grad_logp: float, reward: float, lr: float) -> float:\n    return lr * reward * grad_logp\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/reinforce/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn reinforce_update(grad_logp: f64, reward: f64, lr: f64) -> f64 {\n    lr * reward * grad_logp\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "return-discount",
      "title": "Reward, Return, Discount",
      "path": "modules/ml/reinforcement-learning/return-discount",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# Reward, Return, Discount\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nReturn is the discounted sum of rewards.\n\n## Math\n\n$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}$$\n\n## Function\n\n```python\ndef discounted_return(rewards: list[float], gamma: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/return-discount/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/return-discount/python/return_discount.py",
          "language": "python",
          "content": "def discounted_return(rewards: list[float], gamma: float) -> float:\n    total = 0.0\n    for i, r in enumerate(rewards):\n        total += (gamma ** i) * r\n    return total\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/return-discount/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn discounted_return(rewards: &[f64], gamma: f64) -> f64 {\n    let mut total = 0.0;\n    for (i, r) in rewards.iter().enumerate() {\n        total += gamma.powi(i as i32) * r;\n    }\n    total\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "sarsa",
      "title": "SARSA",
      "path": "modules/ml/reinforcement-learning/sarsa",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# SARSA\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nSARSA uses the next action actually taken for updates.\n\n## Math\n\n$$Q \\leftarrow Q + \\alpha\\left(r + \\gamma Q' - Q\\right)$$\n\n## Function\n\n```python\ndef sarsa_update(q: float, reward: float, next_q: float, alpha: float, gamma: float) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/sarsa/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/sarsa/python/sarsa.py",
          "language": "python",
          "content": "def sarsa_update(q: float, reward: float, next_q: float, alpha: float, gamma: float) -> float:\n    return q + alpha * (reward + gamma * next_q - q)\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/sarsa/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn sarsa_update(q: f64, reward: f64, next_q: f64, alpha: f64, gamma: f64) -> f64 {\n    q + alpha * (reward + gamma * next_q - q)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "ucb",
      "title": "UCB",
      "path": "modules/ml/reinforcement-learning/ucb",
      "summary": "> Track: `ml` | Topic: `reinforcement-learning`",
      "readme": "# UCB\n\n> Track: `ml` | Topic: `reinforcement-learning`\n\n## Concept\n\nUpper Confidence Bound balances mean reward and uncertainty.\n\n## Math\n\n$$\\mathrm{UCB} = Q + c \\sqrt{\\frac{\\ln t}{N}}$$\n\n## Function\n\n```python\ndef ucb_score(q: float, t: int, n: int, c: float = 1.0) -> float:\n```\n\n## Run tests\n\n```bash\npytest modules/ml/reinforcement-learning/ucb/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/reinforcement-learning/ucb/python/ucb.py",
          "language": "python",
          "content": "import math\n\n\ndef ucb_score(q: float, t: int, n: int, c: float = 1.0) -> float:\n    return q + c * math.sqrt(math.log(t) / n)\n"
        },
        {
          "path": "modules/ml/reinforcement-learning/ucb/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn ucb_score(q: f64, t: f64, n: f64, c: f64) -> f64 {\n    q + c * (t.ln() / n).sqrt()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "backward-pass",
      "title": "Backward Pass",
      "path": "modules/ml/systems/backward-pass",
      "summary": "> Track: `ml` | Topic: `systems`",
      "readme": "# Backward Pass\n\n> Track: `ml` | Topic: `systems`\n\n## Concept\n\nThe backward pass computes the gradient of the loss function with respect to every learnable parameter in the network. Starting from the scalar loss at the output, it applies the chain rule layer by layer in reverse order, propagating error signals back through the computational graph. This process is the core of backpropagation and is what makes gradient-based optimization possible.\n\nEach node in the computational graph stores intermediate activations from the forward pass so that partial derivatives can be computed locally. The chain rule then multiplies these local gradients together along every path from the loss to a given parameter. Because modern frameworks build dynamic or static computation graphs automatically, the backward pass is typically invoked with a single call, but understanding its mechanics is essential for debugging vanishing or exploding gradients.\n\n## Math\n\nFor a parameter $w$ in a layer that produces output $y$ contributing to loss $L$, the chain rule gives:\n\n$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w}$$\n\nFor a multi-layer network with layers $l = 1, \\dots, N$, the gradient flows recursively:\n\n$$\\frac{\\partial L}{\\partial h^{(l)}} = \\frac{\\partial L}{\\partial h^{(l+1)}} \\cdot \\frac{\\partial h^{(l+1)}}{\\partial h^{(l)}}$$\n\n- $L$ -- scalar loss value\n- $w$ -- a learnable weight parameter\n- $y$ -- layer output (activation)\n- $h^{(l)}$ -- hidden representation at layer $l$\n\n## Key Points\n\n- The backward pass must run after the forward pass because it relies on cached intermediate activations.\n- The computation graph stores every operation and intermediate value, which is why training uses significantly more memory than inference.\n- Vanishing gradients occur when many small derivatives are multiplied together; exploding gradients occur with large derivatives -- both are diagnosed by inspecting backward-pass outputs.\n- Detaching a tensor from the graph stops gradient flow, which is useful for frozen layers or stop-gradient tricks.\n\n## Function\n\n```python\ndef backward(dy: float, x: float) -> float:\n```\n\n- `dy` -- upstream gradient $\\frac{\\partial L}{\\partial y}$ flowing back from the next layer\n- `x` -- input activation cached during the forward pass\n\n## Run tests\n\n```bash\npytest modules/ml/systems/backward-pass/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/systems/backward-pass/python/backward_pass.py",
          "language": "python",
          "content": "def backward(dy: float, x: float) -> float:\n    return dy * x\n"
        },
        {
          "path": "modules/ml/systems/backward-pass/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn backward(dy: f64, x: f64) -> f64 {\n    dy * x\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "check-gradients",
      "title": "Check Gradients Are Flowing",
      "path": "modules/ml/systems/check-gradients",
      "summary": "> Track: `ml` | Topic: `systems`",
      "readme": "# Check Gradients Are Flowing\n\n> Track: `ml` | Topic: `systems`\n\n## Concept\n\nGradient checking validates that the analytic gradients computed by backpropagation are correct by comparing them against a numerical finite-difference approximation. This is one of the most reliable debugging tools when implementing custom layers, loss functions, or activation functions. If the analytic gradient disagrees with the numerical estimate, there is almost certainly a bug in the backward pass implementation.\n\nThe central idea is to perturb each parameter by a small amount $\\epsilon$ in both directions and observe how the loss changes. The two-sided finite difference gives a second-order accurate approximation of the true gradient. By computing the relative error between this approximation and the analytic gradient, you can determine whether your implementation is correct. A relative error below $10^{-7}$ typically indicates a correct implementation, while values above $10^{-5}$ suggest a bug.\n\n## Math\n\nThe two-sided finite-difference approximation for each parameter $\\theta_i$ is:\n\n$$g_{\\text{approx},i} = \\frac{f(\\theta_i + \\epsilon) - f(\\theta_i - \\epsilon)}{2\\epsilon}$$\n\nThe relative error between the analytic gradient $g$ and the approximation $g_{\\text{approx}}$ is:\n\n$$e = \\frac{\\|g - g_{\\text{approx}}\\|}{\\|g\\| + \\|g_{\\text{approx}}\\| + \\delta}$$\n\n- $\\epsilon$ -- small perturbation, typically $10^{-5}$ to $10^{-7}$\n- $g$ -- analytic gradient from backpropagation\n- $g_{\\text{approx}}$ -- numerical finite-difference gradient\n- $\\delta$ -- small constant to avoid division by zero\n\n## Key Points\n\n- Use gradient checking only for debugging, never during training -- it is orders of magnitude slower than backpropagation.\n- Disable dropout and batch normalization during the check, as their stochastic behavior breaks the finite-difference assumption.\n- A relative error below $10^{-7}$ is excellent; between $10^{-5}$ and $10^{-7}$ is acceptable; above $10^{-5}$ likely indicates a bug.\n- Check that gradients are non-zero and finite: all-zero gradients signal dead neurons or disconnected graph paths, while NaN or Inf gradients indicate numerical instability.\n\n## Function\n\n```python\ndef gradients_ok(grads: list[float]) -> bool:\n```\n\n- `grads` -- list of gradient values to validate (one per parameter)\n\n## Run tests\n\n```bash\npytest modules/ml/systems/check-gradients/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/systems/check-gradients/python/check_gradients.py",
          "language": "python",
          "content": "import math\n\n\ndef gradients_ok(grads: list[float]) -> bool:\n    if any(math.isnan(g) for g in grads):\n        return False\n    return any(abs(g) > 0 for g in grads)\n"
        },
        {
          "path": "modules/ml/systems/check-gradients/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn gradients_ok(grads: &[f64]) -> bool {\n    if grads.iter().any(|g| g.is_nan()) { return false; }\n    grads.iter().any(|g| g.abs() > 0.0)\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "debug-overfit-underfit",
      "title": "Debug Overfitting vs Underfitting",
      "path": "modules/ml/systems/debug-overfit-underfit",
      "summary": "> Track: `ml` | Topic: `systems`",
      "readme": "# Debug Overfitting vs Underfitting\n\n> Track: `ml` | Topic: `systems`\n\n## Concept\n\nDebugging overfitting and underfitting is the process of comparing training loss against validation loss to diagnose whether a model has the right capacity for the task. Overfitting means the model memorizes training data but fails to generalize, visible as low training loss paired with high validation loss. Underfitting means the model lacks the capacity or has not trained long enough, visible as both losses remaining high.\n\nThese failure modes connect directly to the bias-variance tradeoff. An underfit model has high bias -- it cannot capture the true underlying pattern. An overfit model has high variance -- it is too sensitive to noise in the training set. A practical debugging workflow starts by overfitting a single small batch to verify the model and optimizer are wired correctly, then scales up to the full dataset while monitoring the gap between training and validation loss.\n\n## Math\n\nThe expected test error decomposes into bias, variance, and irreducible noise:\n\n$$\\mathbb{E}[L_{\\text{test}}] = \\mathrm{Bias}^2 + \\mathrm{Variance} + \\sigma^2_{\\text{noise}}$$\n\nDiagnostic rules based on observed losses:\n\n$$L_{\\text{train}} \\ll L_{\\text{val}} \\implies \\text{overfitting (high variance)}$$\n\n$$L_{\\text{train}} \\approx L_{\\text{val}} \\gg L^{*} \\implies \\text{underfitting (high bias)}$$\n\n- $L_{\\text{train}}$ -- average loss on the training set\n- $L_{\\text{val}}$ -- average loss on the validation set\n- $L^{*}$ -- irreducible (Bayes-optimal) error\n- $\\sigma^2_{\\text{noise}}$ -- variance of the irreducible noise\n\n## Key Points\n\n- Always overfit a single small batch first to confirm the model can memorize -- if it cannot, there is a bug before you even consider generalization.\n- Overfitting remedies include regularization (dropout, weight decay, data augmentation) and reducing model size.\n- Underfitting remedies include increasing model capacity, training longer, or reducing regularization strength.\n- The gap between training and validation loss is more informative than either value alone.\n- Early stopping monitors validation loss and halts training when it begins to rise, providing a simple defense against overfitting.\n\n## Function\n\n```python\ndef diagnose(train_loss: float, val_loss: float) -> str:\n```\n\n- `train_loss` -- average loss computed on the training set\n- `val_loss` -- average loss computed on the validation set\n\n## Run tests\n\n```bash\npytest modules/ml/systems/debug-overfit-underfit/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/systems/debug-overfit-underfit/python/debug_overfit_underfit.py",
          "language": "python",
          "content": "def diagnose(train_loss: float, val_loss: float) -> str:\n    if train_loss < val_loss * 0.7:\n        return \"overfit\"\n    if train_loss > 1.0 and val_loss > 1.0:\n        return \"underfit\"\n    return \"ok\"\n"
        },
        {
          "path": "modules/ml/systems/debug-overfit-underfit/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn diagnose(train_loss: f64, val_loss: f64) -> String {\n    if train_loss < val_loss * 0.7 {\n        \"overfit\".to_string()\n    } else if train_loss > 1.0 && val_loss > 1.0 {\n        \"underfit\".to_string()\n    } else {\n        \"ok\".to_string()\n    }\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "forward-pass",
      "title": "Forward Pass",
      "path": "modules/ml/systems/forward-pass",
      "summary": "> Track: `ml` | Topic: `systems`",
      "readme": "# Forward Pass\n\n> Track: `ml` | Topic: `systems`\n\n## Concept\n\nThe forward pass computes the model's output by propagating input data through each layer sequentially, from the first hidden layer to the final prediction. At every layer, a linear transformation is applied followed by a non-linear activation function. The composition of these transformations gives the network its expressive power.\n\nDuring training, the forward pass also records all intermediate activations in the computational graph so that the backward pass can compute gradients. During inference, this bookkeeping is unnecessary and can be disabled (e.g., `torch.no_grad()`) to save memory and improve throughput. Understanding the forward pass is essential because the structure of the computation determines both the model's capacity and the shape of gradients that flow backward.\n\n## Math\n\nFor a single layer with weight matrix $W$, bias vector $b$, and activation function $\\sigma$:\n\n$$h = \\sigma(W x + b)$$\n\nFor a network with $N$ layers, the full forward pass composes each layer:\n\n$$h^{(l)} = \\sigma^{(l)}\\!\\left(W^{(l)} h^{(l-1)} + b^{(l)}\\right), \\quad l = 1, \\dots, N$$\n\n- $x$ -- input feature vector (or $h^{(0)}$)\n- $W^{(l)}$ -- weight matrix at layer $l$\n- $b^{(l)}$ -- bias vector at layer $l$\n- $\\sigma^{(l)}$ -- activation function at layer $l$ (e.g., ReLU, sigmoid)\n- $h^{(l)}$ -- hidden representation (activation) at layer $l$\n\n## Key Points\n\n- Intermediate activations must be stored during training for use in the backward pass, which is why training consumes more memory than inference.\n- In inference mode, gradient tracking is disabled to reduce memory usage and computation.\n- The choice of activation function at each layer affects both the forward output and the gradient behavior during backpropagation.\n- Batch dimensions are handled implicitly: $x$ is typically a matrix where each row is one sample in the mini-batch.\n\n## Function\n\n```python\ndef forward(x: list[float], w: list[float], b: float) -> float:\n```\n\n- `x` -- input feature vector\n- `w` -- weight vector (one weight per input feature)\n- `b` -- scalar bias term\n\n## Run tests\n\n```bash\npytest modules/ml/systems/forward-pass/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/systems/forward-pass/python/forward_pass.py",
          "language": "python",
          "content": "def forward(x: list[float], w: list[float], b: float) -> float:\n    return sum(wi * xi for wi, xi in zip(w, x)) + b\n"
        },
        {
          "path": "modules/ml/systems/forward-pass/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn forward(x: &[f64], w: &[f64], b: f64) -> f64 {\n    let mut out = b;\n    for (wi, xi) in w.iter().zip(x.iter()) {\n        out += wi * xi;\n    }\n    out\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "gradient-accumulation",
      "title": "Gradient Accumulation",
      "path": "modules/ml/systems/gradient-accumulation",
      "summary": "> Track: `ml` | Topic: `systems`",
      "readme": "# Gradient Accumulation\n\n> Track: `ml` | Topic: `systems`\n\n## Concept\n\nGradient accumulation is a technique that simulates training with a large batch size by splitting it into several smaller micro-batches. Instead of updating parameters after every micro-batch, gradients are summed (or averaged) over $K$ micro-batches, and only then is the optimizer step performed. This produces mathematically equivalent updates to using a single large batch, while requiring only enough GPU memory to process one micro-batch at a time.\n\nThis technique is especially important when training large models where the desired batch size exceeds available GPU memory. It is also the foundation of distributed training strategies like data parallelism, where each device computes gradients on its own micro-batch and the results are aggregated before the optimizer step. The key implementation detail is to divide the accumulated gradient by the number of accumulation steps $K$ so the effective learning rate remains consistent.\n\n## Math\n\nGiven $K$ micro-batches each of size $B_{\\text{micro}}$, the effective gradient used for the parameter update is:\n\n$$g_{\\text{eff}} = \\frac{1}{K} \\sum_{k=1}^{K} g_k$$\n\nThe effective batch size is:\n\n$$B_{\\text{eff}} = K \\cdot B_{\\text{micro}}$$\n\n- $g_k$ -- gradient computed on the $k$-th micro-batch\n- $K$ -- number of accumulation steps\n- $B_{\\text{micro}}$ -- micro-batch size (samples per forward/backward pass)\n- $B_{\\text{eff}}$ -- effective batch size seen by the optimizer\n\n## Key Points\n\n- Gradient accumulation enables large effective batch sizes on hardware with limited memory by trading compute time for memory.\n- Always divide the accumulated gradient by $K$ (or equivalently scale the loss by $\\frac{1}{K}$) -- forgetting this changes the effective learning rate.\n- Zero the gradients only after the optimizer step, not after each micro-batch, otherwise the accumulation is lost.\n- Batch normalization statistics are computed per micro-batch, not per effective batch, which can slightly change training dynamics compared to true large-batch training.\n\n## Function\n\n```python\ndef accumulate(grads: list[list[float]]) -> list[float]:\n```\n\n- `grads` -- list of $K$ gradient vectors, one per micro-batch; each inner list has one entry per parameter\n\n## Run tests\n\n```bash\npytest modules/ml/systems/gradient-accumulation/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/systems/gradient-accumulation/python/gradient_accumulation.py",
          "language": "python",
          "content": "def accumulate(grads: list[list[float]]) -> list[float]:\n    if not grads:\n        return []\n    total = [0.0 for _ in grads[0]]\n    for g in grads:\n        total = [a + b for a, b in zip(total, g)]\n    return total\n"
        },
        {
          "path": "modules/ml/systems/gradient-accumulation/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn accumulate(grads: &[Vec<f64>]) -> Vec<f64> {\n    if grads.is_empty() { return Vec::new(); }\n    let mut total = vec![0.0; grads[0].len()];\n    for g in grads {\n        for i in 0..g.len() {\n            total[i] += g[i];\n        }\n    }\n    total\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "mixed-precision",
      "title": "Mixed Precision Training",
      "path": "modules/ml/systems/mixed-precision",
      "summary": "> Track: `ml` | Topic: `systems`",
      "readme": "# Mixed Precision Training\n\n> Track: `ml` | Topic: `systems`\n\n## Concept\n\nMixed precision training accelerates deep learning by performing the forward and backward passes in half precision (FP16) while maintaining a master copy of the weights in full precision (FP32). FP16 arithmetic is significantly faster on modern GPUs equipped with tensor cores and uses half the memory, enabling larger models and batch sizes. The FP32 master weights ensure that small gradient updates are not lost due to the limited precision of FP16.\n\nThe main challenge with FP16 is its narrow dynamic range: gradients can underflow to zero during the backward pass. Loss scaling addresses this by multiplying the loss by a large factor before backpropagation, which shifts the gradients into the representable FP16 range. After the backward pass, the scaled gradients are divided by the same factor before the optimizer step. An alternative is BF16 (bfloat16), which has the same exponent range as FP32 and therefore rarely requires loss scaling, though it sacrifices some mantissa precision.\n\n## Math\n\nThe loss-scaled backward pass computes:\n\n$$g_{\\text{scaled}} = \\frac{\\partial (S \\cdot L)}{\\partial \\theta} = S \\cdot \\frac{\\partial L}{\\partial \\theta}$$\n\nBefore the optimizer update, the gradients are unscaled:\n\n$$g = \\frac{g_{\\text{scaled}}}{S}$$\n\nThe master weight update remains in FP32:\n\n$$\\theta_{\\text{FP32}} \\leftarrow \\theta_{\\text{FP32}} - \\alpha \\cdot g$$\n\n- $S$ -- loss scale factor (a large constant, often dynamically adjusted)\n- $L$ -- loss value\n- $g$ -- true gradient in FP32\n- $g_{\\text{scaled}}$ -- gradient computed under loss scaling in FP16\n- $\\alpha$ -- learning rate\n- $\\theta_{\\text{FP32}}$ -- master copy of weights in full precision\n\n## Key Points\n\n- Loss scaling prevents gradient underflow in FP16 by shifting small gradient magnitudes into the representable range.\n- BF16 has the same exponent range as FP32, so it largely eliminates the need for loss scaling at the cost of slightly lower mantissa precision.\n- Always keep the optimizer state and master weights in FP32 -- accumulating small updates in FP16 leads to stagnation.\n- Dynamic loss scaling automatically adjusts the scale factor: it increases the scale when no overflow is detected and decreases it upon encountering Inf or NaN gradients.\n\n## Function\n\n```python\ndef scale_gradients(grads: list[float], scale: float) -> list[float]:\n```\n\n- `grads` -- list of gradient values to be scaled\n- `scale` -- loss scale factor $S$ to multiply each gradient by\n\n## Run tests\n\n```bash\npytest modules/ml/systems/mixed-precision/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/systems/mixed-precision/python/mixed_precision.py",
          "language": "python",
          "content": "def scale_gradients(grads: list[float], scale: float) -> list[float]:\n    return [g * scale for g in grads]\n"
        },
        {
          "path": "modules/ml/systems/mixed-precision/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn scale_gradients(grads: &[f64], scale: f64) -> Vec<f64> {\n    grads.iter().map(|g| g * scale).collect()\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "optimizer-step",
      "title": "Optimizer Step",
      "path": "modules/ml/systems/optimizer-step",
      "summary": "> Track: `ml` | Topic: `systems`",
      "readme": "# Optimizer Step\n\n> Track: `ml` | Topic: `systems`\n\n## Concept\n\nThe optimizer step applies the accumulated gradients to update model parameters, moving them in the direction that reduces the loss. In its simplest form (SGD), each parameter is adjusted by subtracting the gradient scaled by a learning rate. More advanced optimizers like Adam maintain additional state (momentum, second moments) to adapt the step size per parameter, but the fundamental idea remains the same: use gradient information to take a step in parameter space.\n\nThe optimizer step is always called after the backward pass has populated the gradient buffers. The learning rate is the single most important hyperparameter controlling step size -- too large and training diverges, too small and convergence is painfully slow. Learning rate schedules (warmup, cosine decay, step decay) dynamically adjust the learning rate during training to balance fast early progress with fine-grained later convergence.\n\n## Math\n\nThe basic SGD parameter update rule is:\n\n$$\\theta \\leftarrow \\theta - \\alpha \\cdot g$$\n\nWith momentum $\\beta$, the update becomes:\n\n$$v \\leftarrow \\beta \\cdot v + g$$\n\n$$\\theta \\leftarrow \\theta - \\alpha \\cdot v$$\n\n- $\\theta$ -- model parameter (weight or bias)\n- $\\alpha$ -- learning rate (step size)\n- $g$ -- gradient $\\nabla_\\theta L$ of the loss with respect to $\\theta$\n- $v$ -- velocity (momentum buffer)\n- $\\beta$ -- momentum coefficient, typically $0.9$\n\n## Key Points\n\n- The optimizer step must be called after the backward pass and before zeroing the gradients for the next iteration.\n- The learning rate $\\alpha$ controls the step size: too large causes divergence, too small causes slow convergence.\n- Advanced optimizers (Adam, AdaGrad, RMSProp) maintain per-parameter adaptive learning rates using gradient statistics.\n- Gradient clipping is often applied before the optimizer step to prevent excessively large updates from destabilizing training.\n\n## Function\n\n```python\ndef step(w: float, grad: float, lr: float) -> float:\n```\n\n- `w` -- current parameter value $\\theta$\n- `grad` -- gradient of the loss with respect to `w`\n- `lr` -- learning rate $\\alpha$\n\n## Run tests\n\n```bash\npytest modules/ml/systems/optimizer-step/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/systems/optimizer-step/python/optimizer_step.py",
          "language": "python",
          "content": "def step(w: float, grad: float, lr: float) -> float:\n    return w - lr * grad\n"
        },
        {
          "path": "modules/ml/systems/optimizer-step/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn step(w: f64, grad: f64, lr: f64) -> f64 {\n    w - lr * grad\n}\n"
        }
      ]
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "zero-gradients",
      "title": "Zeroing Gradients",
      "path": "modules/ml/systems/zero-gradients",
      "summary": "> Track: `ml` | Topic: `systems`",
      "readme": "# Zeroing Gradients\n\n> Track: `ml` | Topic: `systems`\n\n## Concept\n\nZeroing gradients resets all parameter gradient buffers to zero before the next backward pass. In most deep learning frameworks, gradients are accumulated by default rather than overwritten, so calling the backward pass without zeroing first will add the new gradients on top of the old ones. This is almost always a bug -- unless you are intentionally using gradient accumulation.\n\nThe standard training loop follows the pattern: zero gradients, forward pass, compute loss, backward pass, optimizer step. Forgetting the zeroing step is one of the most common mistakes when writing training loops from scratch. The resulting incorrect gradient accumulation causes erratic loss curves and makes the model appear to diverge even when the learning rate and architecture are correct. Frameworks like PyTorch provide `optimizer.zero_grad()` to handle this in one call across all parameter groups.\n\n## Math\n\nBefore each backward pass, every gradient buffer is reset:\n\n$$g_i \\leftarrow 0, \\quad \\forall\\, i \\in \\{1, \\dots, P\\}$$\n\nWithout zeroing, the gradient at iteration $t$ incorrectly becomes:\n\n$$g_i^{(t)} = \\sum_{s=1}^{t} \\nabla_{\\theta_i} L^{(s)}$$\n\n- $g_i$ -- gradient buffer for the $i$-th parameter\n- $P$ -- total number of learnable parameters\n- $\\nabla_{\\theta_i} L^{(s)}$ -- gradient contribution from iteration $s$\n\n## Key Points\n\n- Forgetting to zero gradients causes gradients from previous iterations to accumulate, leading to incorrect and increasingly large updates.\n- In PyTorch, `optimizer.zero_grad()` or `model.zero_grad()` clears all gradient buffers; calling it at the start of each iteration is standard practice.\n- When using gradient accumulation intentionally, you skip the zeroing step for $K-1$ micro-batches and only zero after the optimizer step.\n- Setting gradients to `None` instead of zero can be slightly more memory-efficient in some frameworks, as it avoids allocating the zero tensor.\n\n## Function\n\n```python\ndef zero_grad(grads: list[float]) -> list[float]:\n```\n\n- `grads` -- list of current gradient values (one per parameter) to be reset to zero\n\n## Run tests\n\n```bash\npytest modules/ml/systems/zero-gradients/python -q\n```\n",
      "sources": [
        {
          "path": "modules/ml/systems/zero-gradients/python/zero_gradients.py",
          "language": "python",
          "content": "def zero_grad(grads: list[float]) -> list[float]:\n    return [0.0 for _ in grads]\n"
        },
        {
          "path": "modules/ml/systems/zero-gradients/rust/src/lib.rs",
          "language": "rust",
          "content": "pub fn zero_grad(grads: &[f64]) -> Vec<f64> {\n    vec![0.0; grads.len()]\n}\n"
        }
      ]
    }
  ],
  "docs": [
    {
      "track": "dsa",
      "topic": "array",
      "slug": "array",
      "title": "Array",
      "path": "docs/dsa/array/README.md",
      "summary": "Order of modules for this topic:",
      "content": "# Array\n\nOrder of modules for this topic:\n\n- [Introduction to Arrays](modules/dsa/array/introduction-to-arrays)\n- [704.Binary Search](modules/dsa/array/704-binary-search)\n- [27.Remove Element](modules/dsa/array/27-remove-element)\n- [977.Squares of a Sorted Array](modules/dsa/array/977-squares-of-a-sorted-array)\n- [209.Minimum Size Subarray Sum](modules/dsa/array/209-minimum-size-subarray-sum)\n- [59.Spiral Matrix II](modules/dsa/array/59-spiral-matrix-ii)\n- [Array Summary](modules/dsa/array/array-summary)\n"
    },
    {
      "track": "dsa",
      "topic": "backtracking",
      "slug": "backtracking",
      "title": "Backtracking",
      "path": "docs/dsa/backtracking/README.md",
      "summary": "Order of modules for this topic:",
      "content": "# Backtracking\n\nOrder of modules for this topic:\n\n- [Introduction to Backtracking](modules/dsa/backtracking/introduction-to-backtracking)\n- [77.Combinations](modules/dsa/backtracking/77-combinations)\n- [77.Combinations (Optimized)](modules/dsa/backtracking/77-combinations-optimized)\n- [216.Combination Sum III](modules/dsa/backtracking/216-combination-sum-iii)\n- [17.Letter Combinations of a Phone Number](modules/dsa/backtracking/17-letter-combinations-of-a-phone-number)\n- [39.Combination Sum](modules/dsa/backtracking/39-combination-sum)\n- [40.Combination Sum II](modules/dsa/backtracking/40-combination-sum-ii)\n- [131.Palindrome Partitioning](modules/dsa/backtracking/131-palindrome-partitioning)\n- [93.Restore IP Addresses](modules/dsa/backtracking/93-restore-ip-addresses)\n- [78.Subsets](modules/dsa/backtracking/78-subsets)\n- [90.Subsets II](modules/dsa/backtracking/90-subsets-ii)\n- [491.Increasing Subsequences](modules/dsa/backtracking/491-increasing-subsequences)\n- [46.Permutations](modules/dsa/backtracking/46-permutations)\n- [47.Permutations II](modules/dsa/backtracking/47-permutations-ii)\n- [Alternative Deduplication in Backtracking](modules/dsa/backtracking/alternative-deduplication-in-backtracking)\n- [332.Reconstruct Itinerary](modules/dsa/backtracking/332-reconstruct-itinerary)\n- [51.N-Queens](modules/dsa/backtracking/51-n-queens)\n- [37.Sudoku Solver](modules/dsa/backtracking/37-sudoku-solver)\n- [Backtracking Summary](modules/dsa/backtracking/backtracking-summary)\n"
    },
    {
      "track": "dsa",
      "topic": "binary-tree",
      "slug": "binary-tree",
      "title": "Binary Tree",
      "path": "docs/dsa/binary-tree/README.md",
      "summary": "Order of modules for this topic:",
      "content": "# Binary Tree\n\nOrder of modules for this topic:\n\n- [Introduction to Binary Trees](modules/dsa/binary-tree/introduction-to-binary-trees)\n- [Recursive Traversal of Binary Trees](modules/dsa/binary-tree/recursive-traversal-of-binary-trees)\n- [Iterative Traversal of Binary Trees](modules/dsa/binary-tree/iterative-traversal-of-binary-trees)\n- [Unified Iterative Method for Binary Trees](modules/dsa/binary-tree/unified-iterative-method-for-binary-trees)\n- [102.Binary Tree Level Order Traversal](modules/dsa/binary-tree/102-binary-tree-level-order-traversal)\n- [226.Invert Binary Tree](modules/dsa/binary-tree/226-invert-binary-tree)\n- [101.Symmetric Tree](modules/dsa/binary-tree/101-symmetric-tree)\n- [104.Maximum Depth of Binary Tree](modules/dsa/binary-tree/104-maximum-depth-of-binary-tree)\n- [111.Minimum Depth of Binary Tree](modules/dsa/binary-tree/111-minimum-depth-of-binary-tree)\n- [222.Count Complete Tree Nodes](modules/dsa/binary-tree/222-count-complete-tree-nodes)\n- [110.Balanced Binary Tree](modules/dsa/binary-tree/110-balanced-binary-tree)\n- [257.Binary Tree Paths](modules/dsa/binary-tree/257-binary-tree-paths)\n- [404.Sum of Left Leaves](modules/dsa/binary-tree/404-sum-of-left-leaves)\n- [513.Find Bottom Left Tree Value](modules/dsa/binary-tree/513-find-bottom-left-tree-value)\n- [112.Path Sum](modules/dsa/binary-tree/112-path-sum)\n- [106.Construct Binary Tree from Inorder and Postorder Traversal](modules/dsa/binary-tree/106-construct-binary-tree-from-inorder-and-postorder-traversal)\n- [654.Maximum Binary Tree](modules/dsa/binary-tree/654-maximum-binary-tree)\n- [617.Merge Two Binary Trees](modules/dsa/binary-tree/617-merge-two-binary-trees)\n- [700.Search in a Binary Search Tree](modules/dsa/binary-tree/700-search-in-a-binary-search-tree)\n- [98.Validate Binary Search Tree](modules/dsa/binary-tree/98-validate-binary-search-tree)\n- [530.Minimum Absolute Difference in BST](modules/dsa/binary-tree/530-minimum-absolute-difference-in-bst)\n- [501.Find Mode in Binary Search Tree](modules/dsa/binary-tree/501-find-mode-in-binary-search-tree)\n- [236.Lowest Common Ancestor of a Binary Tree](modules/dsa/binary-tree/236-lowest-common-ancestor-of-a-binary-tree)\n- [235.Lowest Common Ancestor of a BST](modules/dsa/binary-tree/235-lowest-common-ancestor-of-a-bst)\n- [701.Insert into a Binary Search Tree](modules/dsa/binary-tree/701-insert-into-a-binary-search-tree)\n- [450.Delete Node in a BST](modules/dsa/binary-tree/450-delete-node-in-a-bst)\n- [669.Trim a Binary Search Tree](modules/dsa/binary-tree/669-trim-a-binary-search-tree)\n- [108.Convert Sorted Array to BST](modules/dsa/binary-tree/108-convert-sorted-array-to-bst)\n- [538.Convert BST to Greater Tree](modules/dsa/binary-tree/538-convert-bst-to-greater-tree)\n- [Binary Tree Summary](modules/dsa/binary-tree/binary-tree-summary)\n"
    },
    {
      "track": "dsa",
      "topic": "double-pointers",
      "slug": "double-pointers",
      "title": "Double Pointers",
      "path": "docs/dsa/double-pointers/README.md",
      "summary": "Order of modules for this topic:",
      "content": "# Double Pointers\n\nOrder of modules for this topic:\n\n- [27.Remove Element](modules/dsa/double-pointers/27-remove-element)\n- [344.Reverse String](modules/dsa/double-pointers/344-reverse-string)\n- [151.Reverse Words in a String](modules/dsa/double-pointers/151-reverse-words-in-a-string)\n- [206.Reverse Linked List](modules/dsa/double-pointers/206-reverse-linked-list)\n- [19.Remove Nth Node From End of List](modules/dsa/double-pointers/19-remove-nth-node-from-end-of-list)\n- [160.Intersection of Two Linked Lists](modules/dsa/double-pointers/160-intersection-of-two-linked-lists)\n- [142.Linked List Cycle II](modules/dsa/double-pointers/142-linked-list-cycle-ii)\n- [15.3Sum](modules/dsa/double-pointers/15-3sum)\n- [18.4Sum](modules/dsa/double-pointers/18-4sum)\n- [Double Pointers Summary](modules/dsa/double-pointers/double-pointers-summary)\n"
    },
    {
      "track": "dsa",
      "topic": "dynamic-programming",
      "slug": "dynamic-programming",
      "title": "Dynamic Programming",
      "path": "docs/dsa/dynamic-programming/README.md",
      "summary": "Order of modules for this topic:",
      "content": "# Dynamic Programming\n\nOrder of modules for this topic:\n\n- [Introduction to Dynamic Programming](modules/dsa/dynamic-programming/introduction-to-dynamic-programming)\n- [509.Fibonacci Number](modules/dsa/dynamic-programming/509-fibonacci-number)\n- [70.Climbing Stairs](modules/dsa/dynamic-programming/70-climbing-stairs)\n- [746.Min Cost Climbing Stairs](modules/dsa/dynamic-programming/746-min-cost-climbing-stairs)\n- [62.Unique Paths](modules/dsa/dynamic-programming/62-unique-paths)\n- [63.Unique Paths II](modules/dsa/dynamic-programming/63-unique-paths-ii)\n- [343.Integer Break](modules/dsa/dynamic-programming/343-integer-break)\n- [96.Unique Binary Search Trees](modules/dsa/dynamic-programming/96-unique-binary-search-trees)\n- [0-1 Knapsack Theory (Part 1)](modules/dsa/dynamic-programming/0-1-knapsack-theory-part-1)\n- [0-1 Knapsack Theory (Part 2)](modules/dsa/dynamic-programming/0-1-knapsack-theory-part-2)\n- [416.Partition Equal Subset Sum](modules/dsa/dynamic-programming/416-partition-equal-subset-sum)\n- [1049.Last Stone Weight II](modules/dsa/dynamic-programming/1049-last-stone-weight-ii)\n- [494.Target Sum](modules/dsa/dynamic-programming/494-target-sum)\n- [474.Ones and Zeroes](modules/dsa/dynamic-programming/474-ones-and-zeroes)\n- [Complete Knapsack Theory](modules/dsa/dynamic-programming/complete-knapsack-theory)\n- [518.Coin Change II](modules/dsa/dynamic-programming/518-coin-change-ii)\n- [377.Combination Sum IV](modules/dsa/dynamic-programming/377-combination-sum-iv)\n- [Climbing Stairs (Advanced)](modules/dsa/dynamic-programming/climbing-stairs-advanced)\n- [322.Coin Change](modules/dsa/dynamic-programming/322-coin-change)\n- [279.Perfect Squares](modules/dsa/dynamic-programming/279-perfect-squares)\n- [139.Word Break](modules/dsa/dynamic-programming/139-word-break)\n- [Multiple Knapsack Theory](modules/dsa/dynamic-programming/multiple-knapsack-theory)\n- [Knapsack Summary](modules/dsa/dynamic-programming/knapsack-summary)\n- [198.House Robber](modules/dsa/dynamic-programming/198-house-robber)\n- [213.House Robber II](modules/dsa/dynamic-programming/213-house-robber-ii)\n- [337.House Robber III](modules/dsa/dynamic-programming/337-house-robber-iii)\n- [121.Best Time to Buy and Sell Stock](modules/dsa/dynamic-programming/121-best-time-to-buy-and-sell-stock)\n- [122.Best Time to Buy and Sell Stock II (DP)](modules/dsa/dynamic-programming/122-best-time-to-buy-and-sell-stock-ii-dp)\n- [123.Best Time to Buy and Sell Stock III](modules/dsa/dynamic-programming/123-best-time-to-buy-and-sell-stock-iii)\n- [188.Best Time to Buy and Sell Stock IV](modules/dsa/dynamic-programming/188-best-time-to-buy-and-sell-stock-iv)\n- [309.Best Time to Buy and Sell Stock with Cooldown](modules/dsa/dynamic-programming/309-best-time-to-buy-and-sell-stock-with-cooldown)\n- [714.Best Time to Buy and Sell Stock with Transaction Fee](modules/dsa/dynamic-programming/714-best-time-to-buy-and-sell-stock-with-transaction-fee)\n- [Stock Problems Summary](modules/dsa/dynamic-programming/stock-problems-summary)\n- [300.Longest Increasing Subsequence](modules/dsa/dynamic-programming/300-longest-increasing-subsequence)\n- [674.Longest Continuous Increasing Subsequence](modules/dsa/dynamic-programming/674-longest-continuous-increasing-subsequence)\n- [718.Maximum Length of Repeated Subarray](modules/dsa/dynamic-programming/718-maximum-length-of-repeated-subarray)\n- [1143.Longest Common Subsequence](modules/dsa/dynamic-programming/1143-longest-common-subsequence)\n- [1035.Uncrossed Lines](modules/dsa/dynamic-programming/1035-uncrossed-lines)\n- [53.Maximum Subarray (DP)](modules/dsa/dynamic-programming/53-maximum-subarray-dp)\n- [392.Is Subsequence](modules/dsa/dynamic-programming/392-is-subsequence)\n- [115.Distinct Subsequences](modules/dsa/dynamic-programming/115-distinct-subsequences)\n- [583.Delete Operation for Two Strings](modules/dsa/dynamic-programming/583-delete-operation-for-two-strings)\n- [72.Edit Distance](modules/dsa/dynamic-programming/72-edit-distance)\n- [Edit Distance Summary](modules/dsa/dynamic-programming/edit-distance-summary)\n- [647.Palindromic Substrings](modules/dsa/dynamic-programming/647-palindromic-substrings)\n- [516.Longest Palindromic Subsequence](modules/dsa/dynamic-programming/516-longest-palindromic-subsequence)\n- [Dynamic Programming Summary](modules/dsa/dynamic-programming/dynamic-programming-summary)\n"
    },
    {
      "track": "dsa",
      "topic": "greedy-algorithm",
      "slug": "greedy-algorithm",
      "title": "Greedy Algorithm",
      "path": "docs/dsa/greedy-algorithm/README.md",
      "summary": "Order of modules for this topic:",
      "content": "# Greedy Algorithm\n\nOrder of modules for this topic:\n\n- [Introduction to Greedy Algorithm](modules/dsa/greedy-algorithm/introduction-to-greedy-algorithm)\n- [455.Assign Cookies](modules/dsa/greedy-algorithm/455-assign-cookies)\n- [376.Wiggle Subsequence](modules/dsa/greedy-algorithm/376-wiggle-subsequence)\n- [53.Maximum Subarray](modules/dsa/greedy-algorithm/53-maximum-subarray)\n- [122.Best Time to Buy and Sell Stock II](modules/dsa/greedy-algorithm/122-best-time-to-buy-and-sell-stock-ii)\n- [55.Jump Game](modules/dsa/greedy-algorithm/55-jump-game)\n- [45.Jump Game II](modules/dsa/greedy-algorithm/45-jump-game-ii)\n- [1005.Maximize Sum After K Negations](modules/dsa/greedy-algorithm/1005-maximize-sum-after-k-negations)\n- [134.Gas Station](modules/dsa/greedy-algorithm/134-gas-station)\n- [135.Candy](modules/dsa/greedy-algorithm/135-candy)\n- [860.Lemonade Change](modules/dsa/greedy-algorithm/860-lemonade-change)\n- [406.Queue Reconstruction by Height](modules/dsa/greedy-algorithm/406-queue-reconstruction-by-height)\n- [Queue Reconstruction by Height (Vector Explanation)](modules/dsa/greedy-algorithm/queue-reconstruction-by-height-vector-explanation)\n- [452.Minimum Number of Arrows to Burst Balloons](modules/dsa/greedy-algorithm/452-minimum-number-of-arrows-to-burst-balloons)\n- [435.Non-overlapping Intervals](modules/dsa/greedy-algorithm/435-non-overlapping-intervals)\n- [763.Partition Labels](modules/dsa/greedy-algorithm/763-partition-labels)\n- [56.Merge Intervals](modules/dsa/greedy-algorithm/56-merge-intervals)\n- [738.Monotone Increasing Digits](modules/dsa/greedy-algorithm/738-monotone-increasing-digits)\n- [968.Binary Tree Cameras](modules/dsa/greedy-algorithm/968-binary-tree-cameras)\n- [Greedy Algorithms Summary](modules/dsa/greedy-algorithm/greedy-algorithms-summary)\n"
    },
    {
      "track": "dsa",
      "topic": "hash-tables",
      "slug": "hash-tables",
      "title": "Hash Tables",
      "path": "docs/dsa/hash-tables/README.md",
      "summary": "Order of modules for this topic:",
      "content": "# Hash Tables\n\nOrder of modules for this topic:\n\n- [Introduction to Hash Table](modules/dsa/hash-tables/introduction-to-hash-table)\n- [242.Valid Anagram](modules/dsa/hash-tables/242-valid-anagram)\n- [349.Intersection of Two Arrays](modules/dsa/hash-tables/349-intersection-of-two-arrays)\n- [202.Happy Number](modules/dsa/hash-tables/202-happy-number)\n- [1.Two Sum](modules/dsa/hash-tables/1-two-sum)\n- [454.4Sum II](modules/dsa/hash-tables/454-4sum-ii)\n- [383.Ransom Note](modules/dsa/hash-tables/383-ransom-note)\n- [15.3Sum](modules/dsa/hash-tables/15-3sum)\n- [18.4Sum](modules/dsa/hash-tables/18-4sum)\n- [Hash Table Summary](modules/dsa/hash-tables/hash-table-summary)\n"
    },
    {
      "track": "dsa",
      "topic": "linked-list",
      "slug": "linked-list",
      "title": "Linked List",
      "path": "docs/dsa/linked-list/README.md",
      "summary": "Order of modules for this topic:",
      "content": "# Linked List\n\nOrder of modules for this topic:\n\n- [Linked List Basics](modules/dsa/linked-list/linked-list-basics)\n- [203.Remove Linked List Elements](modules/dsa/linked-list/203-remove-linked-list-elements)\n- [707.Design Linked List](modules/dsa/linked-list/707-design-linked-list)\n- [206.Reverse Linked List](modules/dsa/linked-list/206-reverse-linked-list)\n- [24.Swap Nodes In Pairs](modules/dsa/linked-list/24-swap-nodes-in-pairs)\n- [19.Remove Nth Node From End Of List](modules/dsa/linked-list/19-remove-nth-node-from-end-of-list)\n- [160.Intersection Of Two Linked Lists](modules/dsa/linked-list/160-intersection-of-two-linked-lists)\n- [142.Linked List Cycle II](modules/dsa/linked-list/142-linked-list-cycle-ii)\n- [Linked List Summary](modules/dsa/linked-list/linked-list-summary)\n"
    },
    {
      "track": "dsa",
      "topic": "monotonic-stack",
      "slug": "monotonic-stack",
      "title": "Monotonic Stack",
      "path": "docs/dsa/monotonic-stack/README.md",
      "summary": "Order of modules for this topic:",
      "content": "# Monotonic Stack\n\nOrder of modules for this topic:\n\n- [739.Daily Temperatures](modules/dsa/monotonic-stack/739-daily-temperatures)\n- [496.Next Greater Element I](modules/dsa/monotonic-stack/496-next-greater-element-i)\n- [503.Next Greater Element II](modules/dsa/monotonic-stack/503-next-greater-element-ii)\n- [42.Trapping Rain Water](modules/dsa/monotonic-stack/42-trapping-rain-water)\n- [84.Largest Rectangle in Histogram](modules/dsa/monotonic-stack/84-largest-rectangle-in-histogram)\n"
    },
    {
      "track": "dsa",
      "topic": "stack-and-queue",
      "slug": "stack-and-queue",
      "title": "Stack and Queue",
      "path": "docs/dsa/stack-and-queue/README.md",
      "summary": "Order of modules for this topic:",
      "content": "# Stack and Queue\n\nOrder of modules for this topic:\n\n- [Introduction to Stack and Queue](modules/dsa/stack-and-queue/introduction-to-stack-and-queue)\n- [232.Implement Queue using Stacks](modules/dsa/stack-and-queue/232-implement-queue-using-stacks)\n- [225.Implement Stack using Queues](modules/dsa/stack-and-queue/225-implement-stack-using-queues)\n- [20.Valid Parentheses](modules/dsa/stack-and-queue/20-valid-parentheses)\n- [1047.Remove All Adjacent Duplicates in String](modules/dsa/stack-and-queue/1047-remove-all-adjacent-duplicates-in-string)\n- [150.Evaluate Reverse Polish Notation](modules/dsa/stack-and-queue/150-evaluate-reverse-polish-notation)\n- [239.Sliding Window Maximum](modules/dsa/stack-and-queue/239-sliding-window-maximum)\n- [347.Top K Frequent Elements](modules/dsa/stack-and-queue/347-top-k-frequent-elements)\n- [Stack and Queue Summary](modules/dsa/stack-and-queue/stack-and-queue-summary)\n"
    },
    {
      "track": "dsa",
      "topic": "string",
      "slug": "string",
      "title": "String",
      "path": "docs/dsa/string/README.md",
      "summary": "Order of modules for this topic:",
      "content": "# String\n\nOrder of modules for this topic:\n\n- [344.Reverse String](modules/dsa/string/344-reverse-string)\n- [541.Reverse String II](modules/dsa/string/541-reverse-string-ii)\n- [151.Reverse Words in a String](modules/dsa/string/151-reverse-words-in-a-string)\n- [28.Implement strStr()](modules/dsa/string/28-implement-strstr)\n- [459.Repeated Substring Pattern](modules/dsa/string/459-repeated-substring-pattern)\n- [String Summary](modules/dsa/string/string-summary)\n"
    },
    {
      "track": "ml",
      "topic": "computer-vision",
      "slug": "computer-vision",
      "title": "Computer Vision",
      "path": "docs/ml/computer-vision/README.md",
      "summary": "Vision preprocessing and CNN architecture basics. Each bullet maps to a module under `modules/ml/computer-vision/`.",
      "content": "# Computer Vision\n\nVision preprocessing and CNN architecture basics.\nEach bullet maps to a module under `modules/ml/computer-vision/`.\n\n## CNN Fundamentals\n\n- CNNs (`modules/ml/computer-vision/cnn-basics`)\n- Convolution layers (`modules/ml/computer-vision/convolution-layer`)\n- Pooling (max, average) (`modules/ml/computer-vision/pooling`)\n- 2D vs 3D CNN (`modules/ml/computer-vision/cnn-2d-vs-3d`)\n- Image preprocessing (`modules/ml/computer-vision/image-preprocessing`)\n- RGB to grayscale (`modules/ml/computer-vision/rgb-to-grayscale`)\n- Contrast and brightness (`modules/ml/computer-vision/contrast-brightness`)\n- Bilinear resizing (`modules/ml/computer-vision/bilinear-resizing`)\n- Sobel edge detection (`modules/ml/computer-vision/sobel-edge-detection`)\n- Optical flow (EPE) (`modules/ml/computer-vision/optical-flow-epe`)\n- Data augmentation (`modules/ml/computer-vision/data-augmentation`)\n- Non-maximum suppression (`modules/ml/computer-vision/non-maximum-suppression`)\n\n## Classic Architectures\n\n- LeNet-5 (`modules/ml/computer-vision/lenet-5`)\n- AlexNet (`modules/ml/computer-vision/alexnet`)\n- VGGNet (`modules/ml/computer-vision/vggnet`)\n- ResNet (`modules/ml/computer-vision/resnet`)\n"
    },
    {
      "track": "ml",
      "topic": "data",
      "slug": "data",
      "title": "Data and Splitting",
      "path": "docs/ml/data/README.md",
      "summary": "Reliable data setup and evaluation splits. Each bullet maps to a module under `modules/ml/data/`.",
      "content": "# Data and Splitting\n\nReliable data setup and evaluation splits.\nEach bullet maps to a module under `modules/ml/data/`.\n\n## Concepts\n\n- Dataset vs batch vs epoch (`modules/ml/data/dataset-batch-epoch`)\n- Batch iterator (`modules/ml/data/batch-iterator`)\n- Train / validation / test split (`modules/ml/data/train-validation-test-split`)\n- Stratified split (`modules/ml/data/stratified-split`)\n- Data leakage (common failure modes) (`modules/ml/data/data-leakage`)\n- Polynomial feature expansion (`modules/ml/data/polynomial-features`)\n- Handling class imbalance (`modules/ml/data/class-imbalance`)\n"
    },
    {
      "track": "ml",
      "topic": "deep-learning",
      "slug": "deep-learning",
      "title": "Foundations of Deep Learning",
      "path": "docs/ml/deep-learning/README.md",
      "summary": "Neural networks and training mechanics from first principles.",
      "content": "# Foundations of Deep Learning\n\nNeural networks and training mechanics from first principles.\n\n## Neural Networks and Backprop\n\n- Feedforward neural networks\n- Neuron, weights, bias, activation\n- Backpropagation (chain rule, local gradients)\n- Automatic differentiation (forward vs backward mode)\n- Vanishing and exploding gradients\n- Gradient checking (finite differences)\n\n## Activation Functions\n\n- Sigmoid, Tanh, Hard Sigmoid, Dynamic Tanh, Hardtanh\n- ReLU family: ReLU, Leaky ReLU, ELU, PReLU\n- Modern: GeLU, Swish, SwiGLU, Mish\n- Softmax, Softplus, Softsign\n- Failure modes (dead ReLU, saturation)\n\n## Initialization\n\n- Xavier / Glorot: symmetric activations\n- He initialization: ReLU\n- Effect on gradient flow\n\n## Regularization\n\n- L1 (Lasso)\n- L2 (Ridge)\n- Weight decay\n- Dropout\n- Early stopping\n- LoRA (conceptual regularization effect)\n\n## Loss Functions\n\n### Classification\n\n- Cross-entropy\n- Hinge loss\n- Focal loss (imbalanced data)\n\n### Regression\n\n- MSE\n- MAE\n- RMSE\n- Huber loss\n\n### Special\n\n- Knowledge distillation loss\n- KL divergence\n\n## Normalization\n\n- BatchNorm (train vs eval behavior)\n- LayerNorm\n- RMSNorm\n- GroupNorm\n- InstanceNorm\n- Why BatchNorm is bad for Transformers\n"
    },
    {
      "track": "ml",
      "topic": "evaluation",
      "slug": "evaluation",
      "title": "Metrics and Evaluation",
      "path": "docs/ml/evaluation/README.md",
      "summary": "Metrics and evaluation patterns for common tasks. Each bullet maps to a module under `modules/ml/evaluation/` unless noted.",
      "content": "# Metrics and Evaluation\n\nMetrics and evaluation patterns for common tasks.\nEach bullet maps to a module under `modules/ml/evaluation/` unless noted.\n\n## Classification\n\n- Accuracy (when it fails) (`modules/ml/evaluation/accuracy`)\n- Precision / Recall (`modules/ml/evaluation/precision-recall`)\n- F1 (`modules/ml/evaluation/f1-score`)\n- ROC-AUC (`modules/ml/evaluation/roc-auc`)\n- Confusion matrix (`modules/ml/evaluation/confusion-matrix`)\n- Matthews correlation coefficient (`modules/ml/evaluation/matthews-correlation`)\n- Jaccard index (`modules/ml/evaluation/jaccard-index`)\n- Dice score (`modules/ml/evaluation/dice-score`)\n- Gini impurity (`modules/ml/evaluation/gini-impurity`)\n\n## Clustering\n\n- Silhouette score (`modules/ml/evaluation/silhouette-score`)\n- Davies-Bouldin index (`modules/ml/evaluation/davies-bouldin`)\n- Calinski-Harabasz index (`modules/ml/evaluation/calinski-harabasz`)\n\n## Regression\n\n- MAE vs MSE (`modules/ml/evaluation/mae-vs-mse`)\n- RMSE (see `modules/ml/deep-learning/rmse-loss`)\n- R2 (pitfalls) (`modules/ml/evaluation/r2-score`)\n"
    },
    {
      "track": "ml",
      "topic": "fundamentals",
      "slug": "fundamentals",
      "title": "Math for ML",
      "path": "docs/ml/fundamentals/README.md",
      "summary": "Core math for models and training. Each bullet maps to a module under `modules/ml/fundamentals/`.",
      "content": "# Math for ML\n\nCore math for models and training.\nEach bullet maps to a module under `modules/ml/fundamentals/`.\n\n## Linear Algebra\n\n- Vectors, matrices (`modules/ml/fundamentals/vectors-matrices`)\n- Jacobian (`modules/ml/fundamentals/jacobian`)\n- Hessian (`modules/ml/fundamentals/hessian`)\n- SVD (`modules/ml/fundamentals/svd`)\n- Cosine similarity (`modules/ml/fundamentals/cosine-similarity`)\n\n## Probability and Statistics\n\n- Distributions (`modules/ml/fundamentals/distributions`)\n- Empirical PMF (`modules/ml/fundamentals/empirical-pmf`)\n- Expectation (`modules/ml/fundamentals/expectation`)\n- Covariance (`modules/ml/fundamentals/covariance`)\n- Markov chains (`modules/ml/fundamentals/markov-chains`)\n- KL divergence (`modules/ml/fundamentals/kl-divergence`)\n- Jensen-Shannon divergence (`modules/ml/fundamentals/jensen-shannon-divergence`)\n- Mutual information (`modules/ml/fundamentals/mutual-information`)\n- Two-sample t-test (`modules/ml/fundamentals/two-sample-t-test`)\n- Bayesian inference (Beta-Binomial) (`modules/ml/fundamentals/beta-binomial`)\n\n## Optimization\n\n- Gradient descent (`modules/ml/fundamentals/gradient-descent`)\n- Newton's method (`modules/ml/fundamentals/newtons-method`)\n- Convex vs non-convex intuition (`modules/ml/fundamentals/convex-vs-nonconvex`)\n- ELBO (variational inference) (`modules/ml/fundamentals/elbo`)\n"
    },
    {
      "track": "ml",
      "topic": "generative",
      "slug": "generative",
      "title": "Generative Models",
      "path": "docs/ml/generative/README.md",
      "summary": "Core generative families and trade-offs. Each bullet maps to a module under `modules/ml/generative/`.",
      "content": "# Generative Models\n\nCore generative families and trade-offs.\nEach bullet maps to a module under `modules/ml/generative/`.\n\n## Core Families\n\n- GAN (generator vs discriminator, mode collapse) (`modules/ml/generative/gan`)\n- VAE (ELBO, reconstruction + KL) (`modules/ml/generative/vae`)\n- Diffusion models (forward noise / reverse denoise, training objective) (`modules/ml/generative/diffusion-models`)\n\n## Interview-Level Skills\n\n- Choose GAN vs VAE vs Diffusion for a task (trade-offs) (`modules/ml/generative/model-selection`)\n- GAN instability, mode collapse (`modules/ml/generative/gan-mode-collapse`)\n- VAE blurry samples, posterior collapse (`modules/ml/generative/vae-posterior-collapse`)\n- Diffusion slow sampling, guidance trade-offs (`modules/ml/generative/diffusion-guidance-tradeoffs`)\n"
    },
    {
      "track": "ml",
      "topic": "llm",
      "slug": "llm",
      "title": "NLP and LLMs",
      "path": "docs/ml/llm/README.md",
      "summary": "Transformers, training stages, and alignment for LLMs. Each bullet maps to a module under `modules/ml/llm/`.",
      "content": "# NLP and LLMs\n\nTransformers, training stages, and alignment for LLMs.\nEach bullet maps to a module under `modules/ml/llm/`.\n\n## Core Concepts\n\n- Tokenization (`modules/ml/llm/tokenization`)\n- Embeddings (`modules/ml/llm/embeddings`)\n- Positional encoding (`modules/ml/llm/positional-encoding`)\n- Transformer (`modules/ml/llm/transformer`)\n- Self-attention (`modules/ml/llm/self-attention`)\n- Multi-head attention (`modules/ml/llm/multi-head-attention`)\n- Masked attention (`modules/ml/llm/attention-causal`)\n\n## Training Stages\n\n- Pretraining (next-token prediction / PTX loss) (`modules/ml/llm/pretraining`)\n- Supervised fine-tuning (SFT) (`modules/ml/llm/supervised-fine-tuning`)\n- Alignment / preference learning (`modules/ml/llm/preference-learning`)\n\n## Alignment and Optimization\n\n- RLHF (`modules/ml/llm/rlhf`)\n- DPO (`modules/ml/llm/dpo`)\n- KL regularization (`modules/ml/llm/kl-regularization`)\n- PTX anchoring (`modules/ml/llm/ptx-anchoring`)\n\n## Efficiency and Systems\n\n- LoRA (`modules/ml/llm/lora`) / QLoRA (`modules/ml/llm/qlora`)\n- Inference head pruning (`modules/ml/llm/inference-head-pruning`)\n- Sparse attention (`modules/ml/llm/sparse-attention`)\n- MoE (Top-K routing, Noisy gating) (`modules/ml/llm/moe-routing`)\n- FP16 / BF16 / FP8 (`modules/ml/llm/fp16-bf16-fp8`)\n- INT8 / INT4 (`modules/ml/llm/int8-int4-quantization`)\n"
    },
    {
      "track": "ml",
      "topic": "mlops",
      "slug": "mlops",
      "title": "MLOps and Production ML",
      "path": "docs/ml/mlops/README.md",
      "summary": "Operational concerns for production ML. Each bullet maps to a module under `modules/ml/mlops/`.",
      "content": "# MLOps and Production ML\n\nOperational concerns for production ML.\nEach bullet maps to a module under `modules/ml/mlops/`.\n\n## Production Concepts\n\n- ETL pipeline (`modules/ml/mlops/etl-pipeline`)\n- Offline vs online inference (`modules/ml/mlops/offline-online-inference`)\n- Batch vs real-time inference (`modules/ml/mlops/batch-vs-realtime`)\n- Data quality checks (`modules/ml/mlops/data-quality-checks`)\n- Feature drift detection (PSI) (`modules/ml/mlops/feature-drift-psi`)\n- Prediction distribution monitoring (`modules/ml/mlops/prediction-monitoring`)\n- Canary deployment (`modules/ml/mlops/canary-deployment`)\n- A/B testing (`modules/ml/mlops/ab-testing`)\n- SLA metrics (`modules/ml/mlops/sla-metrics`)\n- Request batching (`modules/ml/mlops/request-batching`)\n"
    },
    {
      "track": "ml",
      "topic": "models",
      "slug": "models",
      "title": "Models and Classical ML",
      "path": "docs/ml/models/README.md",
      "summary": "Classical ML models and baselines. Each bullet maps to a module under `modules/ml/models/`.",
      "content": "# Models and Classical ML\n\nClassical ML models and baselines.\nEach bullet maps to a module under `modules/ml/models/`.\n\n## Core Models\n\n- Linear regression (`modules/ml/models/linear-regression`)\n- Logistic regression (`modules/ml/models/logistic-regression`)\n- Softmax regression (`modules/ml/models/softmax-regression`)\n- Elastic Net (`modules/ml/models/elastic-net`)\n- Decision Trees (`modules/ml/models/decision-trees`)\n- Random Forest (bagging) (`modules/ml/models/random-forest`)\n- AdaBoost (`modules/ml/models/adaboost`)\n- KNN (`modules/ml/models/knn`)\n- K-Means (`modules/ml/models/k-means`)\n- DBSCAN (`modules/ml/models/dbscan`)\n- PCA (`modules/ml/models/pca`)\n- Gaussian Naive Bayes (`modules/ml/models/gaussian-naive-bayes`)\n- Bernoulli Naive Bayes (`modules/ml/models/bernoulli-naive-bayes`)\n- SVM (Pegasos) (`modules/ml/models/svm-pegasos`)\n- Gaussian Process Regression (GPR) (`modules/ml/models/gaussian-process-regression`)\n"
    },
    {
      "track": "ml",
      "topic": "optimization",
      "slug": "optimization",
      "title": "Optimization and Training Dynamics",
      "path": "docs/ml/optimization/README.md",
      "summary": "How gradients become stable updates. Each bullet maps to a module under `modules/ml/optimization/`.",
      "content": "# Optimization and Training Dynamics\n\nHow gradients become stable updates.\nEach bullet maps to a module under `modules/ml/optimization/`.\n\n## Optimizers\n\n- SGD (`modules/ml/optimization/sgd`)\n- SGD with Momentum (`modules/ml/optimization/sgd-momentum`)\n- Nesterov Accelerated Gradient (`modules/ml/optimization/nesterov`)\n- Adam (`modules/ml/optimization/adam`)\n- AdamW (decoupled weight decay) (`modules/ml/optimization/adamw`)\n- RMSProp (`modules/ml/optimization/rmsprop`)\n- Adagrad (`modules/ml/optimization/adagrad`)\n- Muon (high-level intuition) (`modules/ml/optimization/muon-optimizer`)\n\n## Learning Rate Strategies\n\n- Constant LR (`modules/ml/optimization/lr-constant`)\n- Step decay (`modules/ml/optimization/lr-step-decay`)\n- Exponential decay (`modules/ml/optimization/lr-exponential-decay`)\n- Warmup (`modules/ml/optimization/lr-warmup`)\n- Cosine decay (`modules/ml/optimization/lr-cosine-decay`)\n\n## Training Stability\n\n- Gradient clipping (global norm) (`modules/ml/optimization/gradient-clipping`)\n- Loss scaling (mixed precision) (`modules/ml/optimization/loss-scaling`)\n- Detecting NaNs / divergence (`modules/ml/optimization/detect-nans`)\n"
    },
    {
      "track": "ml",
      "topic": "reinforcement-learning",
      "slug": "reinforcement-learning",
      "title": "Reinforcement Learning",
      "path": "docs/ml/reinforcement-learning/README.md",
      "summary": "Agents learning from rewards and exploration. Each bullet maps to a module under `modules/ml/reinforcement-learning/`.",
      "content": "# Reinforcement Learning\n\nAgents learning from rewards and exploration.\nEach bullet maps to a module under `modules/ml/reinforcement-learning/`.\n\n## Core Ideas\n\n- MDPs (`modules/ml/reinforcement-learning/mdp`)\n- Reward, return, discount factor (`modules/ml/reinforcement-learning/return-discount`)\n- Exploration vs exploitation (`modules/ml/reinforcement-learning/exploration-exploitation`)\n\n## Algorithms\n\n- Bandits (`modules/ml/reinforcement-learning/bandits`)\n- epsilon-greedy (`modules/ml/reinforcement-learning/bandit-epsilon-greedy`)\n- UCB (`modules/ml/reinforcement-learning/ucb`)\n- REINFORCE (`modules/ml/reinforcement-learning/reinforce`)\n- PPO (why clipping helps) (`modules/ml/reinforcement-learning/ppo`)\n- Q-Learning (`modules/ml/reinforcement-learning/q-learning`)\n- SARSA (`modules/ml/reinforcement-learning/sarsa`)\n\n## LLM-related RL\n\n- DPO vs PPO (`modules/ml/reinforcement-learning/dpo-vs-ppo`)\n- Group-based optimization (GSPO / GRPO) (`modules/ml/reinforcement-learning/group-based-optimization`)\n"
    },
    {
      "track": "ml",
      "topic": "representation",
      "slug": "representation",
      "title": "Representation Learning",
      "path": "docs/ml/representation/README.md",
      "summary": "Cross-cutting representation items.",
      "content": "# Representation Learning\n\nCross-cutting representation items.\n\n## Core Concepts\n\n- Embeddings\n- Positional encoding\n- Cosine similarity\n- PCA\n- SVD\n- Polynomial feature expansion\n"
    },
    {
      "track": "ml",
      "topic": "systems",
      "slug": "systems",
      "title": "Training Loop Mechanics",
      "path": "docs/ml/systems/README.md",
      "summary": "Training loop mechanics and debugging signals. Each bullet maps to a module under `modules/ml/systems/`.",
      "content": "# Training Loop Mechanics\n\nTraining loop mechanics and debugging signals.\nEach bullet maps to a module under `modules/ml/systems/`.\n\n## Core Steps\n\n- Zeroing gradients (`modules/ml/systems/zero-gradients`)\n- Forward pass (`modules/ml/systems/forward-pass`)\n- Backward pass (`modules/ml/systems/backward-pass`)\n- Optimizer step (`modules/ml/systems/optimizer-step`)\n- Gradient accumulation (`modules/ml/systems/gradient-accumulation`)\n- Mixed precision training (`modules/ml/systems/mixed-precision`)\n- Check gradients are flowing (`modules/ml/systems/check-gradients`)\n- Debug overfitting vs underfitting (`modules/ml/systems/debug-overfit-underfit`)\n"
    }
  ]
}
